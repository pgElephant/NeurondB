#!/bin/bash
# ============================================================================
# load_documentation.sh
#    Generic script to load documentation files and create embeddings
#    using NeurondB
#
# Usage:
#    ./load_documentation.sh [OPTIONS]
#
# Options:
#    -d, --directory DIR     Directory containing documentation files
#    -D, --database DB       Database name (default: postgres)
#    -U, --user USER         Database user (default: postgres)
#    -H, --host HOST         Database host (default: localhost)
#    -p, --port PORT         Database port (default: 5432)
#    -m, --model MODEL       Embedding model (default: all-MiniLM-L6-v2)
#    -c, --chunk-size SIZE   Chunk size in characters (default: 1000)
#    -o, --overlap SIZE      Chunk overlap in characters (default: 200)
#    -t, --table-prefix PFX   Table name prefix (default: docs)
#    --skip-embeddings        Skip embedding generation
#    --skip-index            Skip index creation
#    -v, --verbose           Verbose output
#    -h, --help              Show this help message
#
# Example:
#    ./load_documentation.sh -d /path/to/docs -D mydb -U myuser
#
# Copyright (c) 2024-2025, pgElephant, Inc.
# ============================================================================

set -euo pipefail

# Default values
DOC_DIR=""
DB_NAME="postgres"
DB_USER="postgres"
DB_HOST="localhost"
DB_PORT="5432"
EMBEDDING_MODEL="all-MiniLM-L6-v2"
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TABLE_PREFIX="docs"
SKIP_EMBEDDINGS=false
SKIP_INDEX=false
VERBOSE=false

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Parse command line arguments
while [[ $# -gt 0 ]]; do
	case $1 in
		-d|--directory)
			DOC_DIR="$2"
			shift 2
			;;
		-D|--database)
			DB_NAME="$2"
			shift 2
			;;
		-U|--user)
			DB_USER="$2"
			shift 2
			;;
		-H|--host)
			DB_HOST="$2"
			shift 2
			;;
		-p|--port)
			DB_PORT="$2"
			shift 2
			;;
		-m|--model)
			EMBEDDING_MODEL="$2"
			shift 2
			;;
		-c|--chunk-size)
			CHUNK_SIZE="$2"
			shift 2
			;;
		-o|--overlap)
			CHUNK_OVERLAP="$2"
			shift 2
			;;
		-t|--table-prefix)
			TABLE_PREFIX="$2"
			shift 2
			;;
		--skip-embeddings)
			SKIP_EMBEDDINGS=true
			shift
			;;
		--skip-index)
			SKIP_INDEX=true
			shift
			;;
		-v|--verbose)
			VERBOSE=true
			shift
			;;
		-h|--help)
			head -n 30 "$0" | tail -n +2
			exit 0
			;;
		*)
			echo -e "${RED}Unknown option: $1${NC}"
			echo "Use -h or --help for usage information"
			exit 1
			;;
	esac
done

# Validate required parameters
if [[ -z "$DOC_DIR" ]]; then
	echo -e "${RED}Error: Directory is required${NC}"
	echo "Use -d or --directory to specify the documentation directory"
	exit 1
fi

if [[ ! -d "$DOC_DIR" ]]; then
	echo -e "${RED}Error: Directory does not exist: $DOC_DIR${NC}"
	exit 1
fi

# Logging function
log() {
	local level=$1
	shift
	local message="$@"
	local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
	
	case $level in
		INFO)
			echo -e "${GREEN}[INFO]${NC} [$timestamp] $message"
			;;
		WARN)
			echo -e "${YELLOW}[WARN]${NC} [$timestamp] $message"
			;;
		ERROR)
			echo -e "${RED}[ERROR]${NC} [$timestamp] $message" >&2
			;;
		DEBUG)
			if [[ "$VERBOSE" == "true" ]]; then
				echo -e "[DEBUG] [$timestamp] $message"
			fi
			;;
	esac
}

# Check if psql is available
if ! command -v psql &> /dev/null; then
	log ERROR "psql command not found. Please install PostgreSQL client tools."
	exit 1
fi

# Test database connection
log INFO "Testing database connection..."
if ! PGPASSWORD="${PGPASSWORD:-}" psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" -c "SELECT 1;" > /dev/null 2>&1; then
	log ERROR "Cannot connect to database. Check your credentials."
	exit 1
fi
log INFO "Database connection successful"

# Create temporary SQL file
SQL_FILE=$(mktemp)
trap "rm -f $SQL_FILE" EXIT

log INFO "Creating schema and tables..."

# Generate SQL for schema creation
cat > "$SQL_FILE" <<EOF
/*-------------------------------------------------------------------------
 * Documentation Embedding Schema
 * Generated by load_documentation.sh
 *-------------------------------------------------------------------------
 */

-- Main documents table
CREATE TABLE IF NOT EXISTS ${TABLE_PREFIX}_documents (
	doc_id SERIAL PRIMARY KEY,
	filepath TEXT NOT NULL UNIQUE,
	filename TEXT NOT NULL,
	title TEXT,
	content TEXT NOT NULL,
	file_size INTEGER,
	file_type TEXT,
	created_at TIMESTAMPTZ DEFAULT now(),
	updated_at TIMESTAMPTZ DEFAULT now(),
	metadata JSONB DEFAULT '{}'::jsonb
);

-- Document chunks table
CREATE TABLE IF NOT EXISTS ${TABLE_PREFIX}_chunks (
	chunk_id SERIAL PRIMARY KEY,
	doc_id INTEGER REFERENCES ${TABLE_PREFIX}_documents(doc_id) ON DELETE CASCADE,
	chunk_index INTEGER NOT NULL,
	chunk_text TEXT NOT NULL,
	chunk_tokens INTEGER,
	embedding VECTOR(384),
	metadata JSONB DEFAULT '{}'::jsonb,
	created_at TIMESTAMPTZ DEFAULT now(),
	UNIQUE(doc_id, chunk_index)
);

-- Indexes
CREATE INDEX IF NOT EXISTS idx_${TABLE_PREFIX}_docs_filepath 
	ON ${TABLE_PREFIX}_documents(filepath);
CREATE INDEX IF NOT EXISTS idx_${TABLE_PREFIX}_docs_filename 
	ON ${TABLE_PREFIX}_documents(filename);
CREATE INDEX IF NOT EXISTS idx_${TABLE_PREFIX}_chunks_doc_id 
	ON ${TABLE_PREFIX}_chunks(doc_id);
CREATE INDEX IF NOT EXISTS idx_${TABLE_PREFIX}_chunks_doc_chunk 
	ON ${TABLE_PREFIX}_chunks(doc_id, chunk_index);

-- Function to update updated_at timestamp
CREATE OR REPLACE FUNCTION update_${TABLE_PREFIX}_updated_at()
RETURNS TRIGGER AS \$\$
BEGIN
	NEW.updated_at = now();
	RETURN NEW;
END;
\$\$ LANGUAGE plpgsql;

-- Trigger for updated_at
DROP TRIGGER IF EXISTS trigger_${TABLE_PREFIX}_docs_updated_at 
	ON ${TABLE_PREFIX}_documents;
CREATE TRIGGER trigger_${TABLE_PREFIX}_docs_updated_at
	BEFORE UPDATE ON ${TABLE_PREFIX}_documents
	FOR EACH ROW
	EXECUTE FUNCTION update_${TABLE_PREFIX}_updated_at();

-- Statistics view
CREATE OR REPLACE VIEW ${TABLE_PREFIX}_stats AS
SELECT 
	COUNT(DISTINCT d.doc_id) AS total_documents,
	COUNT(c.chunk_id) AS total_chunks,
	COUNT(c.chunk_id) FILTER (WHERE c.embedding IS NOT NULL) AS chunks_with_embeddings,
	AVG(LENGTH(d.content)) AS avg_doc_length,
	SUM(LENGTH(d.content)) AS total_content_size,
	MIN(d.created_at) AS first_loaded,
	MAX(d.updated_at) AS last_updated
FROM ${TABLE_PREFIX}_documents d
LEFT JOIN ${TABLE_PREFIX}_chunks c ON d.doc_id = c.doc_id;

EOF

# Execute schema creation
if PGPASSWORD="${PGPASSWORD:-}" psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" -f "$SQL_FILE" > /dev/null 2>&1; then
	log INFO "Schema created successfully"
else
	log ERROR "Failed to create schema"
	exit 1
fi

# Count files
log INFO "Scanning directory: $DOC_DIR"
FILE_COUNT=$(find "$DOC_DIR" -type f \( -name "*.html" -o -name "*.md" -o -name "*.txt" -o -name "*.rst" -o -name "*.xml" \) | wc -l)
log INFO "Found $FILE_COUNT documentation files"

if [[ $FILE_COUNT -eq 0 ]]; then
	log WARN "No documentation files found. Supported formats: .html, .md, .txt, .rst, .xml"
	exit 0
fi

# Create Python script for file loading
PYTHON_SCRIPT=$(mktemp)
trap "rm -f $SQL_FILE $PYTHON_SCRIPT" EXIT

cat > "$PYTHON_SCRIPT" <<'PYEOF'
#!/usr/bin/env python3
import sys
import os
import re
import json
import psycopg2
from pathlib import Path
from html.parser import HTMLParser

class HTMLTextExtractor(HTMLParser):
	def __init__(self):
		super().__init__()
		self.text = []
		self.skip_tags = {'script', 'style', 'meta', 'link'}
		self.in_skip = False
	
	def handle_starttag(self, tag, attrs):
		if tag.lower() in self.skip_tags:
			self.in_skip = True
	
	def handle_endtag(self, tag):
		if tag.lower() in self.skip_tags:
			self.in_skip = False
	
	def handle_data(self, data):
		if not self.in_skip:
			self.text.append(data.strip())
	
	def get_text(self):
		return ' '.join(self.text)

def extract_title(content, filepath):
	"""Extract title from content or use filename"""
	# Try HTML title tag
	match = re.search(r'<title[^>]*>(.*?)</title>', content, re.IGNORECASE | re.DOTALL)
	if match:
		title = re.sub(r'<[^>]+>', '', match.group(1)).strip()
		if title:
			return title
	
	# Try Markdown title
	match = re.match(r'^#\s+(.+)$', content, re.MULTILINE)
	if match:
		return match.group(1).strip()
	
	# Try RST title
	match = re.match(r'^(.+)\n=+\s*$', content, re.MULTILINE)
	if match:
		return match.group(1).strip()
	
	# Use filename
	return Path(filepath).stem.replace('_', ' ').replace('-', ' ').title()

def clean_html(content):
	"""Remove HTML tags and clean text"""
	extractor = HTMLTextExtractor()
	extractor.feed(content)
	text = extractor.get_text()
	# Clean up whitespace
	text = re.sub(r'\s+', ' ', text)
	return text.strip()

def clean_markdown(content):
	"""Basic markdown cleaning - remove formatting, keep text"""
	# Remove code blocks
	content = re.sub(r'```[^`]*```', '', content, flags=re.DOTALL)
	content = re.sub(r'`[^`]+`', '', content)
	# Remove links but keep text
	content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)
	# Remove images
	content = re.sub(r'!\[[^\]]*\]\([^\)]+\)', '', content)
	# Remove headers but keep text
	content = re.sub(r'^#+\s+', '', content, flags=re.MULTILINE)
	# Clean whitespace
	content = re.sub(r'\s+', ' ', content)
	return content.strip()

def process_file(filepath):
	"""Process a single file and return (title, content, metadata)"""
	path = Path(filepath)
	
	try:
		with open(path, 'r', encoding='utf-8', errors='ignore') as f:
			content = f.read()
	except Exception as e:
		return None, None, None
	
	if len(content.strip()) < 50:  # Skip very short files
		return None, None, None
	
	# Determine file type and clean content
	ext = path.suffix.lower()
	if ext == '.html' or ext == '.xml':
		cleaned = clean_html(content)
		file_type = 'html'
	elif ext == '.md':
		cleaned = clean_markdown(content)
		file_type = 'markdown'
	else:
		# Plain text
		cleaned = re.sub(r'\s+', ' ', content).strip()
		file_type = 'text'
	
	if len(cleaned) < 50:
		return None, None, None
	
	title = extract_title(content, filepath)
	metadata = {
		"extension": ext,
		"file_type": file_type,
		"original_length": len(content),
		"cleaned_length": len(cleaned)
	}
	
	return title, cleaned, metadata

def load_files(conn, doc_dir, table_prefix, verbose=False):
	"""Load all files from directory into database"""
	cur = conn.cursor()
	
	# Find all documentation files
	extensions = ('.html', '.md', '.txt', '.rst', '.xml')
	files = []
	for ext in extensions:
		files.extend(Path(doc_dir).rglob(f'*{ext}'))
	
	total = len(files)
	loaded = 0
	skipped = 0
	errors = 0
	
	print(f"Processing {total} files...")
	
	for i, filepath in enumerate(files, 1):
		if verbose and i % 100 == 0:
			print(f"Progress: {i}/{total} files processed...")
		
		title, content, metadata = process_file(filepath)
		
		if title is None:
			skipped += 1
			continue
		
		try:
			cur.execute(f"""
				INSERT INTO {table_prefix}_documents 
					(filepath, filename, title, content, file_size, file_type, metadata)
				VALUES (%s, %s, %s, %s, %s, %s, %s)
				ON CONFLICT (filepath) DO UPDATE SET
					title = EXCLUDED.title,
					content = EXCLUDED.content,
					file_size = EXCLUDED.file_size,
					file_type = EXCLUDED.file_type,
					metadata = EXCLUDED.metadata,
					updated_at = now()
			""", (
				str(filepath),
				Path(filepath).name,
				title,
				content,
				filepath.stat().st_size,
				metadata['file_type'],
				json.dumps(metadata)
			))
			loaded += 1
		except Exception as e:
			errors += 1
			if verbose:
				print(f"Error loading {filepath}: {e}")
	
	conn.commit()
	cur.close()
	
	return loaded, skipped, errors

if __name__ == '__main__':
	if len(sys.argv) < 5:
		print("Usage: python3 script.py <db_host> <db_port> <db_name> <db_user> <doc_dir> <table_prefix> [verbose]")
		sys.exit(1)
	
	db_host = sys.argv[1]
	db_port = sys.argv[2]
	db_name = sys.argv[3]
	db_user = sys.argv[4]
	doc_dir = sys.argv[5]
	table_prefix = sys.argv[6]
	verbose = len(sys.argv) > 7 and sys.argv[7] == 'true'
	
	conn = psycopg2.connect(
		host=db_host,
		port=db_port,
		database=db_name,
		user=db_user,
		password=os.environ.get('PGPASSWORD', '')
	)
	
	loaded, skipped, errors = load_files(conn, doc_dir, table_prefix, verbose)
	
	print(f"Loaded: {loaded}, Skipped: {skipped}, Errors: {errors}")
	
	conn.close()
	
	sys.exit(0 if errors == 0 else 1)
PYEOF

chmod +x "$PYTHON_SCRIPT"

# Load files
log INFO "Loading files into database..."
VERBOSE_FLAG=""
if [[ "$VERBOSE" == "true" ]]; then
	VERBOSE_FLAG="true"
fi

if python3 "$PYTHON_SCRIPT" "$DB_HOST" "$DB_PORT" "$DB_NAME" "$DB_USER" "$DOC_DIR" "$TABLE_PREFIX" "$VERBOSE_FLAG"; then
	log INFO "Files loaded successfully"
else
	log ERROR "Failed to load files"
	exit 1
fi

# Chunk documents
log INFO "Chunking documents..."
cat > "$SQL_FILE" <<EOF
/*-------------------------------------------------------------------------
 * Chunk documents into smaller segments
 *-------------------------------------------------------------------------
 */

-- Function to chunk text with overlap
CREATE OR REPLACE FUNCTION chunk_text(
	text_content TEXT,
	chunk_size INTEGER,
	chunk_overlap INTEGER
) RETURNS TABLE(chunk_text TEXT, chunk_index INTEGER) AS \$\$
DECLARE
	text_len INTEGER;
	start_pos INTEGER := 1;
	current_index INTEGER := 0;
	current_chunk TEXT;
	chunk_end INTEGER;
BEGIN
	text_len := LENGTH(text_content);
	
	WHILE start_pos <= text_len LOOP
		chunk_end := LEAST(start_pos + chunk_size - 1, text_len);
		current_chunk := SUBSTRING(text_content FROM start_pos FOR (chunk_end - start_pos + 1));
		
		-- Try to break at sentence boundary if not at end
		IF chunk_end < text_len THEN
			DECLARE
				sentence_end INTEGER;
			BEGIN
				-- Look for sentence endings within last 20% of chunk
				sentence_end := GREATEST(
					LENGTH(current_chunk) * 8 / 10,
					LENGTH(current_chunk) - 100
				);
				
				-- Find last sentence boundary
				FOR sentence_end IN REVERSE sentence_end..1 LOOP
					IF SUBSTRING(current_chunk FROM sentence_end FOR 1) IN ('.', '!', '?', '\n') THEN
						current_chunk := SUBSTRING(current_chunk FROM 1 FOR sentence_end);
						chunk_end := start_pos + sentence_end - 1;
						EXIT;
					END IF;
				END LOOP;
			END;
		END IF;
		
		-- Skip very short chunks
		IF LENGTH(TRIM(current_chunk)) >= 50 THEN
			RETURN QUERY SELECT TRIM(current_chunk), current_index;
			current_index := current_index + 1;
		END IF;
		
		-- Move start position with overlap
		start_pos := chunk_end + 1 - chunk_overlap;
		IF start_pos <= (chunk_end - chunk_size + chunk_overlap) THEN
			start_pos := chunk_end + 1;
		END IF;
	END LOOP;
END;
\$\$ LANGUAGE plpgsql;

-- Delete existing chunks for re-chunking
DELETE FROM ${TABLE_PREFIX}_chunks;

-- Insert chunks
INSERT INTO ${TABLE_PREFIX}_chunks (doc_id, chunk_index, chunk_text, chunk_tokens)
SELECT 
	d.doc_id,
	ct.chunk_index,
	ct.chunk_text,
	array_length(regexp_split_to_array(ct.chunk_text, '\s+'), 1) AS chunk_tokens
FROM ${TABLE_PREFIX}_documents d
CROSS JOIN LATERAL chunk_text(d.content, $CHUNK_SIZE, $CHUNK_OVERLAP) ct
WHERE d.content IS NOT NULL AND LENGTH(d.content) > 0;

-- Statistics
SELECT 
	'Chunking complete' AS status,
	COUNT(*) AS total_chunks,
	AVG(chunk_tokens)::INTEGER AS avg_tokens_per_chunk,
	MIN(chunk_tokens) AS min_tokens,
	MAX(chunk_tokens) AS max_tokens
FROM ${TABLE_PREFIX}_chunks;

EOF

if PGPASSWORD="${PGPASSWORD:-}" psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" -f "$SQL_FILE" 2>&1 | tee /dev/stderr | grep -q "Chunking complete"; then
	log INFO "Documents chunked successfully"
else
	log WARN "Chunking completed with warnings (check output above)"
fi

# Generate embeddings
if [[ "$SKIP_EMBEDDINGS" != "true" ]]; then
	log INFO "Generating embeddings using model: $EMBEDDING_MODEL"
	log INFO "This may take a while for large document sets..."
	
	cat > "$SQL_FILE" <<EOF
/*-------------------------------------------------------------------------
 * Generate embeddings for document chunks
 *-------------------------------------------------------------------------
 */

-- Update chunks with embeddings in batches
DO \$\$
DECLARE
	total_chunks INTEGER;
	processed INTEGER := 0;
	batch_size INTEGER := 100;
	chunk_rec RECORD;
BEGIN
	SELECT COUNT(*) INTO total_chunks 
	FROM ${TABLE_PREFIX}_chunks 
	WHERE embedding IS NULL;
	
	RAISE NOTICE 'Total chunks to process: %', total_chunks;
	
	FOR chunk_rec IN 
		SELECT chunk_id, chunk_text 
		FROM ${TABLE_PREFIX}_chunks 
		WHERE embedding IS NULL
		ORDER BY chunk_id
	LOOP
		BEGIN
			UPDATE ${TABLE_PREFIX}_chunks
			SET embedding = embed_text(chunk_rec.chunk_text, '$EMBEDDING_MODEL')
			WHERE chunk_id = chunk_rec.chunk_id;
			
			processed := processed + 1;
			
			IF processed % batch_size = 0 THEN
				RAISE NOTICE 'Processed %/% chunks...', processed, total_chunks;
				COMMIT;
			END IF;
		EXCEPTION WHEN OTHERS THEN
			RAISE WARNING 'Failed to generate embedding for chunk %: %', 
				chunk_rec.chunk_id, SQLERRM;
		END;
	END LOOP;
	
	RAISE NOTICE 'Embedding generation complete. Processed: % chunks', processed;
END;
\$\$;

-- Statistics
SELECT 
	'Embedding generation complete' AS status,
	COUNT(*) AS total_chunks,
	COUNT(*) FILTER (WHERE embedding IS NOT NULL) AS chunks_with_embeddings,
	COUNT(*) FILTER (WHERE embedding IS NULL) AS chunks_without_embeddings,
	ROUND(100.0 * COUNT(*) FILTER (WHERE embedding IS NOT NULL) / COUNT(*), 2) AS success_rate
FROM ${TABLE_PREFIX}_chunks;

EOF

	if PGPASSWORD="${PGPASSWORD:-}" psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" -f "$SQL_FILE" 2>&1 | tee /dev/stderr; then
		log INFO "Embeddings generated successfully"
	else
		log WARN "Embedding generation completed with warnings"
	fi
else
	log INFO "Skipping embedding generation (--skip-embeddings flag set)"
fi

# Create vector index
if [[ "$SKIP_INDEX" != "true" ]] && [[ "$SKIP_EMBEDDINGS" != "true" ]]; then
	log INFO "Creating vector index for similarity search..."
	
	cat > "$SQL_FILE" <<EOF
/*-------------------------------------------------------------------------
 * Create HNSW index for fast vector similarity search
 *-------------------------------------------------------------------------
 */

-- Drop existing index if present
DROP INDEX IF EXISTS idx_${TABLE_PREFIX}_chunks_embedding;

-- Create HNSW index
CREATE INDEX idx_${TABLE_PREFIX}_chunks_embedding 
	ON ${TABLE_PREFIX}_chunks 
	USING hnsw (embedding vector_l2_ops)
	WITH (m = 16, ef_construction = 200);

-- Analyze table for query planner
ANALYZE ${TABLE_PREFIX}_chunks;

EOF

	if PGPASSWORD="${PGPASSWORD:-}" psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" -f "$SQL_FILE" > /dev/null 2>&1; then
		log INFO "Vector index created successfully"
	else
		log WARN "Index creation completed with warnings (may already exist)"
	fi
else
	log INFO "Skipping index creation"
fi

# Final statistics
log INFO "Generating final statistics..."
cat > "$SQL_FILE" <<EOF
SELECT * FROM ${TABLE_PREFIX}_stats;
EOF

log INFO "=== Final Statistics ==="
PGPASSWORD="${PGPASSWORD:-}" psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" -f "$SQL_FILE"

log INFO "Documentation loading complete!"
log INFO ""
log INFO "You can now query your documentation using:"
log INFO "  SELECT * FROM ${TABLE_PREFIX}_documents LIMIT 10;"
log INFO "  SELECT * FROM ${TABLE_PREFIX}_chunks LIMIT 10;"
log INFO ""
log INFO "For semantic search, use:"
log INFO "  WITH q AS (SELECT embed_text('your query', '$EMBEDDING_MODEL') AS emb)"
log INFO "  SELECT c.chunk_text, c.embedding <-> q.emb AS distance"
log INFO "  FROM ${TABLE_PREFIX}_chunks c, q"
log INFO "  ORDER BY distance LIMIT 10;"

