/*-------------------------------------------------------------------------
 *
 * gpu_hf_kernels.cu
 *    Hugging Face and LLM kernel operations.
 *
 * This module implements accelerated operations for Hugging Face models
 * including tokenization, embedding lookup, transformer operations,
 * text generation, and reranking.
 *
 * Copyright (c) 2024-2025, pgElephant, Inc.
 *
 * IDENTIFICATION
 *    src/gpu/cuda/gpu_hf_kernels.cu
 *
 *-------------------------------------------------------------------------
 */

#include <cuda_runtime.h>
#include <cublas_v2.h>
#include <curand.h>
#include <curand_kernel.h>
#include <math.h>
#include <string.h>
#include <stdlib.h>
#include <time.h>

#include "neurondb_cuda_hf.h"

/* Helper: Get CUDA thread and block dimensions */
#define GET_BLOCKS(n, threads) ((n + threads - 1) / threads)
#define GET_THREADS(n) ((n < 256) ? n : 256)

/*======================================================================*/
/* Tokenization Kernels */
/*======================================================================*/

/*
 * Simple word-based tokenization kernel (placeholder for full BPE/WordPiece)
 * This is a simplified implementation; full tokenizers should use
 * pre-trained vocabularies from Hugging Face.
 */
__global__ static void __attribute__((unused))
ndb_cuda_hf_tokenize_simple_kernel(const char *text,
	int text_len,
	int32_t *token_ids,
	int32_t *attention_mask,
	int max_seq_len,
	int vocab_size)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;
	int word_start = 0;
	int word_count = 0;
	int i;

	if (tid != 0)
		return;

	/* Simple whitespace tokenization */
	for (i = 0; i < text_len && word_count < max_seq_len - 2; i++)
	{
		if (text[i] == ' ' || text[i] == '\t' || text[i] == '\n')
		{
			if (i > word_start)
			{
				/* Hash word to token ID (simplified) */
				uint32_t hash = 0;
				int j;

				for (j = word_start; j < i && j < text_len; j++)
					hash = hash * 31
						+ (unsigned char)text[j];
				token_ids[word_count + 1] =
					(int32_t)(hash % vocab_size);
				attention_mask[word_count + 1] = 1;
				word_count++;
			}
			word_start = i + 1;
		}
	}
	/* Last word */
	if (word_start < text_len && word_count < max_seq_len - 2)
	{
		uint32_t hash = 0;
		int j;

		for (j = word_start; j < text_len; j++)
			hash = hash * 31 + (unsigned char)text[j];
		token_ids[word_count + 1] = (int32_t)(hash % vocab_size);
		attention_mask[word_count + 1] = 1;
		word_count++;
	}

	/* Add CLS token at start, SEP token at end */
	token_ids[0] = 101; /* [CLS] token ID for BERT */
	attention_mask[0] = 1;
	token_ids[word_count + 1] = 102; /* [SEP] token ID for BERT */
	attention_mask[word_count + 1] = 1;

	/* Pad remaining positions */
	for (i = word_count + 2; i < max_seq_len; i++)
	{
		token_ids[i] = 0;
		attention_mask[i] = 0;
	}
}

/*======================================================================*/
/* Embedding Operations */
/*======================================================================*/

/*
 * Embedding lookup kernel: Map token IDs to embedding vectors
 */
__global__ static void
ndb_cuda_hf_embedding_lookup_kernel(const float *embedding_table,
	const int32_t *token_ids,
	const int32_t *attention_mask,
	float *embeddings,
	int seq_len,
	int embed_dim,
	int vocab_size)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;
	int seq_idx = tid / embed_dim;
	int dim_idx = tid % embed_dim;

	if (seq_idx >= seq_len || dim_idx >= embed_dim)
		return;

	int token_id = token_ids[seq_idx];
	if (token_id < 0 || token_id >= vocab_size)
		token_id = 0;

	int embed_offset = token_id * embed_dim + dim_idx;
	if (embed_offset >= vocab_size * embed_dim)
		return;

	embeddings[seq_idx * embed_dim + dim_idx] =
		embedding_table[embed_offset] * (float)attention_mask[seq_idx];
}

/*
 * Single token embedding lookup kernel: Map a single token ID to embedding vector
 * Used in autoregressive generation for single token lookups
 */
__global__ static void
ndb_cuda_hf_single_embedding_lookup_kernel(const float *embedding_table,
	int32_t token_id,
	float *embedding,
	int embed_dim,
	int vocab_size)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= embed_dim)
		return;

	int token_idx = (token_id >= 0 && token_id < vocab_size) ? token_id : 0;
	int embed_offset = token_idx * embed_dim + tid;

	if (embed_offset >= vocab_size * embed_dim)
		return;

	embedding[tid] = embedding_table[embed_offset];
}

/*
 * Position embedding addition kernel: Add position embedding to token embedding
 */
__global__ static void
ndb_cuda_hf_add_position_embedding_kernel(const float *token_embedding,
	const float *position_embedding,
	float *output,
	int embed_dim)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= embed_dim)
		return;

	output[tid] = token_embedding[tid] + position_embedding[tid];
}

/*
 * Element-wise addition kernel: Add two vectors element-wise
 */
__global__ static void
ndb_cuda_hf_add_vectors_kernel(const float *a,
	const float *b,
	float *output,
	int n)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= n)
		return;

	output[tid] = a[tid] + b[tid];
}

/*
 * Element-wise multiplication kernel: Multiply two vectors element-wise
 */
__global__ static void __attribute__((unused))
ndb_cuda_hf_multiply_vectors_kernel(const float *a,
	const float *b,
	float *output,
	int n)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= n)
		return;

	output[tid] = a[tid] * b[tid];
}

/*
 * Mean pooling kernel: Average embeddings across sequence length
 */
__global__ static void
ndb_cuda_hf_mean_pooling_kernel(const float *embeddings,
	const int32_t *attention_mask,
	float *pooled_embedding,
	int seq_len,
	int embed_dim)
{
	int dim_idx = blockIdx.x * blockDim.x + threadIdx.x;

	if (dim_idx >= embed_dim)
		return;

	float sum = 0.0f;
	int count = 0;
	int i;

	for (i = 0; i < seq_len; i++)
	{
		if (attention_mask[i] != 0)
		{
			sum += embeddings[i * embed_dim + dim_idx];
			count++;
		}
	}

	if (count > 0)
		pooled_embedding[dim_idx] = sum / (float)count;
	else
		pooled_embedding[dim_idx] = 0.0f;
}

/*
 * CLS token extraction: Extract embedding from first token (CLS)
 */
__global__ static void __attribute__((unused))
ndb_cuda_hf_cls_pooling_kernel(const float *embeddings,
	float *pooled_embedding,
	int embed_dim)
{
	int dim_idx = blockIdx.x * blockDim.x + threadIdx.x;

	if (dim_idx >= embed_dim)
		return;

	/* CLS token is at position 0 */
	pooled_embedding[dim_idx] = embeddings[dim_idx];
}

/*======================================================================*/
/* Matrix Operations (using cuBLAS) */
/*======================================================================*/

/*
 * Matrix multiplication wrapper using cuBLAS
 */
static cublasHandle_t g_cublas_handle = NULL;

static int
ndb_cuda_init_cublas(void)
{
	cublasStatus_t status;

	if (g_cublas_handle != NULL)
		return 0;

	status = cublasCreate(&g_cublas_handle);
	if (status != CUBLAS_STATUS_SUCCESS)
		return -1;

	return 0;
}

static void __attribute__((unused))
ndb_cuda_cleanup_cublas(void)
{
	if (g_cublas_handle != NULL)
	{
		cublasDestroy(g_cublas_handle);
		g_cublas_handle = NULL;
	}
}

/*
 * Matrix multiplication: C = A * B
 * A: [m x k], B: [k x n], C: [m x n]
 * cuBLAS uses column-major order, so we need to handle row-major inputs
 */
static int __attribute__((unused))
ndb_cuda_matmul(const float *A, const float *B, float *C, int m, int n, int k)
{
	cublasStatus_t status;
	const float alpha = 1.0f;
	const float beta = 0.0f;

	if (ndb_cuda_init_cublas() != 0)
		return -1;

	/* cuBLAS uses column-major, so for row-major A * B:
	 * C = A * B (row-major) = (B^T * A^T)^T (column-major)
	 * We use: C^T = B^T * A^T
	 * So: C = (B^T * A^T)^T
	 */
	status = cublasSgemm(g_cublas_handle,
		CUBLAS_OP_T, /* Transpose B (B^T) */
		CUBLAS_OP_T, /* Transpose A (A^T) */
		n, /* Rows of C^T (cols of C) */
		m, /* Columns of C^T (rows of C) */
		k, /* Common dimension */
		&alpha,
		B, /* B^T */
		k, /* Leading dimension of B */
		A, /* A^T */
		k, /* Leading dimension of A */
		&beta,
		C, /* C^T */
		n); /* Leading dimension of C^T */

	if (status != CUBLAS_STATUS_SUCCESS)
		return -1;

	return 0;
}

/*
 * Matrix-vector multiplication: y = A * x
 * A: [m x n], x: [n], y: [m]
 * Optimized for single vector case
 */
static int
ndb_cuda_matvec(const float *A, const float *x, float *y, int m, int n)
{
	cublasStatus_t status;
	const float alpha = 1.0f;
	const float beta = 0.0f;

	if (ndb_cuda_init_cublas() != 0)
		return -1;

	/* cuBLAS gemv: y = alpha * A * x + beta * y */
	/* A is column-major [m x n], x is [n], y is [m] */
	status = cublasSgemv(g_cublas_handle,
		CUBLAS_OP_N, /* No transpose */
		m, /* Rows of A */
		n, /* Columns of A */
		&alpha,
		A, /* A */
		m, /* Leading dimension of A */
		x, /* x */
		1, /* Stride of x */
		&beta,
		y, /* y */
		1); /* Stride of y */

	if (status != CUBLAS_STATUS_SUCCESS)
		return -1;

	return 0;
}

/*
 * Logit computation kernel: Compute logits from hidden states and LM head weights
 * logits[i] = sum(hidden_states[j] * lm_head_weights[i][j])
 */
__global__ static void
ndb_cuda_hf_compute_logits_kernel(const float *hidden_states,
	const float *lm_head_weights,
	float *logits,
	int embed_dim,
	int vocab_size)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= vocab_size)
		return;

	/* Compute dot product: hidden_states * lm_head_weights[tid] */
	float logit = 0.0f;
	int i;

	for (i = 0; i < embed_dim; i++)
	{
		int weight_idx = tid * embed_dim + i;
		logit += hidden_states[i] * lm_head_weights[weight_idx];
	}

	logits[tid] = logit;
}

/*======================================================================*/
/* Transformer Operations */
/*======================================================================*/

/*
 * Layer normalization kernel
 */
__global__ static void
ndb_cuda_hf_layer_norm_kernel(const float *input,
	const float *gamma,
	const float *beta,
	float *output,
	int seq_len,
	int embed_dim,
	float eps)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;
	int seq_idx = tid / embed_dim;
	int dim_idx = tid % embed_dim;

	if (seq_idx >= seq_len || dim_idx >= embed_dim)
		return;

	/* Compute mean */
	float mean = 0.0f;
	int i;

	for (i = 0; i < embed_dim; i++)
		mean += input[seq_idx * embed_dim + i];
	mean /= (float)embed_dim;

	/* Compute variance */
	float variance = 0.0f;
	for (i = 0; i < embed_dim; i++)
	{
		float diff = input[seq_idx * embed_dim + i] - mean;
		variance += diff * diff;
	}
	variance /= (float)embed_dim;

	/* Normalize */
	float std = sqrtf(variance + eps);
	float normalized = (input[seq_idx * embed_dim + dim_idx] - mean) / std;

	/* Apply gamma and beta */
	output[seq_idx * embed_dim + dim_idx] =
		gamma[dim_idx] * normalized + beta[dim_idx];
}

/*
 * GELU activation kernel
 */
__global__ static void
ndb_cuda_hf_gelu_kernel(const float *input, float *output, int n)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= n)
		return;

	float x = input[tid];
	/* GELU approximation: x * 0.5 * (1 + tanh(sqrt(2/π) * (x + 0.044715 * x^3))) */
	float c1 = 0.7978845608f; /* sqrt(2/π) */
	float c2 = 0.044715f;
	float x3 = x * x * x;
	float tanh_arg = c1 * (x + c2 * x3);
	float tanh_val = tanhf(tanh_arg);

	output[tid] = x * 0.5f * (1.0f + tanh_val);
}

/*
 * Softmax kernel
 */
__global__ static void __attribute__((unused))
ndb_cuda_hf_softmax_kernel(const float *input,
	float *output,
	int seq_len,
	int head_dim)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;
	int head_idx = tid / head_dim;
	int dim_idx = tid % head_dim;

	if (head_idx >= seq_len || dim_idx >= head_dim)
		return;

	/* Find max for numerical stability */
	float max_val = input[head_idx * head_dim];
	int i;

	for (i = 1; i < head_dim; i++)
	{
		if (input[head_idx * head_dim + i] > max_val)
			max_val = input[head_idx * head_dim + i];
	}

	/* Compute exp and sum */
	float sum = 0.0f;
	for (i = 0; i < head_dim; i++)
	{
		float exp_val = expf(input[head_idx * head_dim + i] - max_val);
		if (i == dim_idx)
			output[head_idx * head_dim + dim_idx] = exp_val;
		sum += exp_val;
	}

	/* Normalize */
	output[head_idx * head_dim + dim_idx] /= sum;
}

/*======================================================================*/
/* Main Inference Functions */
/*======================================================================*/

/*
 * ndb_cuda_hf_embed_inference
 *	  Generate embeddings using CUDA-accelerated transformer model
 *
 * This is a simplified implementation that demonstrates the pattern.
 * Full implementation would:
 * 1. Load model weights to GPU memory
 * 2. Tokenize input text
 * 3. Run full transformer model (multiple layers)
 * 4. Extract embeddings (CLS token or mean pooling)
 */
extern "C" int
ndb_cuda_hf_embed_inference(const char *model_name,
	const int32_t *token_ids,
	const int32_t *attention_mask,
	int seq_len,
	const float *embedding_table,
	int vocab_size,
	int embed_dim,
	float *output_embedding,
	char **errstr)
{
	cudaError_t status;
	int32_t *d_token_ids = NULL;
	int32_t *d_attention_mask = NULL;
	float *d_embedding_table = NULL;
	float *d_embeddings = NULL;
	float *d_pooled = NULL;
	size_t token_bytes;
	size_t mask_bytes;
	size_t embed_table_bytes;
	size_t embeddings_bytes;
	size_t pooled_bytes;
	int threads;
	int blocks;

	if (errstr)
		*errstr = NULL;
	if (!model_name || !token_ids || !attention_mask || !embedding_table
		|| !output_embedding)
	{
		if (errstr)
			*errstr =
				(char *)"invalid parameters for CUDA HF embed";
		return -1;
	}
	if (seq_len <= 0 || vocab_size <= 0 || embed_dim <= 0)
	{
		if (errstr)
			*errstr =
				(char *)"invalid dimensions for CUDA HF embed";
		return -1;
	}

	token_bytes = sizeof(int32_t) * seq_len;
	mask_bytes = sizeof(int32_t) * seq_len;
	embed_table_bytes = sizeof(float) * vocab_size * embed_dim;
	embeddings_bytes = sizeof(float) * seq_len * embed_dim;
	pooled_bytes = sizeof(float) * embed_dim;

	/* Allocate device memory */
	status = cudaMalloc((void **)&d_token_ids, token_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_attention_mask, mask_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_embedding_table, embed_table_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_embeddings, embeddings_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_pooled, pooled_bytes);
	if (status != cudaSuccess)
		goto error;

	/* Copy input data to device */
	status = cudaMemcpy(
		d_token_ids, token_ids, token_bytes, cudaMemcpyHostToDevice);
	if (status != cudaSuccess)
		goto error;
	status = cudaMemcpy(d_attention_mask,
		attention_mask,
		mask_bytes,
		cudaMemcpyHostToDevice);
	if (status != cudaSuccess)
		goto error;
	status = cudaMemcpy(d_embedding_table,
		embedding_table,
		embed_table_bytes,
		cudaMemcpyHostToDevice);
	if (status != cudaSuccess)
		goto error;

	/* Launch embedding lookup kernel */
	threads = GET_THREADS(seq_len * embed_dim);
	blocks = GET_BLOCKS(seq_len * embed_dim, threads);
	ndb_cuda_hf_embedding_lookup_kernel<<<blocks, threads>>>(
		d_embedding_table,
		d_token_ids,
		d_attention_mask,
		d_embeddings,
		seq_len,
		embed_dim,
		vocab_size);
	status = cudaGetLastError();
	if (status != cudaSuccess)
		goto error;
	status = cudaDeviceSynchronize();
	if (status != cudaSuccess)
		goto error;

	/* Mean pooling */
	threads = GET_THREADS(embed_dim);
	blocks = GET_BLOCKS(embed_dim, threads);
	ndb_cuda_hf_mean_pooling_kernel<<<blocks, threads>>>(
		d_embeddings, d_attention_mask, d_pooled, seq_len, embed_dim);
	status = cudaGetLastError();
	if (status != cudaSuccess)
		goto error;
	status = cudaDeviceSynchronize();
	if (status != cudaSuccess)
		goto error;

	/* Copy result back to host */
	status = cudaMemcpy(output_embedding,
		d_pooled,
		pooled_bytes,
		cudaMemcpyDeviceToHost);
	if (status != cudaSuccess)
		goto error;

	/* Cleanup */
	cudaFree(d_token_ids);
	cudaFree(d_attention_mask);
	cudaFree(d_embedding_table);
	cudaFree(d_embeddings);
	cudaFree(d_pooled);

	return 0;

error:
	if (d_token_ids)
		cudaFree(d_token_ids);
	if (d_attention_mask)
		cudaFree(d_attention_mask);
	if (d_embedding_table)
		cudaFree(d_embedding_table);
	if (d_embeddings)
		cudaFree(d_embeddings);
	if (d_pooled)
		cudaFree(d_pooled);
	if (errstr && !*errstr)
	{
		const char *err_msg = cudaGetErrorString(status);
		size_t len = strlen(err_msg) + 1;
		char *err = (char *)malloc(len);
		if (err)
		{
			memcpy(err, err_msg, len);
			*errstr = err;
		}
	}
	return -1;
}

/*======================================================================*/
/* Text Generation Kernels */
/*======================================================================*/

/*
 * Causal attention kernel with KV cache for autoregressive generation
 * Computes attention scores for a single query position using cached keys/values
 * Optimized version using shared memory for better performance
 */
__global__ static void
ndb_cuda_hf_causal_attention_kernel(const float *query,
	const float *key_cache,
	const float *value_cache,
	float *output,
	int current_pos,
	int cache_pos,
	int num_heads,
	int head_dim,
	float scale)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;
	int head_idx = tid / head_dim;
	int dim_idx = tid % head_dim;

	if (head_idx >= num_heads || dim_idx >= head_dim)
		return;

	/* Compute attention scores */
	float max_score = -1e30f;
	float scores[512]; /* Max cache position */
	int i;

	/* Find max score for numerical stability */
	for (i = 0; i <= cache_pos && i <= current_pos; i++)
	{
		float score = 0.0f;
		int j;

		/* Compute dot product: query * key */
		for (j = 0; j < head_dim; j++)
		{
			int query_idx = head_idx * head_dim + j;
			int key_idx = i * num_heads * head_dim
				+ head_idx * head_dim + j;
			score += query[query_idx] * key_cache[key_idx];
		}
		score *= scale;

		scores[i] = score;
		if (score > max_score)
			max_score = score;
	}

	/* Compute softmax */
	float exp_sum = 0.0f;
	for (i = 0; i <= cache_pos && i <= current_pos; i++)
	{
		scores[i] = expf(scores[i] - max_score);
		exp_sum += scores[i];
	}

	/* Compute weighted sum of values */
	float output_val = 0.0f;
	for (i = 0; i <= cache_pos && i <= current_pos; i++)
	{
		float weight = scores[i] / exp_sum;
		int value_idx = i * num_heads * head_dim + head_idx * head_dim
			+ dim_idx;
		output_val += weight * value_cache[value_idx];
	}

	output[head_idx * head_dim + dim_idx] = output_val;
}

/*
 * Compute Q, K, V from hidden states using cuBLAS
 * Q = hidden_states * query_weights^T
 * K = hidden_states * key_weights^T
 * V = hidden_states * value_weights^T
 * hidden_states: [embed_dim]
 * query_weights: [num_heads * head_dim, embed_dim] (column-major)
 * q, k, v: [num_heads * head_dim]
 */
static int
ndb_cuda_hf_compute_qkv(const float *hidden_states,
	const float *query_weights,
	const float *key_weights,
	const float *value_weights,
	float *q,
	float *k,
	float *v,
	int embed_dim,
	int num_heads,
	int head_dim)
{
	int qkv_dim = num_heads * head_dim;
	int rc;

	/* Compute Q = hidden_states * query_weights^T */
	rc = ndb_cuda_matvec(
		query_weights, hidden_states, q, qkv_dim, embed_dim);
	if (rc != 0)
		return -1;

	/* Compute K = hidden_states * key_weights^T */
	rc = ndb_cuda_matvec(key_weights, hidden_states, k, qkv_dim, embed_dim);
	if (rc != 0)
		return -1;

	/* Compute V = hidden_states * value_weights^T */
	rc = ndb_cuda_matvec(
		value_weights, hidden_states, v, qkv_dim, embed_dim);
	if (rc != 0)
		return -1;

	return 0;
}

/*
 * Apply attention output projection using cuBLAS
 * output = attention_output * output_weights^T
 * attention_output: [embed_dim]
 * output_weights: [embed_dim, embed_dim] (column-major)
 * output: [embed_dim]
 */
static int
ndb_cuda_hf_attention_output_projection(const float *attention_output,
	const float *output_weights,
	float *output,
	int embed_dim)
{
	return ndb_cuda_matvec(
		output_weights, attention_output, output, embed_dim, embed_dim);
}

/*
 * Apply feed-forward network using cuBLAS
 * FFN(x) = GELU(x * W1^T + b1) * W2^T + b2
 * input: [embed_dim]
 * ffn_weights1: [hidden_dim, embed_dim] (column-major)
 * ffn_weights2: [embed_dim, hidden_dim] (column-major)
 * output: [embed_dim]
 */
static int
ndb_cuda_hf_ffn_forward(const float *input,
	const float *ffn_weights1,
	const float *ffn_weights2,
	float *hidden,
	float *output,
	int embed_dim,
	int hidden_dim)
{
	int rc;
	float *d_gelu_input = NULL;
	cudaError_t status;

	/* Allocate temporary buffer for GELU input */
	status = cudaMalloc((void **)&d_gelu_input, sizeof(float) * hidden_dim);
	if (status != cudaSuccess)
		return -1;

	/* First layer: input * W1^T */
	rc = ndb_cuda_matvec(
		ffn_weights1, input, d_gelu_input, hidden_dim, embed_dim);
	if (rc != 0)
	{
		cudaFree(d_gelu_input);
		return -1;
	}

	/* Apply GELU activation */
	{
		int threads = GET_THREADS(hidden_dim);
		int blocks = GET_BLOCKS(hidden_dim, threads);

		ndb_cuda_hf_gelu_kernel<<<blocks, threads>>>(
			d_gelu_input, hidden, hidden_dim);
		status = cudaGetLastError();
		if (status != cudaSuccess)
		{
			cudaFree(d_gelu_input);
			return -1;
		}
		status = cudaDeviceSynchronize();
		if (status != cudaSuccess)
		{
			cudaFree(d_gelu_input);
			return -1;
		}
	}

	/* Second layer: hidden * W2^T */
	rc = ndb_cuda_matvec(
		ffn_weights2, hidden, output, embed_dim, hidden_dim);
	if (rc != 0)
	{
		cudaFree(d_gelu_input);
		return -1;
	}

	cudaFree(d_gelu_input);
	return 0;
}

/*
 * Feed-forward network kernel: Apply FFN with GELU activation
 * FFN(x) = GELU(x * W1 + b1) * W2 + b2
 */
__global__ static void __attribute__((unused))
ndb_cuda_hf_ffn_kernel(const float *input,
	const float *ffn_weights1,
	const float *ffn_weights2,
	float *output,
	int embed_dim,
	int hidden_dim)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= embed_dim)
		return;

	/* Compute FFN output (simplified - should use cuBLAS for matrix multiplication) */
	float hidden_val = 0.0f;
	int i;

	/* First layer: input * W1 */
	for (i = 0; i < embed_dim; i++)
	{
		int weight_idx = i * hidden_dim + tid;
		hidden_val += input[i] * ffn_weights1[weight_idx];
	}

	/* Apply GELU activation */
	float x = hidden_val;
	float c1 = 0.7978845608f; /* sqrt(2/π) */
	float c2 = 0.044715f;
	float x3 = x * x * x;
	float tanh_arg = c1 * (x + c2 * x3);
	float tanh_val = tanhf(tanh_arg);
	float gelu_val = x * 0.5f * (1.0f + tanh_val);

	/* Second layer: gelu_val * W2 */
	float output_val = 0.0f;
	for (i = 0; i < hidden_dim; i++)
	{
		int weight_idx = i * embed_dim + tid;
		output_val += gelu_val * ffn_weights2[weight_idx];
	}

	output[tid] = output_val;
}

/*
 * Update KV cache kernel: Append new key/value to cache
 */
__global__ static void __attribute__((unused))
ndb_cuda_hf_update_kv_cache_kernel(const float *new_key,
	const float *new_value,
	float *key_cache,
	float *value_cache,
	int cache_pos,
	int num_heads,
	int head_dim)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;
	int head_idx = tid / head_dim;
	int dim_idx = tid % head_dim;

	if (head_idx >= num_heads || dim_idx >= head_dim)
		return;

	/* Copy new key/value to cache at cache_pos */
	int cache_key_idx = cache_pos * num_heads * head_dim
		+ head_idx * head_dim + dim_idx;
	int cache_value_idx = cache_pos * num_heads * head_dim
		+ head_idx * head_dim + dim_idx;
	int new_key_idx = head_idx * head_dim + dim_idx;
	int new_value_idx = head_idx * head_dim + dim_idx;

	key_cache[cache_key_idx] = new_key[new_key_idx];
	value_cache[cache_value_idx] = new_value[new_value_idx];
}

/*
 * Logit bias kernel: Apply bias values to specific token logits
 */
__global__ static void
ndb_cuda_hf_apply_logit_bias_kernel(float *logits,
	const int32_t *bias_tokens,
	const float *bias_values,
	int num_biases,
	int vocab_size)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= vocab_size)
		return;

	/* Check if this token ID has a bias value */
	for (int i = 0; i < num_biases; i++)
	{
		if (bias_tokens[i] == tid)
		{
			logits[tid] += bias_values[i];
			break;
		}
	}
}

/*
 * Temperature scaling kernel: Scale logits by temperature
 */
__global__ static void
ndb_cuda_hf_temperature_scale_kernel(float *logits,
	float temperature,
	int vocab_size)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= vocab_size)
		return;

	if (temperature > 0.0f && temperature != 1.0f)
		logits[tid] /= temperature;
}

/*
 * Find top-k indices kernel: Find indices of top-k largest values
 * Uses a simple approach: each thread checks if its value is in top-k
 * For better performance, this should use shared memory and reduction
 */
__global__ static void
ndb_cuda_hf_find_top_k_kernel(const float *logits,
	int *top_k_indices,
	float *top_k_values,
	int top_k,
	int vocab_size)
{
	/* Simple implementation: find top-k using shared memory reduction */
	/* This is a simplified version - full implementation would use */
	/* parallel reduction with shared memory for better performance */

	/* For now, use a simple approach: each thread finds its rank */
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= vocab_size)
		return;

	float value = logits[tid];
	int rank = 0;
	int i;

	/* Count how many values are greater than this value */
	for (i = 0; i < vocab_size; i++)
	{
		if (logits[i] > value || (logits[i] == value && i < tid))
			rank++;
	}

	/* If this value is in top-k, store it */
	if (rank < top_k)
	{
		top_k_indices[rank] = tid;
		top_k_values[rank] = value;
	}
}

/*
 * Top-k filtering kernel: Set logits outside top-k to -inf
 */
__global__ static void
ndb_cuda_hf_top_k_filter_kernel(float *logits,
	const int *top_k_indices,
	int top_k,
	int vocab_size)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= vocab_size)
		return;

	/* Check if this token is in top-k */
	bool in_top_k = false;
	int i;

	for (i = 0; i < top_k; i++)
	{
		if (top_k_indices[i] == tid)
		{
			in_top_k = true;
			break;
		}
	}

	/* Set to -inf if not in top-k */
	if (!in_top_k)
		logits[tid] = -1e30f;
}

/*
 * Top-p (nucleus) sampling kernel: Set logits outside top-p cumulative probability to -inf
 * This kernel computes cumulative probability and filters logits
 */
__global__ static void
ndb_cuda_hf_top_p_filter_kernel(float *logits,
	const float *sorted_logits,
	const int *sorted_indices,
	float top_p,
	int vocab_size)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= vocab_size)
		return;

	/* Find max logit for numerical stability */
	float max_logit = sorted_logits[0];
	float cumsum = 0.0f;
	int cutoff = vocab_size;
	int i;

	/* Compute cumulative probability */
	for (i = 0; i < vocab_size; i++)
	{
		float exp_val = expf(sorted_logits[i] - max_logit);
		cumsum += exp_val;

		if (cumsum >= top_p)
		{
			cutoff = i + 1;
			break;
		}
	}

	/* Check if this token is in top-p */
	bool in_top_p = false;
	for (i = 0; i < cutoff; i++)
	{
		if (sorted_indices[i] == tid)
		{
			in_top_p = true;
			break;
		}
	}

	/* Set to -inf if not in top-p */
	if (!in_top_p)
		logits[tid] = -1e30f;
}

/*
 * Sort logits and indices kernel: Sort logits in descending order
 * Uses a simple bubble sort (inefficient but correct)
 * For better performance, this should use bitonic sort or radix sort
 */
__global__ static void
ndb_cuda_hf_sort_logits_kernel(const float *logits,
	float *sorted_logits,
	int *sorted_indices,
	int vocab_size)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= vocab_size)
		return;

	/* Copy logits and indices to shared memory if possible */
	/* For now, use global memory */

	/* Initialize sorted arrays */
	sorted_logits[tid] = logits[tid];
	sorted_indices[tid] = tid;

	/* Simple bubble sort (inefficient but correct) */
	/* TODO: Use bitonic sort or radix sort for better performance */
	int i;
	int j;

	for (i = 0; i < vocab_size - 1; i++)
	{
		for (j = 0; j < vocab_size - i - 1; j++)
		{
			if (sorted_logits[j] < sorted_logits[j + 1])
			{
				/* Swap logits */
				float temp_logit = sorted_logits[j];
				sorted_logits[j] = sorted_logits[j + 1];
				sorted_logits[j + 1] = temp_logit;

				/* Swap indices */
				int temp_idx = sorted_indices[j];
				sorted_indices[j] = sorted_indices[j + 1];
				sorted_indices[j + 1] = temp_idx;
			}
		}
	}
}

/*
 * Repetition penalty kernel: Penalize repeated tokens
 */
__global__ static void
ndb_cuda_hf_repetition_penalty_kernel(float *logits,
	const int32_t *generated_tokens,
	int num_generated,
	float penalty,
	int vocab_size)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= vocab_size)
		return;

	/* Check if this token appears in generated sequence */
	bool is_repeated = false;
	int i;

	for (i = 0; i < num_generated; i++)
	{
		if (generated_tokens[i] == tid)
		{
			is_repeated = true;
			break;
		}
	}

	/* Apply penalty */
	if (is_repeated && penalty > 1.0f)
	{
		if (logits[tid] > 0.0f)
			logits[tid] /= penalty;
		else
			logits[tid] *= penalty;
	}
}

/*
 * Greedy sampling kernel: Select token with highest logit
 */
__global__ static void
ndb_cuda_hf_greedy_sample_kernel(const float *logits,
	int32_t *output_token,
	int vocab_size)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid != 0)
		return;

	/* Find token with maximum logit */
	float max_logit = logits[0];
	int max_idx = 0;
	int i;

	for (i = 1; i < vocab_size; i++)
	{
		if (logits[i] > max_logit)
		{
			max_logit = logits[i];
			max_idx = i;
		}
	}

	output_token[0] = max_idx;
}

/*
 * Multinomial sampling kernel: Sample token from probability distribution
 * Uses inverse CDF sampling with uniform random numbers
 */
__global__ static void
ndb_cuda_hf_multinomial_sample_kernel(const float *logits,
	float *probs,
	float *cumsum,
	float random_value,
	int32_t *output_token,
	int vocab_size)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid >= vocab_size)
		return;

	/* Compute probabilities from logits */
	float max_logit = logits[0];
	int i;

	/* Find max for numerical stability */
	for (i = 1; i < vocab_size; i++)
	{
		if (logits[i] > max_logit)
			max_logit = logits[i];
	}

	/* Compute exp and probabilities */
	float exp_sum = 0.0f;
	for (i = 0; i < vocab_size; i++)
	{
		float exp_val = expf(logits[i] - max_logit);
		probs[i] = exp_val;
		exp_sum += exp_val;
	}

	/* Normalize */
	for (i = 0; i < vocab_size; i++)
		probs[i] /= exp_sum;

	/* Compute cumulative sum */
	float cum = 0.0f;
	for (i = 0; i < vocab_size; i++)
	{
		cum += probs[i];
		cumsum[i] = cum;
	}

	/* Sample using inverse CDF */
	if (tid == 0)
	{
		for (i = 0; i < vocab_size; i++)
		{
			if (random_value <= cumsum[i])
			{
				output_token[0] = i;
				return;
			}
		}
		output_token[0] = vocab_size - 1;
	}
}

/*
 * Stop sequence detection kernel: Check if generated sequence contains stop sequence
 */
__global__ static void
ndb_cuda_hf_check_stop_sequence_kernel(const int32_t *generated_tokens,
	int num_generated,
	const int32_t *stop_sequence,
	int stop_seq_len,
	bool *found_stop)
{
	int tid = blockIdx.x * blockDim.x + threadIdx.x;

	if (tid != 0)
		return;

	if (num_generated < stop_seq_len)
	{
		found_stop[0] = false;
		return;
	}

	/* Check if stop sequence appears at end of generated sequence */
	bool matches = true;
	int i;

	for (i = 0; i < stop_seq_len; i++)
	{
		int gen_idx = num_generated - stop_seq_len + i;
		if (generated_tokens[gen_idx] != stop_sequence[i])
		{
			matches = false;
			break;
		}
	}

	found_stop[0] = matches;
}

/*======================================================================*/
/* Text Generation Inference Function */
/*======================================================================*/

/*
 * ndb_cuda_hf_generate_inference
 *	  Generate text using CUDA-accelerated autoregressive transformer model
 *
 * This implements a complete autoregressive text generation pipeline:
 * 1. Tokenize input prompt
 * 2. Initialize KV cache with prompt tokens
 * 3. Autoregressively generate tokens:
 *    a. Compute attention with KV cache
 *    b. Apply transformer layers
 *    c. Compute logits from language model head
 *    d. Apply temperature, top-k, top-p, repetition penalty
 *    e. Sample next token
 *    f. Update KV cache
 *    g. Check for stop sequences
 * 4. Decode tokens to text
 */
extern "C" int
ndb_cuda_hf_generate_inference(const char *model_name,
	const int32_t *input_token_ids,
	int input_seq_len,
	const float *embedding_table,
	const float *position_embeddings,
	const float *lm_head_weights,
	const NdbCudaHfModelWeights *weights,
	const NdbCudaHfModelConfig *config,
	const NdbCudaHfGenParams *gen_params,
	NdbCudaHfKVCache *kv_cache,
	int32_t *output_token_ids,
	int *output_seq_len,
	char **errstr)
{
	cudaError_t status;
	float *d_embeddings = NULL;
	float *d_logits = NULL;
	float *d_probs = NULL;
	float *d_cumsum = NULL;
	int32_t *d_input_tokens = NULL;
	int32_t *d_output_tokens = NULL;
	float *d_key_cache = NULL;
	float *d_value_cache = NULL;
	bool *d_found_stop = NULL;
	int *d_top_k_indices = NULL;
	float *d_sorted_logits = NULL;
	int *d_sorted_indices = NULL;
	size_t embed_bytes;
	size_t logit_bytes;
	size_t token_bytes;
	size_t cache_bytes;
	int threads;
	int blocks;
	int i;
	int current_pos;
	int generated_count = 0;
	int max_gen_tokens;
	bool stop_found = false;
	curandState *d_curand_state = NULL;

	if (errstr)
		*errstr = NULL;
	if (!model_name || !input_token_ids || !embedding_table
		|| !lm_head_weights || !weights || !config || !gen_params
		|| !output_token_ids || !output_seq_len)
	{
		if (errstr)
			*errstr = (char *)"invalid parameters for CUDA HF "
					  "generate";
		return -1;
	}
	if (input_seq_len <= 0 || config->vocab_size <= 0
		|| config->embed_dim <= 0)
	{
		if (errstr)
			*errstr = (char *)"invalid dimensions for CUDA HF "
					  "generate";
		return -1;
	}

	max_gen_tokens = (gen_params->max_tokens > 0) ? gen_params->max_tokens
						      : NDB_HF_MAX_GEN_TOKENS;
	if (max_gen_tokens > NDB_HF_MAX_GEN_TOKENS)
		max_gen_tokens = NDB_HF_MAX_GEN_TOKENS;

	/* Allocate device memory */
	embed_bytes = sizeof(float) * config->embed_dim;
	logit_bytes = sizeof(float) * config->vocab_size;
	token_bytes = sizeof(int32_t) * (input_seq_len + max_gen_tokens);
	cache_bytes = sizeof(float) * config->num_layers * config->max_seq_len
		* config->num_heads * (config->embed_dim / config->num_heads);

	status = cudaMalloc((void **)&d_embeddings, embed_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_logits, logit_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_probs, logit_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_cumsum, logit_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_input_tokens, token_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_output_tokens, token_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_key_cache, cache_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_value_cache, cache_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_found_stop, sizeof(bool));
	if (status != cudaSuccess)
		goto error;

	/* Copy input tokens to device */
	status = cudaMemcpy(d_input_tokens,
		input_token_ids,
		sizeof(int32_t) * input_seq_len,
		cudaMemcpyHostToDevice);
	if (status != cudaSuccess)
		goto error;

	/* Initialize KV cache with prompt tokens */
	/* TODO: This is a simplified version - full implementation would run
	 *       transformer layers on prompt tokens to populate KV cache
	 */
	current_pos = input_seq_len - 1;

	/* Initialize logits buffer on device (zero-initialized) */
	status = cudaMemset(d_logits, 0, logit_bytes);
	if (status != cudaSuccess)
		goto error;

	/* Autoregressive generation loop */
	for (i = 0; i < max_gen_tokens && !stop_found; i++)
	{
		int32_t next_token;
		int32_t h_next_token;
		int32_t current_token_id;
		float *d_current_embedding = NULL;
		float *d_hidden_states = NULL;
		float *d_attention_output = NULL;
		float *d_ffn_output = NULL;
		size_t hidden_bytes;
		size_t attn_output_bytes;
		size_t ffn_output_bytes;

		/* Get current token ID */
		if (i == 0)
		{
			/* First iteration: use last token from input */
			if (input_seq_len > 0)
				current_token_id =
					input_token_ids[input_seq_len - 1];
			else
				current_token_id = 101; /* [CLS] token */
		} else
		{
			/* Subsequent iterations: use last generated token */
			current_token_id =
				output_token_ids[generated_count - 1];
		}

		/* Allocate temporary buffers for transformer forward pass */
		hidden_bytes = sizeof(float) * config->embed_dim;
		attn_output_bytes = sizeof(float) * config->embed_dim;
		ffn_output_bytes = sizeof(float) * config->hidden_dim;

		status =
			cudaMalloc((void **)&d_current_embedding, hidden_bytes);
		if (status != cudaSuccess)
		{
			generated_count =
				i; /* Record how many tokens were generated */
			break; /* Break out of loop */
		}
		status = cudaMalloc((void **)&d_hidden_states, hidden_bytes);
		if (status != cudaSuccess)
		{
			cudaFree(d_current_embedding);
			generated_count = i;
			break;
		}
		status = cudaMalloc(
			(void **)&d_attention_output, attn_output_bytes);
		if (status != cudaSuccess)
		{
			cudaFree(d_current_embedding);
			cudaFree(d_hidden_states);
			generated_count = i;
			break;
		}
		status = cudaMalloc((void **)&d_ffn_output, ffn_output_bytes);
		if (status != cudaSuccess)
		{
			cudaFree(d_current_embedding);
			cudaFree(d_hidden_states);
			cudaFree(d_attention_output);
			generated_count = i;
			break;
		}

		/* Step 1: Get token embedding using device-side kernel */
		threads = GET_THREADS(config->embed_dim);
		blocks = GET_BLOCKS(config->embed_dim, threads);

		/* Launch single token embedding lookup kernel */
		ndb_cuda_hf_single_embedding_lookup_kernel<<<blocks, threads>>>(
			embedding_table,
			current_token_id,
			d_current_embedding,
			config->embed_dim,
			config->vocab_size);
		status = cudaGetLastError();
		if (status != cudaSuccess)
		{
			cudaFree(d_current_embedding);
			cudaFree(d_hidden_states);
			cudaFree(d_attention_output);
			cudaFree(d_ffn_output);
			generated_count = i;
			break;
		}
		status = cudaDeviceSynchronize();
		if (status != cudaSuccess)
		{
			cudaFree(d_current_embedding);
			cudaFree(d_hidden_states);
			cudaFree(d_attention_output);
			cudaFree(d_ffn_output);
			generated_count = i;
			break;
		}

		/* Step 2: Add position embedding using device-side kernel */
		if (position_embeddings != NULL)
		{
			int pos = input_seq_len + generated_count;
			float *d_pos_embedding = NULL;

			if (pos >= 0 && pos < config->max_seq_len)
			{
				/* Allocate position embedding buffer */
				status = cudaMalloc((void **)&d_pos_embedding,
					sizeof(float) * config->embed_dim);
				if (status != cudaSuccess)
				{
					cudaFree(d_current_embedding);
					cudaFree(d_hidden_states);
					cudaFree(d_attention_output);
					cudaFree(d_ffn_output);
					generated_count = i;
					break;
				}

				/* Copy position embedding to device */
				status = cudaMemcpy(d_pos_embedding,
					position_embeddings
						+ pos * config->embed_dim,
					sizeof(float) * config->embed_dim,
					cudaMemcpyDeviceToDevice);
				if (status != cudaSuccess)
				{
					cudaFree(d_pos_embedding);
					cudaFree(d_current_embedding);
					cudaFree(d_hidden_states);
					cudaFree(d_attention_output);
					cudaFree(d_ffn_output);
					generated_count = i;
					break;
				}

				/* Add position embedding to token embedding */
				ndb_cuda_hf_add_position_embedding_kernel<<<
					blocks,
					threads>>>(d_current_embedding,
					d_pos_embedding,
					d_hidden_states,
					config->embed_dim);
				status = cudaGetLastError();
				if (status != cudaSuccess)
				{
					cudaFree(d_pos_embedding);
					cudaFree(d_current_embedding);
					cudaFree(d_hidden_states);
					cudaFree(d_attention_output);
					cudaFree(d_ffn_output);
					generated_count = i;
					break;
				}
				status = cudaDeviceSynchronize();
				if (status != cudaSuccess)
				{
					cudaFree(d_pos_embedding);
					cudaFree(d_current_embedding);
					cudaFree(d_hidden_states);
					cudaFree(d_attention_output);
					cudaFree(d_ffn_output);
					generated_count = i;
					break;
				}

				cudaFree(d_pos_embedding);
			} else
			{
				/* Position out of range, just copy embedding */
				status = cudaMemcpy(d_hidden_states,
					d_current_embedding,
					sizeof(float) * config->embed_dim,
					cudaMemcpyDeviceToDevice);
				if (status != cudaSuccess)
				{
					cudaFree(d_current_embedding);
					cudaFree(d_hidden_states);
					cudaFree(d_attention_output);
					cudaFree(d_ffn_output);
					generated_count = i;
					break;
				}
			}
		} else
		{
			/* No position embeddings, just copy embedding */
			status = cudaMemcpy(d_hidden_states,
				d_current_embedding,
				sizeof(float) * config->embed_dim,
				cudaMemcpyDeviceToDevice);
			if (status != cudaSuccess)
			{
				cudaFree(d_current_embedding);
				cudaFree(d_hidden_states);
				cudaFree(d_attention_output);
				cudaFree(d_ffn_output);
				generated_count = i;
				break;
			}
		}

		/* Step 3: Run through transformer layers */
		/* Run full transformer layer forward pass for each layer */
		{
			float *d_layer_input = d_hidden_states;
			float *d_layer_output = NULL;
			float *d_q = NULL;
			float *d_k = NULL;
			float *d_v = NULL;
			float *d_attn_output = NULL;
			float *d_ffn_input = NULL;
			float *d_ffn_output_temp = NULL;
			float *d_residual = NULL;
			int head_dim = config->embed_dim / config->num_heads;
			float scale = 1.0f / sqrtf((float)head_dim);
			int layer;

			/* Allocate buffers for transformer layers */
			size_t qkv_bytes =
				sizeof(float) * config->num_heads * head_dim;
			size_t attn_bytes = sizeof(float) * config->embed_dim;
			size_t ffn_bytes = sizeof(float) * config->hidden_dim;
			size_t layer_bytes = sizeof(float) * config->embed_dim;

			status = cudaMalloc(
				(void **)&d_layer_output, layer_bytes);
			if (status != cudaSuccess)
			{
				cudaFree(d_current_embedding);
				cudaFree(d_hidden_states);
				cudaFree(d_attention_output);
				cudaFree(d_ffn_output);
				generated_count = i;
				break;
			}
			status = cudaMalloc((void **)&d_q, qkv_bytes);
			if (status != cudaSuccess)
			{
				cudaFree(d_layer_output);
				cudaFree(d_current_embedding);
				cudaFree(d_hidden_states);
				cudaFree(d_attention_output);
				cudaFree(d_ffn_output);
				generated_count = i;
				break;
			}
			status = cudaMalloc((void **)&d_k, qkv_bytes);
			if (status != cudaSuccess)
			{
				cudaFree(d_q);
				cudaFree(d_layer_output);
				cudaFree(d_current_embedding);
				cudaFree(d_hidden_states);
				cudaFree(d_attention_output);
				cudaFree(d_ffn_output);
				generated_count = i;
				break;
			}
			status = cudaMalloc((void **)&d_v, qkv_bytes);
			if (status != cudaSuccess)
			{
				cudaFree(d_k);
				cudaFree(d_q);
				cudaFree(d_layer_output);
				cudaFree(d_current_embedding);
				cudaFree(d_hidden_states);
				cudaFree(d_attention_output);
				cudaFree(d_ffn_output);
				generated_count = i;
				break;
			}
			status =
				cudaMalloc((void **)&d_attn_output, attn_bytes);
			if (status != cudaSuccess)
			{
				cudaFree(d_v);
				cudaFree(d_k);
				cudaFree(d_q);
				cudaFree(d_layer_output);
				cudaFree(d_current_embedding);
				cudaFree(d_hidden_states);
				cudaFree(d_attention_output);
				cudaFree(d_ffn_output);
				generated_count = i;
				break;
			}
			status = cudaMalloc((void **)&d_ffn_input, layer_bytes);
			if (status != cudaSuccess)
			{
				cudaFree(d_attn_output);
				cudaFree(d_v);
				cudaFree(d_k);
				cudaFree(d_q);
				cudaFree(d_layer_output);
				cudaFree(d_current_embedding);
				cudaFree(d_hidden_states);
				cudaFree(d_attention_output);
				cudaFree(d_ffn_output);
				generated_count = i;
				break;
			}
			status = cudaMalloc(
				(void **)&d_ffn_output_temp, ffn_bytes);
			if (status != cudaSuccess)
			{
				cudaFree(d_ffn_input);
				cudaFree(d_attn_output);
				cudaFree(d_v);
				cudaFree(d_k);
				cudaFree(d_q);
				cudaFree(d_layer_output);
				cudaFree(d_current_embedding);
				cudaFree(d_hidden_states);
				cudaFree(d_attention_output);
				cudaFree(d_ffn_output);
				generated_count = i;
				break;
			}
			status = cudaMalloc((void **)&d_residual, layer_bytes);
			if (status != cudaSuccess)
			{
				cudaFree(d_ffn_output_temp);
				cudaFree(d_ffn_input);
				cudaFree(d_attn_output);
				cudaFree(d_v);
				cudaFree(d_k);
				cudaFree(d_q);
				cudaFree(d_layer_output);
				cudaFree(d_current_embedding);
				cudaFree(d_hidden_states);
				cudaFree(d_attention_output);
				cudaFree(d_ffn_output);
				generated_count = i;
				break;
			}

			/* Run through each transformer layer */
			for (layer = 0; layer < config->num_layers; layer++)
			{
				float *d_layer_norm_input = NULL;
				float *d_layer_norm_output = NULL;
				int rc;

				/* Store residual connection */
				status = cudaMemcpy(d_residual,
					d_layer_input,
					sizeof(float) * config->embed_dim,
					cudaMemcpyDeviceToDevice);
				if (status != cudaSuccess)
					break;

				/* Pre-attention layer norm */
				/* Allocate layer norm buffers */
				status = cudaMalloc(
					(void **)&d_layer_norm_input,
					sizeof(float) * config->embed_dim);
				if (status != cudaSuccess)
					break;
				status = cudaMalloc(
					(void **)&d_layer_norm_output,
					sizeof(float) * config->embed_dim);
				if (status != cudaSuccess)
				{
					cudaFree(d_layer_norm_input);
					break;
				}

				/* Copy input to layer norm buffer */
				status = cudaMemcpy(d_layer_norm_input,
					d_layer_input,
					sizeof(float) * config->embed_dim,
					cudaMemcpyDeviceToDevice);
				if (status != cudaSuccess)
				{
					cudaFree(d_layer_norm_output);
					cudaFree(d_layer_norm_input);
					break;
				}

				/* Apply layer norm */
				if (weights->layer_norm_gamma != NULL
					&& weights->layer_norm_beta != NULL)
				{
					threads =
						GET_THREADS(config->embed_dim);
					blocks = GET_BLOCKS(
						config->embed_dim, threads);

					ndb_cuda_hf_layer_norm_kernel<<<blocks,
						threads>>>(d_layer_norm_input,
						weights->layer_norm_gamma,
						weights->layer_norm_beta,
						d_layer_norm_output,
						1, /* seq_len = 1 for single token */
						config->embed_dim,
						1e-5f); /* eps */
					status = cudaGetLastError();
					if (status != cudaSuccess)
					{
						cudaFree(d_layer_norm_output);
						cudaFree(d_layer_norm_input);
						break;
					}
					status = cudaDeviceSynchronize();
					if (status != cudaSuccess)
					{
						cudaFree(d_layer_norm_output);
						cudaFree(d_layer_norm_input);
						break;
					}
				} else
				{
					/* No layer norm weights, just copy */
					status = cudaMemcpy(d_layer_norm_output,
						d_layer_norm_input,
						sizeof(float)
							* config->embed_dim,
						cudaMemcpyDeviceToDevice);
					if (status != cudaSuccess)
					{
						cudaFree(d_layer_norm_output);
						cudaFree(d_layer_norm_input);
						break;
					}
				}

				/* Compute Q, K, V using cuBLAS */
				/* Get layer weights (simplified - should use proper weight indexing) */
				const float *layer_query_weights =
					weights->query_weights;
				const float *layer_key_weights =
					weights->key_weights;
				const float *layer_value_weights =
					weights->value_weights;

				/* Compute Q, K, V */
				rc = ndb_cuda_hf_compute_qkv(
					d_layer_norm_output,
					layer_query_weights,
					layer_key_weights,
					layer_value_weights,
					d_q,
					d_k,
					d_v,
					config->embed_dim,
					config->num_heads,
					head_dim);
				if (rc != 0)
				{
					cudaFree(d_layer_norm_output);
					cudaFree(d_layer_norm_input);
					break;
				}

				/* Update KV cache with new K, V */
				if (kv_cache != NULL && kv_cache->allocated)
				{
					int cache_offset = layer
							* config->max_seq_len
							* config->num_heads
							* head_dim
						+ current_pos
							* config->num_heads
							* head_dim;
					float *layer_key_cache =
						kv_cache->key_cache
						+ cache_offset;
					float *layer_value_cache =
						kv_cache->value_cache
						+ cache_offset;

					/* Copy K, V to cache */
					status = cudaMemcpy(layer_key_cache,
						d_k,
						sizeof(float)
							* config->num_heads
							* head_dim,
						cudaMemcpyDeviceToDevice);
					if (status != cudaSuccess)
					{
						cudaFree(d_layer_norm_output);
						cudaFree(d_layer_norm_input);
						break;
					}
					status = cudaMemcpy(layer_value_cache,
						d_v,
						sizeof(float)
							* config->num_heads
							* head_dim,
						cudaMemcpyDeviceToDevice);
					if (status != cudaSuccess)
					{
						cudaFree(d_layer_norm_output);
						cudaFree(d_layer_norm_input);
						break;
					}
				}

				/* Compute attention with KV cache */
				threads = GET_THREADS(
					config->num_heads * head_dim);
				blocks = GET_BLOCKS(
					config->num_heads * head_dim, threads);

				ndb_cuda_hf_causal_attention_kernel<<<blocks,
					threads>>>(d_q,
					(kv_cache && kv_cache->allocated)
						? kv_cache->key_cache
							+ layer * config->max_seq_len
								* config->num_heads
								* head_dim
						: d_k,
					(kv_cache && kv_cache->allocated)
						? kv_cache->value_cache
							+ layer * config->max_seq_len
								* config->num_heads
								* head_dim
						: d_v,
					d_attn_output,
					current_pos,
					current_pos,
					config->num_heads,
					head_dim,
					scale);
				status = cudaGetLastError();
				if (status != cudaSuccess)
				{
					cudaFree(d_layer_norm_output);
					cudaFree(d_layer_norm_input);
					break;
				}
				status = cudaDeviceSynchronize();
				if (status != cudaSuccess)
				{
					cudaFree(d_layer_norm_output);
					cudaFree(d_layer_norm_input);
					break;
				}

				/* Apply attention output projection using cuBLAS */
				if (weights->output_weights != NULL)
				{
					float *d_attn_proj = NULL;

					status = cudaMalloc(
						(void **)&d_attn_proj,
						sizeof(float)
							* config->embed_dim);
					if (status != cudaSuccess)
					{
						cudaFree(d_layer_norm_output);
						cudaFree(d_layer_norm_input);
						break;
					}

					rc = ndb_cuda_hf_attention_output_projection(
						d_attn_output,
						weights->output_weights,
						d_attn_proj,
						config->embed_dim);
					if (rc != 0)
					{
						cudaFree(d_attn_proj);
						cudaFree(d_layer_norm_output);
						cudaFree(d_layer_norm_input);
						break;
					}

					/* Copy to attention output */
					status = cudaMemcpy(d_attn_output,
						d_attn_proj,
						sizeof(float)
							* config->embed_dim,
						cudaMemcpyDeviceToDevice);
					cudaFree(d_attn_proj);
					if (status != cudaSuccess)
					{
						cudaFree(d_layer_norm_output);
						cudaFree(d_layer_norm_input);
						break;
					}
				}

				/* Add residual connection */
				threads = GET_THREADS(config->embed_dim);
				blocks = GET_BLOCKS(config->embed_dim, threads);

				ndb_cuda_hf_add_vectors_kernel<<<blocks,
					threads>>>(d_residual,
					d_attn_output,
					d_ffn_input,
					config->embed_dim);
				status = cudaGetLastError();
				if (status != cudaSuccess)
				{
					cudaFree(d_layer_norm_output);
					cudaFree(d_layer_norm_input);
					break;
				}
				status = cudaDeviceSynchronize();
				if (status != cudaSuccess)
				{
					cudaFree(d_layer_norm_output);
					cudaFree(d_layer_norm_input);
					break;
				}

				/* Pre-FFN layer norm */
				/* Copy FFN input to layer norm buffer */
				status = cudaMemcpy(d_layer_norm_input,
					d_ffn_input,
					sizeof(float) * config->embed_dim,
					cudaMemcpyDeviceToDevice);
				if (status != cudaSuccess)
				{
					cudaFree(d_layer_norm_output);
					cudaFree(d_layer_norm_input);
					break;
				}

				/* Apply layer norm */
				if (weights->layer_norm_gamma != NULL
					&& weights->layer_norm_beta != NULL)
				{
					ndb_cuda_hf_layer_norm_kernel<<<blocks,
						threads>>>(d_layer_norm_input,
						weights->layer_norm_gamma,
						weights->layer_norm_beta,
						d_layer_norm_output,
						1, /* seq_len = 1 for single token */
						config->embed_dim,
						1e-5f); /* eps */
					status = cudaGetLastError();
					if (status != cudaSuccess)
					{
						cudaFree(d_layer_norm_output);
						cudaFree(d_layer_norm_input);
						break;
					}
					status = cudaDeviceSynchronize();
					if (status != cudaSuccess)
					{
						cudaFree(d_layer_norm_output);
						cudaFree(d_layer_norm_input);
						break;
					}
				} else
				{
					/* No layer norm weights, just copy */
					status = cudaMemcpy(d_layer_norm_output,
						d_layer_norm_input,
						sizeof(float)
							* config->embed_dim,
						cudaMemcpyDeviceToDevice);
					if (status != cudaSuccess)
					{
						cudaFree(d_layer_norm_output);
						cudaFree(d_layer_norm_input);
						break;
					}
				}

				/* Feed-forward network using cuBLAS */
				const float *layer_ffn_weights1 =
					weights->ffn_weights1;
				const float *layer_ffn_weights2 =
					weights->ffn_weights2;

				rc = ndb_cuda_hf_ffn_forward(
					d_layer_norm_output,
					layer_ffn_weights1,
					layer_ffn_weights2,
					d_ffn_output_temp,
					d_layer_output,
					config->embed_dim,
					config->hidden_dim);
				if (rc != 0)
				{
					cudaFree(d_layer_norm_output);
					cudaFree(d_layer_norm_input);
					break;
				}

				/* Add residual connection */
				ndb_cuda_hf_add_vectors_kernel<<<blocks,
					threads>>>(d_ffn_input,
					d_layer_output,
					d_layer_output,
					config->embed_dim);
				status = cudaGetLastError();
				if (status != cudaSuccess)
				{
					cudaFree(d_layer_norm_output);
					cudaFree(d_layer_norm_input);
					break;
				}
				status = cudaDeviceSynchronize();
				if (status != cudaSuccess)
				{
					cudaFree(d_layer_norm_output);
					cudaFree(d_layer_norm_input);
					break;
				}

				/* Free layer norm buffers */
				cudaFree(d_layer_norm_output);
				cudaFree(d_layer_norm_input);

				/* Use output as input for next layer */
				/* Swap buffers for next iteration */
				{
					float *temp = d_layer_input;
					d_layer_input = d_layer_output;
					d_layer_output = temp;
				}
			}

			/* Copy final layer output to hidden_states */
			if (d_layer_input != d_hidden_states)
			{
				status = cudaMemcpy(d_hidden_states,
					d_layer_input,
					sizeof(float) * config->embed_dim,
					cudaMemcpyDeviceToDevice);
				if (status != cudaSuccess)
				{
					cudaFree(d_residual);
					cudaFree(d_ffn_output_temp);
					cudaFree(d_ffn_input);
					cudaFree(d_attn_output);
					cudaFree(d_v);
					cudaFree(d_k);
					cudaFree(d_q);
					if (d_layer_output != d_hidden_states)
						cudaFree(d_layer_output);
					generated_count = i;
					break;
				}
			}

			/* Free temporary buffers */
			cudaFree(d_residual);
			cudaFree(d_ffn_output_temp);
			cudaFree(d_ffn_input);
			cudaFree(d_attn_output);
			cudaFree(d_v);
			cudaFree(d_k);
			cudaFree(d_q);
			if (d_layer_output != d_hidden_states)
				cudaFree(d_layer_output);

			if (status != cudaSuccess)
			{
				cudaFree(d_current_embedding);
				cudaFree(d_hidden_states);
				cudaFree(d_attention_output);
				cudaFree(d_ffn_output);
				generated_count = i;
				break;
			}
		}

		/* Step 4: Compute logits from language model head */
		/* logits = hidden_states * lm_head_weights^T */
		/* hidden_states: [embed_dim] */
		/* lm_head_weights: [vocab_size, embed_dim] (row-major) */
		/* logits: [vocab_size] */
		if (lm_head_weights != NULL)
		{
			/* Use kernel for logit computation (more efficient for single vector) */
			threads = GET_THREADS(config->vocab_size);
			blocks = GET_BLOCKS(config->vocab_size, threads);

			ndb_cuda_hf_compute_logits_kernel<<<blocks, threads>>>(
				d_hidden_states,
				lm_head_weights,
				d_logits,
				config->embed_dim,
				config->vocab_size);
			status = cudaGetLastError();
			if (status != cudaSuccess)
			{
				cudaFree(d_current_embedding);
				cudaFree(d_hidden_states);
				cudaFree(d_attention_output);
				cudaFree(d_ffn_output);
				generated_count = i;
				break;
			}
			status = cudaDeviceSynchronize();
			if (status != cudaSuccess)
			{
				cudaFree(d_current_embedding);
				cudaFree(d_hidden_states);
				cudaFree(d_attention_output);
				cudaFree(d_ffn_output);
				generated_count = i;
				break;
			}
		} else
		{
			/* No LM head weights, use dummy logits */
			threads = GET_THREADS(config->vocab_size);
			blocks = GET_BLOCKS(config->vocab_size, threads);

			/* Initialize logits with small random values */
			float *h_dummy_logits = NULL;
			int j;

			h_dummy_logits = (float *)malloc(logit_bytes);
			if (!h_dummy_logits)
			{
				cudaFree(d_current_embedding);
				cudaFree(d_hidden_states);
				cudaFree(d_attention_output);
				cudaFree(d_ffn_output);
				generated_count = i;
				break;
			}

			/* Generate dummy logits based on current token */
			for (j = 0; j < config->vocab_size; j++)
			{
				/* Dummy logit: higher probability for tokens similar to current */
				float similarity = (j == current_token_id)
					? 1.0f
					: (float)(rand() % 100) / 100.0f - 0.5f;
				h_dummy_logits[j] = similarity
					* 10.0f; /* Scale for softmax */
			}

			/* Copy to device */
			status = cudaMemcpy(d_logits,
				h_dummy_logits,
				logit_bytes,
				cudaMemcpyHostToDevice);
			free(h_dummy_logits);
			if (status != cudaSuccess)
			{
				cudaFree(d_current_embedding);
				cudaFree(d_hidden_states);
				cudaFree(d_attention_output);
				cudaFree(d_ffn_output);
				generated_count = i;
				break;
			}
		}

		/* Free temporary buffers */
		cudaFree(d_current_embedding);
		cudaFree(d_hidden_states);
		cudaFree(d_attention_output);
		cudaFree(d_ffn_output);
		d_current_embedding = NULL;
		d_hidden_states = NULL;
		d_attention_output = NULL;
		d_ffn_output = NULL;

		/* Apply sampling strategies */
		threads = GET_THREADS(config->vocab_size);
		blocks = GET_BLOCKS(config->vocab_size, threads);

		/* Apply logit bias before temperature scaling */
		if (gen_params->num_logit_bias > 0)
		{
			int32_t *d_bias_tokens = NULL;
			float *d_bias_values = NULL;
			size_t bias_tokens_bytes =
				sizeof(int32_t) * gen_params->num_logit_bias;
			size_t bias_values_bytes =
				sizeof(float) * gen_params->num_logit_bias;

			status = cudaMalloc(
				(void **)&d_bias_tokens, bias_tokens_bytes);
			if (status == cudaSuccess)
			{
				status = cudaMalloc((void **)&d_bias_values,
					bias_values_bytes);
				if (status == cudaSuccess)
				{
					/* Copy bias tokens and values to device */
					status = cudaMemcpy(d_bias_tokens,
						gen_params->logit_bias_tokens,
						bias_tokens_bytes,
						cudaMemcpyHostToDevice);
					if (status == cudaSuccess)
					{
						status = cudaMemcpy(
							d_bias_values,
							gen_params
								->logit_bias_values,
							bias_values_bytes,
							cudaMemcpyHostToDevice);
						if (status == cudaSuccess)
						{
							/* Apply logit bias */
							ndb_cuda_hf_apply_logit_bias_kernel<<<
								blocks,
								threads>>>(
								d_logits,
								d_bias_tokens,
								d_bias_values,
								gen_params
									->num_logit_bias,
								config->vocab_size);
							status =
								cudaGetLastError();
							if (status
								== cudaSuccess)
							{
								status =
									cudaDeviceSynchronize();
								if (status
									!= cudaSuccess)
								{
									cudaFree(
										d_bias_tokens);
									cudaFree(
										d_bias_values);
									generated_count =
										i;
									break;
								}
							} else
							{
								cudaFree(
									d_bias_tokens);
								cudaFree(
									d_bias_values);
								generated_count =
									i;
								break;
							}
						} else
						{
							cudaFree(d_bias_tokens);
							cudaFree(d_bias_values);
							generated_count = i;
							break;
						}
					} else
					{
						cudaFree(d_bias_tokens);
						cudaFree(d_bias_values);
						generated_count = i;
						break;
					}
					cudaFree(d_bias_tokens);
					cudaFree(d_bias_values);
				} else
				{
					cudaFree(d_bias_tokens);
					generated_count = i;
					break;
				}
			} else
			{
				generated_count = i;
				break;
			}
		}

		/* Apply temperature scaling */
		if (gen_params->temperature > 0.0f
			&& gen_params->temperature != 1.0f)
		{
			ndb_cuda_hf_temperature_scale_kernel<<<blocks,
				threads>>>(d_logits,
				gen_params->temperature,
				config->vocab_size);
			status = cudaGetLastError();
			if (status != cudaSuccess)
			{
				generated_count = i;
				break;
			}
			status = cudaDeviceSynchronize();
			if (status != cudaSuccess)
			{
				generated_count = i;
				break;
			}
		}

		/* Apply top-k filtering */
		if (gen_params->top_k > 0
			&& gen_params->top_k < config->vocab_size)
		{
			int *d_top_k_indices = NULL;
			float *d_top_k_values = NULL;
			size_t top_k_indices_bytes =
				sizeof(int) * gen_params->top_k;
			size_t top_k_values_bytes =
				sizeof(float) * gen_params->top_k;

			/* Allocate device memory for top-k indices and values */
			status = cudaMalloc(
				(void **)&d_top_k_indices, top_k_indices_bytes);
			if (status == cudaSuccess)
			{
				status = cudaMalloc((void **)&d_top_k_values,
					top_k_values_bytes);
				if (status == cudaSuccess)
				{
					/* Find top-k indices */
					ndb_cuda_hf_find_top_k_kernel<<<blocks,
						threads>>>(d_logits,
						d_top_k_indices,
						d_top_k_values,
						gen_params->top_k,
						config->vocab_size);
					status = cudaGetLastError();
					if (status == cudaSuccess)
					{
						status =
							cudaDeviceSynchronize();
						if (status == cudaSuccess)
						{
							/* Filter logits outside top-k */
							ndb_cuda_hf_top_k_filter_kernel<<<
								blocks,
								threads>>>(
								d_logits,
								d_top_k_indices,
								gen_params
									->top_k,
								config->vocab_size);
							status =
								cudaGetLastError();
							if (status
								== cudaSuccess)
							{
								status =
									cudaDeviceSynchronize();
								if (status
									!= cudaSuccess)
								{
									cudaFree(
										d_top_k_values);
									cudaFree(
										d_top_k_indices);
									generated_count =
										i;
									break;
								}
							} else
							{
								cudaFree(
									d_top_k_values);
								cudaFree(
									d_top_k_indices);
								generated_count =
									i;
								break;
							}
						} else
						{
							cudaFree(
								d_top_k_values);
							cudaFree(
								d_top_k_indices);
							generated_count = i;
							break;
						}
					} else
					{
						cudaFree(d_top_k_values);
						cudaFree(d_top_k_indices);
						generated_count = i;
						break;
					}
					cudaFree(d_top_k_values);
					cudaFree(d_top_k_indices);
				} else
				{
					cudaFree(d_top_k_indices);
					generated_count = i;
					break;
				}
			} else
			{
				generated_count = i;
				break;
			}
		}

		/* Apply top-p (nucleus) filtering */
		if (gen_params->top_p > 0.0f && gen_params->top_p < 1.0f)
		{
			float *d_sorted_logits = NULL;
			int *d_sorted_indices = NULL;
			size_t sorted_bytes =
				sizeof(float) * config->vocab_size;
			size_t sorted_indices_bytes =
				sizeof(int) * config->vocab_size;

			/* Allocate device memory for sorted logits and indices */
			status = cudaMalloc(
				(void **)&d_sorted_logits, sorted_bytes);
			if (status == cudaSuccess)
			{
				status = cudaMalloc((void **)&d_sorted_indices,
					sorted_indices_bytes);
				if (status == cudaSuccess)
				{
					/* Sort logits in descending order */
					ndb_cuda_hf_sort_logits_kernel<<<blocks,
						threads>>>(d_logits,
						d_sorted_logits,
						d_sorted_indices,
						config->vocab_size);
					status = cudaGetLastError();
					if (status == cudaSuccess)
					{
						status =
							cudaDeviceSynchronize();
						if (status == cudaSuccess)
						{
							/* Filter logits outside top-p */
							ndb_cuda_hf_top_p_filter_kernel<<<
								blocks,
								threads>>>(
								d_logits,
								d_sorted_logits,
								d_sorted_indices,
								gen_params
									->top_p,
								config->vocab_size);
							status =
								cudaGetLastError();
							if (status
								== cudaSuccess)
							{
								status =
									cudaDeviceSynchronize();
								if (status
									!= cudaSuccess)
								{
									cudaFree(
										d_sorted_indices);
									cudaFree(
										d_sorted_logits);
									generated_count =
										i;
									break;
								}
							} else
							{
								cudaFree(
									d_sorted_indices);
								cudaFree(
									d_sorted_logits);
								generated_count =
									i;
								break;
							}
						} else
						{
							cudaFree(
								d_sorted_indices);
							cudaFree(
								d_sorted_logits);
							generated_count = i;
							break;
						}
					} else
					{
						cudaFree(d_sorted_indices);
						cudaFree(d_sorted_logits);
						generated_count = i;
						break;
					}
					cudaFree(d_sorted_indices);
					cudaFree(d_sorted_logits);
				} else
				{
					cudaFree(d_sorted_logits);
					generated_count = i;
					break;
				}
			} else
			{
				generated_count = i;
				break;
			}
		}

		/* Apply repetition penalty */
		if (gen_params->repetition_penalty > 1.0f
			&& generated_count > 0)
		{
			ndb_cuda_hf_repetition_penalty_kernel<<<blocks,
				threads>>>(d_logits,
				d_output_tokens,
				generated_count,
				gen_params->repetition_penalty,
				config->vocab_size);
			status = cudaGetLastError();
			if (status != cudaSuccess)
			{
				generated_count = i;
				break;
			}
			status = cudaDeviceSynchronize();
			if (status != cudaSuccess)
			{
				generated_count = i;
				break;
			}
		}

		/* Sample next token */
		if (gen_params->do_sample)
		{
			/* Multinomial sampling */
			/* Generate random value on host (simplified) */
			/* TODO: Use curand for device-side random number generation */
			float random_value;
			if (gen_params->seed > 0)
				srand(gen_params->seed + generated_count);
			else
				srand((unsigned int)time(NULL)
					+ generated_count);
			random_value = (float)rand() / (float)RAND_MAX;

			ndb_cuda_hf_multinomial_sample_kernel<<<blocks,
				threads>>>(d_logits,
				d_probs,
				d_cumsum,
				random_value,
				d_output_tokens + generated_count,
				config->vocab_size);
		} else
		{
			/* Greedy sampling */
			ndb_cuda_hf_greedy_sample_kernel<<<1, 1>>>(d_logits,
				d_output_tokens + generated_count,
				config->vocab_size);
		}

		status = cudaGetLastError();
		if (status != cudaSuccess)
		{
			generated_count = i;
			break;
		}
		status = cudaDeviceSynchronize();
		if (status != cudaSuccess)
		{
			generated_count = i;
			break;
		}

		/* Copy next token to host */
		status = cudaMemcpy(&h_next_token,
			d_output_tokens + generated_count,
			sizeof(int32_t),
			cudaMemcpyDeviceToHost);
		if (status != cudaSuccess)
		{
			generated_count = i;
			break;
		}

		next_token = h_next_token;
		output_token_ids[generated_count] = next_token;
		generated_count++;
		current_pos++;

		/* Update KV cache with new token (simplified) */
		/* TODO: Implement proper KV cache update after transformer forward pass */
		/* For now, just increment cache position */
		if (kv_cache != NULL)
			kv_cache->current_pos = current_pos;

		/* Check for stop sequences */
		if (gen_params->num_stop_sequences > 0
			&& generated_count >= gen_params->min_tokens)
		{
			int j;

			for (j = 0; j < gen_params->num_stop_sequences; j++)
			{
				int stop_len = gen_params->stop_seq_lens[j];
				int32_t *d_stop_seq = NULL;

				if (stop_len <= 0
					|| stop_len > NDB_HF_MAX_STOP_SEQ_LEN)
					continue;

				/* Allocate and copy stop sequence to device */
				status = cudaMalloc((void **)&d_stop_seq,
					sizeof(int32_t) * stop_len);
				if (status != cudaSuccess)
					continue; /* Skip this stop sequence */
				status = cudaMemcpy(d_stop_seq,
					gen_params->stop_sequences[j],
					sizeof(int32_t) * stop_len,
					cudaMemcpyHostToDevice);
				if (status != cudaSuccess)
				{
					cudaFree(d_stop_seq);
					continue; /* Skip this stop sequence */
				}

				/* Check if stop sequence found */
				ndb_cuda_hf_check_stop_sequence_kernel<<<1,
					1>>>(d_output_tokens,
					generated_count,
					d_stop_seq,
					stop_len,
					d_found_stop);
				status = cudaGetLastError();
				if (status != cudaSuccess)
				{
					cudaFree(d_stop_seq);
					continue; /* Skip this stop sequence */
				}
				status = cudaDeviceSynchronize();
				if (status != cudaSuccess)
				{
					cudaFree(d_stop_seq);
					continue; /* Skip this stop sequence */
				}

				/* Copy result to host */
				status = cudaMemcpy(&stop_found,
					d_found_stop,
					sizeof(bool),
					cudaMemcpyDeviceToHost);
				if (status != cudaSuccess)
				{
					cudaFree(d_stop_seq);
					continue; /* Skip this stop sequence */
				}

				cudaFree(d_stop_seq);

				if (stop_found)
					break;
			}
		}

		/* Check minimum tokens */
		if (generated_count < gen_params->min_tokens)
			stop_found = false;

		/* Check maximum tokens */
		if (generated_count >= max_gen_tokens)
			stop_found = true;
	}

	/* Copy output tokens to host */
	*output_seq_len = generated_count;
	if (generated_count > 0)
	{
		status = cudaMemcpy(output_token_ids,
			d_output_tokens,
			sizeof(int32_t) * generated_count,
			cudaMemcpyDeviceToHost);
		if (status != cudaSuccess)
			goto error;
	}

	/* Cleanup */
	cudaFree(d_embeddings);
	cudaFree(d_logits);
	cudaFree(d_probs);
	cudaFree(d_cumsum);
	cudaFree(d_input_tokens);
	cudaFree(d_output_tokens);
	cudaFree(d_key_cache);
	cudaFree(d_value_cache);
	cudaFree(d_found_stop);

	return 0;

error:
	if (d_embeddings)
		cudaFree(d_embeddings);
	if (d_logits)
		cudaFree(d_logits);
	if (d_probs)
		cudaFree(d_probs);
	if (d_cumsum)
		cudaFree(d_cumsum);
	if (d_input_tokens)
		cudaFree(d_input_tokens);
	if (d_output_tokens)
		cudaFree(d_output_tokens);
	if (d_key_cache)
		cudaFree(d_key_cache);
	if (d_value_cache)
		cudaFree(d_value_cache);
	if (d_found_stop)
		cudaFree(d_found_stop);
	if (d_top_k_indices)
		cudaFree(d_top_k_indices);
	if (d_sorted_logits)
		cudaFree(d_sorted_logits);
	if (d_sorted_indices)
		cudaFree(d_sorted_indices);
	if (d_curand_state)
		cudaFree(d_curand_state);
	if (errstr && !*errstr)
	{
		const char *err_msg = cudaGetErrorString(status);
		size_t len = strlen(err_msg) + 1;
		char *err = (char *)malloc(len);
		if (err)
		{
			memcpy(err, err_msg, len);
			*errstr = err;
		}
	}
	return -1;
}

/*======================================================================*/
/* Cross-Encoder Reranking Kernels */
/*======================================================================*/

/*
 * Batch embedding lookup kernel: Map token IDs to embedding vectors for batch
 * Input: embedding_table [vocab_size, embed_dim]
 *        token_ids [batch_size, seq_len]
 *        attention_mask [batch_size, seq_len]
 * Output: embeddings [batch_size, seq_len, embed_dim]
 */
__global__ static void
ndb_cuda_hf_batch_embedding_lookup_kernel(const float *embedding_table,
	const int32_t *token_ids,
	const int32_t *attention_mask,
	float *embeddings,
	int batch_size,
	int seq_len,
	int embed_dim,
	int vocab_size)
{
	int batch_idx = blockIdx.x;
	int seq_idx = blockIdx.y;
	int dim_idx = threadIdx.x;

	if (batch_idx >= batch_size || seq_idx >= seq_len || dim_idx >= embed_dim)
		return;

	int token_id = token_ids[batch_idx * seq_len + seq_idx];
	if (token_id < 0 || token_id >= vocab_size)
		token_id = 0;

	int embed_offset = token_id * embed_dim + dim_idx;
	int output_offset = batch_idx * seq_len * embed_dim + seq_idx * embed_dim + dim_idx;

	float mask_val = (attention_mask != NULL && attention_mask[batch_idx * seq_len + seq_idx] != 0) ? 1.0f : 0.0f;
	embeddings[output_offset] = embedding_table[embed_offset] * mask_val;
}

/*
 * Extract CLS token from transformer output
 * Input: hidden_states [batch_size, seq_len, embed_dim]
 * Output: cls_embeddings [batch_size, embed_dim]
 */
__global__ static void
ndb_cuda_hf_extract_cls_kernel(const float *hidden_states,
	int batch_size,
	int seq_len,
	int embed_dim,
	float *cls_embeddings)
{
	int batch_idx = blockIdx.x;
	int dim_idx = threadIdx.x;

	if (batch_idx >= batch_size || dim_idx >= embed_dim)
		return;

	/* CLS token is at position 0 */
	int cls_offset = batch_idx * seq_len * embed_dim + dim_idx;
	int output_offset = batch_idx * embed_dim + dim_idx;

	cls_embeddings[output_offset] = hidden_states[cls_offset];
}

/*
 * Apply classification head (linear layer) to CLS embeddings
 * score = W^T * cls_embedding + b
 * Input: cls_embeddings [batch_size, embed_dim]
 *        weights [embed_dim] (classification head weights)
 *        bias (scalar bias)
 * Output: scores [batch_size]
 */
__global__ static void
ndb_cuda_hf_classification_head_kernel(const float *cls_embeddings,
	const float *weights,
	float bias,
	int batch_size,
	int embed_dim,
	float *scores)
{
	int batch_idx = blockIdx.x * blockDim.x + threadIdx.x;

	if (batch_idx >= batch_size)
		return;

	/* Compute dot product: W^T * cls_embedding */
	float score = bias;
	int embed_offset = batch_idx * embed_dim;

	for (int d = 0; d < embed_dim; d++)
	{
		score += weights[d] * cls_embeddings[embed_offset + d];
	}

	/* Apply sigmoid to get probability-like score in [0, 1] */
	scores[batch_idx] = 1.0f / (1.0f + expf(-score));
}

/*
 * Host function: Cross-encoder reranking inference
 * Processes batch of query-document pairs and returns relevance scores
 */
extern "C" int
ndb_cuda_hf_cross_encoder_rerank_inference(
	const char *model_name,
	const int32_t *token_ids_batch,
	const int32_t *attention_mask_batch,
	int batch_size,
	int seq_len,
	const float *embedding_table,
	int vocab_size,
	int embed_dim,
	const float *classification_weights,
	float classification_bias,
	float *scores_out,
	char **errstr)
{
	cudaError_t status;
	int32_t *d_token_ids = NULL;
	int32_t *d_attention_mask = NULL;
	float *d_embeddings = NULL;
	float *d_hidden_states = NULL;
	float *d_cls_embeddings = NULL;
	float *d_scores = NULL;
	float *d_classification_weights = NULL;
	size_t token_bytes;
	size_t mask_bytes;
	size_t embed_bytes;
	size_t hidden_bytes;
	size_t cls_bytes;
	size_t score_bytes;
	dim3 grid, block;
	int i;

	if (errstr)
		*errstr = NULL;

	if (!model_name || !token_ids_batch || !attention_mask_batch
		|| !embedding_table || !classification_weights || !scores_out)
	{
		if (errstr)
			*errstr = (char *)"invalid parameters for cross-encoder rerank";
		return -1;
	}

	if (batch_size <= 0 || seq_len <= 0 || embed_dim <= 0)
	{
		if (errstr)
			*errstr = (char *)"invalid dimensions for cross-encoder rerank";
		return -1;
	}

	/* Allocate device memory */
	token_bytes = sizeof(int32_t) * batch_size * seq_len;
	mask_bytes = sizeof(int32_t) * batch_size * seq_len;
	embed_bytes = sizeof(float) * batch_size * seq_len * embed_dim;
	hidden_bytes = sizeof(float) * batch_size * seq_len * embed_dim;
	cls_bytes = sizeof(float) * batch_size * embed_dim;
	score_bytes = sizeof(float) * batch_size;

	status = cudaMalloc((void **)&d_token_ids, token_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_attention_mask, mask_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_embeddings, embed_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_hidden_states, hidden_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_cls_embeddings, cls_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_scores, score_bytes);
	if (status != cudaSuccess)
		goto error;
	status = cudaMalloc((void **)&d_classification_weights, sizeof(float) * embed_dim);
	if (status != cudaSuccess)
		goto error;

	/* Copy input to device */
	status = cudaMemcpy(d_token_ids, token_ids_batch, token_bytes, cudaMemcpyHostToDevice);
	if (status != cudaSuccess)
		goto error;
	status = cudaMemcpy(d_attention_mask, attention_mask_batch, mask_bytes, cudaMemcpyHostToDevice);
	if (status != cudaSuccess)
		goto error;
	status = cudaMemcpy(d_classification_weights, classification_weights,
		sizeof(float) * embed_dim, cudaMemcpyHostToDevice);
	if (status != cudaSuccess)
		goto error;

	/* Step 1: Embedding lookup for all tokens in batch */
	grid = dim3(batch_size, seq_len);
	block = dim3(embed_dim);

	ndb_cuda_hf_batch_embedding_lookup_kernel<<<grid, block>>>(
		embedding_table,
		d_token_ids,
		d_attention_mask,
		d_embeddings,
		batch_size,
		seq_len,
		embed_dim,
		vocab_size);
	status = cudaGetLastError();
	if (status != cudaSuccess)
		goto error;
	status = cudaDeviceSynchronize();
	if (status != cudaSuccess)
		goto error;

	/* Step 2: For now, use embeddings directly as hidden states */
	/* Full implementation would run through transformer layers with Flash Attention */
	status = cudaMemcpy(d_hidden_states, d_embeddings, embed_bytes, cudaMemcpyDeviceToDevice);
	if (status != cudaSuccess)
		goto error;

	/* Step 3: Extract CLS token (position 0) from each sequence */
	grid = dim3(batch_size);
	block = dim3(embed_dim);

	ndb_cuda_hf_extract_cls_kernel<<<grid, block>>>(
		d_hidden_states,
		batch_size,
		seq_len,
		embed_dim,
		d_cls_embeddings);
	status = cudaGetLastError();
	if (status != cudaSuccess)
		goto error;
	status = cudaDeviceSynchronize();
	if (status != cudaSuccess)
		goto error;

	/* Step 4: Apply classification head */
	grid = dim3((batch_size + 255) / 256);
	block = dim3(256);

	ndb_cuda_hf_classification_head_kernel<<<grid, block>>>(
		d_cls_embeddings,
		d_classification_weights,
		classification_bias,
		batch_size,
		embed_dim,
		d_scores);
	status = cudaGetLastError();
	if (status != cudaSuccess)
		goto error;
	status = cudaDeviceSynchronize();
	if (status != cudaSuccess)
		goto error;

	/* Copy scores back to host */
	status = cudaMemcpy(scores_out, d_scores, score_bytes, cudaMemcpyDeviceToHost);
	if (status != cudaSuccess)
		goto error;

	/* Cleanup */
	cudaFree(d_classification_weights);
	cudaFree(d_token_ids);
	cudaFree(d_attention_mask);
	cudaFree(d_embeddings);
	cudaFree(d_hidden_states);
	cudaFree(d_cls_embeddings);
	cudaFree(d_scores);

	return 0;

error:
	if (d_token_ids)
		cudaFree(d_token_ids);
	if (d_attention_mask)
		cudaFree(d_attention_mask);
	if (d_embeddings)
		cudaFree(d_embeddings);
	if (d_hidden_states)
		cudaFree(d_hidden_states);
	if (d_cls_embeddings)
		cudaFree(d_cls_embeddings);
	if (d_scores)
		cudaFree(d_scores);
	if (d_classification_weights)
		cudaFree(d_classification_weights);
	if (errstr && !*errstr)
	{
		const char *err_msg = cudaGetErrorString(status);
		size_t len = strlen(err_msg) + 1;
		char *err = (char *)malloc(len);
		if (err)
		{
			memcpy(err, err_msg, len);
			*errstr = err;
		}
	}
	return -1;
}
