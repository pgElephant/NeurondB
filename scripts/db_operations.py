#!/usr/bin/env python3
"""
db_operations.py
    Database schema and operations for document storage

Copyright (c) 2024-2025, pgElephant, Inc.
"""

import json
import psycopg2
from typing import Tuple, Optional, Dict, List
from psycopg2.extras import execute_values


class DatabaseSchema:
	"""Database schema creation and management"""
	
	def __init__(self, table_prefix: str = 'docs'):
		self.table_prefix = table_prefix
	
	def create_schema_sql(self) -> str:
		"""Generate SQL to create complete schema"""
		return f"""
		/*-------------------------------------------------------------------------
		 * Documentation Embedding Schema
		 * Generated by load_docs.py
		 *-------------------------------------------------------------------------
		 */

		-- Main documents table
		CREATE TABLE IF NOT EXISTS {self.table_prefix}_documents (
			doc_id SERIAL PRIMARY KEY,
			filepath TEXT NOT NULL UNIQUE,
			filename TEXT NOT NULL,
			title TEXT,
			content TEXT NOT NULL,
			file_size INTEGER,
			file_type TEXT,
			created_at TIMESTAMPTZ DEFAULT now(),
			updated_at TIMESTAMPTZ DEFAULT now(),
			metadata JSONB DEFAULT '{{}}'::jsonb
		);

		-- Document chunks table
		CREATE TABLE IF NOT EXISTS {self.table_prefix}_chunks (
			chunk_id SERIAL PRIMARY KEY,
			doc_id INTEGER REFERENCES {self.table_prefix}_documents(doc_id) 
				ON DELETE CASCADE,
			chunk_index INTEGER NOT NULL,
			chunk_text TEXT NOT NULL,
			chunk_tokens INTEGER,
			embedding VECTOR(384),
			metadata JSONB DEFAULT '{{}}'::jsonb,
			created_at TIMESTAMPTZ DEFAULT now(),
			UNIQUE(doc_id, chunk_index)
		);

		-- Indexes
		CREATE INDEX IF NOT EXISTS idx_{self.table_prefix}_docs_filepath 
			ON {self.table_prefix}_documents(filepath);
		CREATE INDEX IF NOT EXISTS idx_{self.table_prefix}_docs_filename 
			ON {self.table_prefix}_documents(filename);
		CREATE INDEX IF NOT EXISTS idx_{self.table_prefix}_chunks_doc_id 
			ON {self.table_prefix}_chunks(doc_id);
		CREATE INDEX IF NOT EXISTS idx_{self.table_prefix}_chunks_doc_chunk 
			ON {self.table_prefix}_chunks(doc_id, chunk_index);

		-- Function to update updated_at timestamp
		CREATE OR REPLACE FUNCTION update_{self.table_prefix}_updated_at()
		RETURNS TRIGGER AS $$
		BEGIN
			NEW.updated_at = now();
			RETURN NEW;
		END;
		$$ LANGUAGE plpgsql;

		-- Trigger for updated_at
		DROP TRIGGER IF EXISTS trigger_{self.table_prefix}_docs_updated_at 
			ON {self.table_prefix}_documents;
		CREATE TRIGGER trigger_{self.table_prefix}_docs_updated_at
			BEFORE UPDATE ON {self.table_prefix}_documents
			FOR EACH ROW
			EXECUTE FUNCTION update_{self.table_prefix}_updated_at();

		-- Statistics view
		CREATE OR REPLACE VIEW {self.table_prefix}_stats AS
		SELECT 
			COUNT(DISTINCT d.doc_id) AS total_documents,
			COUNT(c.chunk_id) AS total_chunks,
			COUNT(c.chunk_id) FILTER (WHERE c.embedding IS NOT NULL) 
				AS chunks_with_embeddings,
			AVG(LENGTH(d.content)) AS avg_doc_length,
			SUM(LENGTH(d.content)) AS total_content_size,
			MIN(d.created_at) AS first_loaded,
			MAX(d.updated_at) AS last_updated
		FROM {self.table_prefix}_documents d
		LEFT JOIN {self.table_prefix}_chunks c ON d.doc_id = c.doc_id;
		"""
	
	def create_schema(self, conn):
		"""Create schema in database"""
		cur = conn.cursor()
		cur.execute(self.create_schema_sql())
		conn.commit()
		cur.close()


class DocumentLoader:
	"""Load documents into database"""
	
	def __init__(self, conn, table_prefix: str = 'docs'):
		self.conn = conn
		self.table_prefix = table_prefix
	
	def insert_document(self, filepath: str, filename: str, title: str,
		content: str, file_size: int, file_type: str, 
		metadata: Dict) -> bool:
		"""
		Insert or update a single document
		
		Returns:
			True if successful, False otherwise
		"""
		cur = self.conn.cursor()
		try:
			cur.execute(f"""
				INSERT INTO {self.table_prefix}_documents 
					(filepath, filename, title, content, file_size, file_type, metadata)
				VALUES (%s, %s, %s, %s, %s, %s, %s)
				ON CONFLICT (filepath) DO UPDATE SET
					title = EXCLUDED.title,
					content = EXCLUDED.content,
					file_size = EXCLUDED.file_size,
					file_type = EXCLUDED.file_type,
					metadata = EXCLUDED.metadata,
					updated_at = now()
			""", (
				filepath,
				filename,
				title,
				content,
				file_size,
				file_type,
				json.dumps(metadata)
			))
			self.conn.commit()
			return True
		except Exception as e:
			self.conn.rollback()
			raise e
		finally:
			cur.close()
	
	def batch_insert_documents(self, documents: List[Tuple]) -> Tuple[int, int]:
		"""
		Batch insert documents
		
		Args:
			documents: List of (filepath, filename, title, content, 
				file_size, file_type, metadata_json) tuples
		
		Returns:
			(inserted_count, error_count)
		"""
		cur = self.conn.cursor()
		inserted = 0
		errors = 0
		
		for doc in documents:
			try:
				cur.execute(f"""
					INSERT INTO {self.table_prefix}_documents 
						(filepath, filename, title, content, file_size, file_type, metadata)
					VALUES (%s, %s, %s, %s, %s, %s, %s)
					ON CONFLICT (filepath) DO UPDATE SET
						title = EXCLUDED.title,
						content = EXCLUDED.content,
						file_size = EXCLUDED.file_size,
						file_type = EXCLUDED.file_type,
						metadata = EXCLUDED.metadata,
						updated_at = now()
				""", doc)
				inserted += 1
			except Exception:
				errors += 1
		
		self.conn.commit()
		cur.close()
		
		return inserted, errors


class StatisticsManager:
	"""Get statistics from database"""
	
	def __init__(self, conn, table_prefix: str = 'docs'):
		self.conn = conn
		self.table_prefix = table_prefix
	
	def get_statistics(self) -> Optional[Tuple]:
		"""Get overall statistics"""
		cur = self.conn.cursor()
		try:
			cur.execute(f"SELECT * FROM {self.table_prefix}_stats")
			return cur.fetchone()
		finally:
			cur.close()
	
	def get_document_count(self) -> int:
		"""Get total document count"""
		cur = self.conn.cursor()
		try:
			cur.execute(f"SELECT COUNT(*) FROM {self.table_prefix}_documents")
			return cur.fetchone()[0]
		finally:
			cur.close()
	
	def get_chunk_count(self) -> int:
		"""Get total chunk count"""
		cur = self.conn.cursor()
		try:
			cur.execute(f"SELECT COUNT(*) FROM {self.table_prefix}_chunks")
			return cur.fetchone()[0]
		finally:
			cur.close()
	
	def get_embedding_coverage(self) -> Tuple[int, int]:
		"""Get embedding coverage statistics"""
		cur = self.conn.cursor()
		try:
			cur.execute(f"""
				SELECT 
					COUNT(*) AS total,
					COUNT(*) FILTER (WHERE embedding IS NOT NULL) AS with_embeddings
				FROM {self.table_prefix}_chunks
			""")
			return cur.fetchone()
		finally:
			cur.close()

