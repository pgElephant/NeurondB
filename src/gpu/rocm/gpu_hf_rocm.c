/*-------------------------------------------------------------------------
 *
 * gpu_hf_rocm.c
 *    Hugging Face and LLM operations bridge.
 *
 * This module provides bridge functions for Hugging Face model operations
 * and LLM inference.
 *
 * Copyright (c) 2024-2025, pgElephant, Inc.
 *
 * IDENTIFICATION
 *    src/gpu/rocm/gpu_hf_rocm.c
 *
 *-------------------------------------------------------------------------
 */

#include "postgres.h"

#ifdef NDB_GPU_HIP

#include <string.h>
#include <stdlib.h>
#include <math.h>
#include <time.h>

#include "neurondb_rocm_hf.h"
#include "neurondb_rocm_launchers.h"
#include "utils/elog.h"
#include "utils/memutils.h"
#include "utils/typcache.h"
#include "lib/stringinfo.h"
#include "storage/lwlock.h"
#include "storage/shmem.h"
#include <hip/hip_runtime.h>
#include <ctype.h>
#include <errno.h>
#ifdef HAVE_ONNX_RUNTIME
#include "neurondb_onnx.h"
#include "neurondb_validation.h"
#include "neurondb_safe_memory.h"
#include "neurondb_macros.h"
#include "neurondb_json.h"
#endif

typedef struct NdbRocmHfModelEntry
{
	char model_name[NDB_HF_MAX_MODEL_NAME];
	NdbRocmHfModelConfig config;
	NdbRocmHfModelWeights weights;
	bool loaded;
	bool weights_on_device;
	void *device_weights_ptr;
	void *device_position_embeddings;
	void *device_lm_head_weights;
	size_t device_weights_bytes;
	size_t device_position_embed_bytes;
	size_t device_lm_head_bytes;
	time_t last_used;
	struct NdbRocmHfModelEntry *next;
} NdbRocmHfModelEntry;

static NdbRocmHfModelEntry *g_model_cache = NULL;
static int g_model_cache_count = 0;
static int __attribute__((unused)) g_model_cache_max = 10;

static NdbRocmHfModelEntry *ndb_rocm_hf_find_model(const char *model_name);
static int ndb_rocm_hf_load_model_weights(const char *model_name,
	const char *model_path,
	NdbRocmHfModelConfig *config,
	NdbRocmHfModelWeights *weights,
	char **errstr);
static int ndb_rocm_hf_tokenize_text(const char *text,
	const char *model_name,
	int32_t *token_ids,
	int32_t *attention_mask,
	int *seq_len,
	char **errstr);
/* ndb_rocm_hf_parse_gen_params is now replaced by ndb_json_parse_gen_params from neurondb_json.h */
static int ndb_rocm_hf_decode_tokens(const int32_t *token_ids,
	int num_tokens,
	const char *model_name,
	char **text_out,
	char **errstr);
static int ndb_rocm_hf_init_kv_cache(NdbRocmHfKVCache *kv_cache,
	const NdbRocmHfModelConfig *config,
	char **errstr);
static void ndb_rocm_hf_free_kv_cache(NdbRocmHfKVCache *kv_cache);

static NdbRocmHfModelEntry *
ndb_rocm_hf_find_model(const char *model_name)
{
	NdbRocmHfModelEntry *entry;
	size_t model_name_len;

	if (!model_name)
		return NULL;

	if (model_name[0] == '\0')
		return NULL;

	model_name_len = strlen(model_name);
	if (model_name_len >= NDB_HF_MAX_MODEL_NAME)
		return NULL;

	for (entry = g_model_cache; entry != NULL; entry = entry->next)
	{
		if (strncmp(entry->model_name,
				model_name,
				NDB_HF_MAX_MODEL_NAME - 1) == 0)
		{
			if (entry->model_name[model_name_len] == '\0')
				return entry;
		}
	}

	return NULL;
}

static int
ndb_rocm_hf_load_model_weights(const char *model_name,
	const char *model_path,
	NdbRocmHfModelConfig *config,
	NdbRocmHfModelWeights *weights,
	char **errstr)
{
	size_t embed_table_size;
	size_t position_embed_size;
	size_t lm_head_size;
	int i;

	/* Placeholder: In a full implementation, this would:
	 * 1. Load model weights from file (e.g., safetensors, pickle)
	 * 2. Allocate GPU memory for weights
	 * 3. Copy weights to GPU
	 * 4. Initialize config structure
	 *
	 * For now, we'll create dummy weights for demonstration
	 */
	if (!model_name || !config || !weights)
	{
		if (errstr)
			*errstr =
				pstrdup("invalid parameters for model loading");
		return -1;
	}

	memset(config, 0, sizeof(NdbRocmHfModelConfig));
	memset(weights, 0, sizeof(NdbRocmHfModelWeights));

	strncpy(config->model_name, model_name, NDB_HF_MAX_MODEL_NAME - 1);
	config->model_type = NDB_HF_MODEL_EMBEDDING;
	config->vocab_size = 30522;
	config->embed_dim = 768;
	config->max_seq_len = 512;
	config->num_layers = 12;
	config->num_heads = 12;
	config->hidden_dim = 3072;
	config->use_gpu = true;

	embed_table_size =
		config->vocab_size * config->embed_dim * sizeof(float);
	weights->embedding_table = (float *)palloc(embed_table_size);

	for (i = 0; i < config->vocab_size * config->embed_dim; i++)
	{
		weights->embedding_table[i] =
			(float)(rand() % 1000) / 1000.0f - 0.5f;
	}

	position_embed_size =
		config->max_seq_len * config->embed_dim * sizeof(float);
	lm_head_size =
		config->vocab_size * config->embed_dim * sizeof(float);
	weights->position_embeddings = (float *)palloc(position_embed_size);
	weights->lm_head_weights = (float *)palloc(lm_head_size);

	for (i = 0; i < config->max_seq_len * config->embed_dim; i++)
	{
		weights->position_embeddings[i] =
			(float)(rand() % 1000) / 1000.0f - 0.5f;
	}
	for (i = 0; i < config->vocab_size * config->embed_dim; i++)
	{
		weights->lm_head_weights[i] =
			(float)(rand() % 1000) / 1000.0f - 0.5f;
	}

	weights->total_bytes =
		embed_table_size + position_embed_size + lm_head_size;

	return 0;
}

static int
ndb_rocm_hf_tokenize_text(const char *text,
	const char *model_name,
	int32_t *token_ids,
	int32_t *attention_mask,
	int *seq_len,
	char **errstr)
{
	int text_len;
	int word_count = 0;
	int i;
	int word_start = 0;
	int max_tokens = NDB_HF_MAX_SEQ_LEN - 2;

	if (!text || !token_ids || !attention_mask || !seq_len)
	{
		if (errstr)
			*errstr =
				pstrdup("invalid parameters for tokenization");
		return -1;
	}

	text_len = strlen(text);

	for (i = 0; i < text_len && word_count < max_tokens; i++)
	{
		if (text[i] == ' ' || text[i] == '\t' || text[i] == '\n')
		{
			if (i > word_start)
			{
				uint32_t hash = 0;
				int j;

				for (j = word_start; j < i && j < text_len; j++)
					hash = hash * 31
						+ (unsigned char)text[j];

				token_ids[word_count + 1] = (int32_t)(hash
					% 30522);
				attention_mask[word_count + 1] = 1;
				word_count++;
			}
			word_start = i + 1;
		}
	}

	if (word_start < text_len && word_count < max_tokens)
	{
		uint32_t hash = 0;
		int j;

		for (j = word_start; j < text_len; j++)
			hash = hash * 31 + (unsigned char)text[j];

		token_ids[word_count + 1] = (int32_t)(hash % 30522);
		attention_mask[word_count + 1] = 1;
		word_count++;
	}

	/* Add CLS token at start, SEP token at end */
	token_ids[0] = 101; /* [CLS] token ID for BERT */
	attention_mask[0] = 1;
	token_ids[word_count + 1] = 102; /* [SEP] token ID for BERT */
	attention_mask[word_count + 1] = 1;

	/* Pad remaining positions */
	for (i = word_count + 2; i < NDB_HF_MAX_SEQ_LEN; i++)
	{
		token_ids[i] = 0;
		attention_mask[i] = 0;
	}

	*seq_len = word_count + 2;
	return 0;
}

/*
 * ndb_rocm_hf_embed
 *	  Generate text embeddings using ROCm-accelerated Hugging Face model.
 *
 * This implementation:
 * 1. Loads model from cache or loads it if not cached
 * 2. Tokenizes input text
 * 3. Runs embedding lookup and pooling on GPU
 * 4. Returns pooled embedding vector
 *
 * Error handling with defensive checks at every step.
 */
int
ndb_rocm_hf_embed(const char *model_name,
	const char *text,
	float **vec_out,
	int *dim_out,
	char **errstr)
{
	NdbRocmHfModelEntry *entry = NULL;
	NdbRocmHfModelConfig config;
	NdbRocmHfModelWeights weights;
	int32_t token_ids[NDB_HF_MAX_SEQ_LEN];
	int32_t attention_mask[NDB_HF_MAX_SEQ_LEN];
	int seq_len = 0;
	float *embedding = NULL;
	int embed_dim = 0;
	hipError_t hip_status;
	float *d_embedding_table = NULL;
	size_t embed_table_bytes;
	size_t embed_table_size;
	int rc = -1;
	MemoryContext oldcontext;
	MemoryContext embed_context = NULL;

	/* Initialize error string */
	if (errstr)
		*errstr = NULL;

	/* Validate input parameters */
	if (model_name == NULL || text == NULL || vec_out == NULL
		|| dim_out == NULL)
	{
		if (errstr)
			*errstr = pstrdup("invalid parameters for ROCm HF embed");
		return -1;
	}

	/* Validate non-empty strings */
	if (model_name[0] == '\0')
	{
		if (errstr)
			*errstr = pstrdup("model name cannot be empty");
		return -1;
	}
	if (text[0] == '\0')
	{
		if (errstr)
			*errstr = pstrdup("input text cannot be empty");
		return -1;
	}

	/* Validate model name length */
	if (strlen(model_name) >= NDB_HF_MAX_MODEL_NAME)
	{
		if (errstr)
			*errstr = psprintf("model name too long (max %d)",
				NDB_HF_MAX_MODEL_NAME - 1);
		return -1;
	}

	/* Find or load model (check cache first, before creating temp context) */
	entry = ndb_rocm_hf_find_model(model_name);
	if (entry == NULL || !entry->loaded)
	{
		/* Create temporary context for loading weights */
		embed_context = AllocSetContextCreate(CurrentMemoryContext,
			"rocm_hf_load_ctx",
			ALLOCSET_DEFAULT_SIZES);
		if (embed_context == NULL)
		{
			if (errstr)
				*errstr = pstrdup("failed to create memory context");
			return -1;
		}
		oldcontext = MemoryContextSwitchTo(embed_context);

		/* Load model (for now, use dummy weights) */
		rc = ndb_rocm_hf_load_model_weights(
			model_name, NULL, &config, &weights, errstr);
		if (rc != 0)
		{
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(embed_context);
			return -1;
		}

		/* Validate config values */
		if (config.vocab_size <= 0 || config.vocab_size > 1000000)
		{
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(embed_context);
			if (errstr)
				*errstr = psprintf("invalid vocab_size: %d",
					config.vocab_size);
			return -1;
		}
		if (config.embed_dim <= 0 || config.embed_dim > 10000)
		{
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(embed_context);
			if (errstr)
				*errstr = psprintf("invalid embed_dim: %d",
					config.embed_dim);
			return -1;
		}
		if (config.max_seq_len <= 0
			|| config.max_seq_len > NDB_HF_MAX_SEQ_LEN)
		{
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(embed_context);
			if (errstr)
				*errstr = psprintf("invalid max_seq_len: %d",
					config.max_seq_len);
			return -1;
		}

		/* Check for size overflow */
		embed_table_size =
			config.vocab_size * config.embed_dim * sizeof(float);
		if (embed_table_size / sizeof(float)
			!= (size_t) config.vocab_size * config.embed_dim)
		{
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(embed_context);
			if (errstr)
				*errstr = pstrdup("embedding table size overflow");
			return -1;
		}

		/* Validate weights were allocated */
		if (weights.embedding_table == NULL
			|| weights.position_embeddings == NULL
			|| weights.lm_head_weights == NULL)
		{
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(embed_context);
			if (errstr)
				*errstr = pstrdup("model weights not properly loaded");
			return -1;
		}

		/* Switch to CacheMemoryContext for persistent cache entry */
		MemoryContextSwitchTo(oldcontext);
		oldcontext = MemoryContextSwitchTo(CacheMemoryContext);

		/* Allocate new cache entry in CacheMemoryContext (persistent) */
		entry = (NdbRocmHfModelEntry *)palloc(
			sizeof(NdbRocmHfModelEntry));
		if (entry == NULL)
		{
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(embed_context);
			if (errstr)
				*errstr = pstrdup("failed to allocate cache entry");
			return -1;
		}
		memset(entry, 0, sizeof(NdbRocmHfModelEntry));
		strncpy(entry->model_name,
			model_name,
			NDB_HF_MAX_MODEL_NAME - 1);
		entry->model_name[NDB_HF_MAX_MODEL_NAME - 1] = '\0';
		entry->config = config;

		/* Copy weights to parent context with validation */
		{
			size_t position_embed_size;
			size_t lm_head_size;

			position_embed_size =
				config.max_seq_len * config.embed_dim * sizeof(float);
			lm_head_size =
				config.vocab_size * config.embed_dim * sizeof(float);

			/* Check for overflow */
			if (position_embed_size / sizeof(float)
				!= (size_t) config.max_seq_len * config.embed_dim
				|| lm_head_size / sizeof(float)
				!= (size_t) config.vocab_size * config.embed_dim)
			{
				MemoryContextDelete(embed_context);
				if (errstr)
					*errstr = pstrdup("weight size overflow");
				return -1;
			}

			entry->weights.embedding_table =
				(float *)palloc(embed_table_size);
			if (entry->weights.embedding_table == NULL)
			{
				MemoryContextDelete(embed_context);
				if (errstr)
					*errstr = pstrdup("failed to allocate embedding table");
				return -1;
			}
			memcpy(entry->weights.embedding_table,
				weights.embedding_table,
				embed_table_size);

			entry->weights.position_embeddings =
				(float *)palloc(position_embed_size);
			if (entry->weights.position_embeddings == NULL)
			{
				NDB_FREE(entry->weights.embedding_table);
				MemoryContextDelete(embed_context);
				if (errstr)
					*errstr =
						pstrdup("failed to allocate position embeddings");
				return -1;
			}
			memcpy(entry->weights.position_embeddings,
				weights.position_embeddings,
				position_embed_size);

			entry->weights.lm_head_weights =
				(float *)palloc(lm_head_size);
			if (entry->weights.lm_head_weights == NULL)
			{
				NDB_FREE(entry->weights.embedding_table);
				NDB_FREE(entry->weights.position_embeddings);
				MemoryContextDelete(embed_context);
				if (errstr)
					*errstr = pstrdup("failed to allocate LM head weights");
				return -1;
			}
			memcpy(entry->weights.lm_head_weights,
				weights.lm_head_weights,
				lm_head_size);

		entry->weights.total_bytes = weights.total_bytes;
	}

	entry->loaded = true;
		entry->weights_on_device = false;
		entry->last_used = time(NULL);
		entry->next = g_model_cache;
		g_model_cache = entry;
		g_model_cache_count++;

		/* Switch back to original context before deleting temp context */
		MemoryContextSwitchTo(oldcontext);

		/* Delete temporary context used for loading */
		MemoryContextDelete(embed_context);
		embed_context = NULL;
	} else
	{
		/* Validate cached entry */
		if (entry->weights.embedding_table == NULL)
		{
			if (errstr)
				*errstr = psprintf("cached model '%s' has NULL weights",
					model_name);
			return -1;
		}
		config = entry->config;
		entry->last_used = time(NULL);
	}

	/* Validate config from cache */
	if (config.vocab_size <= 0 || config.embed_dim <= 0
		|| config.max_seq_len <= 0)
	{
		if (errstr)
			*errstr = psprintf("invalid config for model '%s'",
				model_name);
		return -1;
	}

	/* Create memory context for this operation's temporary allocations */
	embed_context = AllocSetContextCreate(CurrentMemoryContext,
		"rocm_hf_embed_ctx",
		ALLOCSET_DEFAULT_SIZES);
	if (embed_context == NULL)
	{
		if (errstr)
			*errstr = pstrdup("failed to create embed memory context");
		return -1;
	}
	oldcontext = MemoryContextSwitchTo(embed_context);

	embed_dim = config.embed_dim;

	/* Validate embedding dimension */
	if (embed_dim <= 0 || embed_dim > 10000)
	{
		MemoryContextSwitchTo(oldcontext);
		MemoryContextDelete(embed_context);
		if (errstr)
			*errstr = psprintf("invalid embed_dim: %d", embed_dim);
		return -1;
	}

	/* Tokenize text */
	rc = ndb_rocm_hf_tokenize_text(
		text, model_name, token_ids, attention_mask, &seq_len, errstr);
	if (rc != 0)
	{
		MemoryContextSwitchTo(oldcontext);
		MemoryContextDelete(embed_context);
		return -1;
	}

	/* Validate sequence length */
	if (seq_len <= 0 || seq_len > NDB_HF_MAX_SEQ_LEN)
	{
		MemoryContextSwitchTo(oldcontext);
		MemoryContextDelete(embed_context);
		if (errstr)
			*errstr = psprintf("invalid sequence length: %d", seq_len);
		return -1;
	}

	/* Allocate embedding table on device if not already loaded */
	if (!entry->weights_on_device || entry->device_weights_ptr == NULL)
	{
		embed_table_bytes =
			config.vocab_size * config.embed_dim * sizeof(float);

		/* Check for overflow */
		if (embed_table_bytes / sizeof(float)
			!= (size_t) config.vocab_size * config.embed_dim)
		{
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(embed_context);
			if (errstr)
				*errstr = pstrdup("embedding table size overflow");
			return -1;
		}

		/* Allocate device memory */
		hip_status = hipMalloc(
			(void **)&d_embedding_table, embed_table_bytes);
		if (hip_status != hipSuccess)
		{
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(embed_context);
			if (errstr)
				*errstr = psprintf("ROCm malloc failed: %s",
					hipGetErrorString(hip_status));
			return -1;
		}

		/* Validate embedding table pointer */
		if (entry->weights.embedding_table == NULL)
		{
			hipFree(d_embedding_table);
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(embed_context);
			if (errstr)
				*errstr = pstrdup("embedding table is NULL");
			return -1;
		}

		/* Copy embedding table to device */
		hip_status = hipMemcpy(d_embedding_table,
			entry->weights.embedding_table,
			embed_table_bytes,
			hipMemcpyHostToDevice);
		if (hip_status != hipSuccess)
		{
			hipFree(d_embedding_table);
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(embed_context);
			if (errstr)
				*errstr = psprintf("ROCm memcpy failed: %s",
					hipGetErrorString(hip_status));
			return -1;
		}

		/* Verify copy succeeded */
		hip_status = hipGetLastError();
		if (hip_status != hipSuccess)
		{
			hipFree(d_embedding_table);
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(embed_context);
			if (errstr)
				*errstr = psprintf("ROCm error after memcpy: %s",
					hipGetErrorString(hip_status));
			return -1;
		}

		entry->device_weights_ptr = d_embedding_table;
		entry->device_weights_bytes = embed_table_bytes;
		entry->weights_on_device = true;
	} else
	{
		/* Validate device pointer */
		if (entry->device_weights_ptr == NULL)
		{
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(embed_context);
			if (errstr)
				*errstr = pstrdup("device weights pointer is NULL");
			return -1;
		}
		d_embedding_table = (float *)entry->device_weights_ptr;
	}

	/* Allocate output embedding in embed context */
	embedding = (float *)palloc(sizeof(float) * embed_dim);
	if (embedding == NULL)
	{
		MemoryContextSwitchTo(oldcontext);
		MemoryContextDelete(embed_context);
		if (errstr)
			*errstr = pstrdup("failed to allocate output embedding");
		return -1;
	}

	/* Call ROCm kernel for embedding inference */
	rc = ndb_rocm_hf_embed_inference(model_name,
		token_ids,
		attention_mask,
		seq_len,
		d_embedding_table,
		config.vocab_size,
		embed_dim,
		embedding,
		errstr);
	if (rc != 0)
	{
		MemoryContextSwitchTo(oldcontext);
		MemoryContextDelete(embed_context);
		return -1;
	}

	/* Validate embedding output */
	if (embedding == NULL)
	{
		MemoryContextSwitchTo(oldcontext);
		MemoryContextDelete(embed_context);
		if (errstr)
			*errstr = pstrdup("embedding inference returned NULL");
		return -1;
	}

	/* Copy embedding to parent context */
	MemoryContextSwitchTo(oldcontext);
	*vec_out = (float *)palloc(sizeof(float) * embed_dim);
	if (*vec_out == NULL)
	{
		MemoryContextDelete(embed_context);
		if (errstr)
			*errstr = pstrdup("failed to allocate output vector");
		return -1;
	}
	memcpy(*vec_out, embedding, sizeof(float) * embed_dim);
	*dim_out = embed_dim;

	/* Delete temporary context */
	MemoryContextDelete(embed_context);

	return 0;
}

/*
 * ndb_rocm_hf_parse_gen_params is now replaced by ndb_json_parse_gen_params from neurondb_json.h
 * The old implementation is removed - use centralized JSON parsing
 */
static int
ndb_rocm_hf_parse_gen_params_OLD_REMOVED(const char *params_json,
							 const char *model_name,
							 NdbRocmHfGenParams *gen_params,
							 char **errstr)
{
	char *json_copy = NULL;
	char *p = NULL;
	char *key = NULL;
	char *value = NULL;
	char *endptr = NULL;
	float float_val;
	int int_val;
	int i;

	if (errstr)
		*errstr = NULL;
	if (!params_json || !gen_params)
	{
		if (errstr)
			*errstr = pstrdup(
				"invalid parameters for parse_gen_params");
		return -1;
	}

	/* Initialize with defaults */
	memset(gen_params, 0, sizeof(NdbRocmHfGenParams));
	gen_params->temperature = 1.0f;
	gen_params->top_p = 1.0f;
	gen_params->top_k = 0; /* 0 = disabled */
	gen_params->max_tokens = 100;
	gen_params->min_tokens = 0;
	gen_params->repetition_penalty = 1.0f;
	gen_params->do_sample = false;
	gen_params->num_stop_sequences = 0;
	gen_params->return_prompt = false;
	gen_params->seed = 0;
	gen_params->streaming = false;
	gen_params->num_logit_bias = 0;

	/* Skip empty JSON */
	if (strlen(params_json) == 0 || strcmp(params_json, "{}") == 0)
		return 0;

	/* Simple JSON parsing - find key-value pairs */
	json_copy = pstrdup(params_json);
	p = json_copy;

	/* Skip whitespace and opening brace */
	while (*p && (isspace((unsigned char)*p) || *p == '{'))
		p++;

	/* Parse key-value pairs */
	while (*p && *p != '}')
	{
		/* Skip whitespace and commas */
		while (*p && (isspace((unsigned char)*p) || *p == ','))
			p++;

		if (*p == '}' || *p == '\0')
			break;

		/* Find key */
		if (*p != '"')
		{
			NDB_FREE(json_copy);
			if (errstr)
				*errstr = pstrdup(
					"invalid JSON format: expected key");
			return -1;
		}
		p++; /* Skip opening quote */
		key = p;
		while (*p && *p != '"')
			p++;
		if (*p != '"')
		{
			NDB_FREE(json_copy);
			if (errstr)
				*errstr = pstrdup("invalid JSON format: "
						  "unterminated key");
			return -1;
		}
		*p = '\0'; /* Null-terminate key */
		p++; /* Skip closing quote */

		/* Skip colon */
		while (*p && (isspace((unsigned char)*p) || *p == ':'))
			p++;

		/* Parse value based on key */
		if (strcmp(key, "temperature") == 0)
		{
			float_val = strtof(p, &endptr);
			if (endptr != p && float_val > 0.0f)
				gen_params->temperature = float_val;
		} else if (strcmp(key, "top_p") == 0)
		{
			float_val = strtof(p, &endptr);
			if (endptr != p && float_val > 0.0f
				&& float_val <= 1.0f)
				gen_params->top_p = float_val;
		} else if (strcmp(key, "top_k") == 0)
		{
			int_val = (int)strtol(p, &endptr, 10);
			if (endptr != p && int_val >= 0)
				gen_params->top_k = int_val;
		} else if (strcmp(key, "max_tokens") == 0
			|| strcmp(key, "max_length") == 0)
		{
			int_val = (int)strtol(p, &endptr, 10);
			if (endptr != p && int_val > 0)
				gen_params->max_tokens = int_val;
		} else if (strcmp(key, "min_tokens") == 0
			|| strcmp(key, "min_length") == 0)
		{
			int_val = (int)strtol(p, &endptr, 10);
			if (endptr != p && int_val >= 0)
				gen_params->min_tokens = int_val;
		} else if (strcmp(key, "repetition_penalty") == 0)
		{
			float_val = strtof(p, &endptr);
			if (endptr != p && float_val > 0.0f)
				gen_params->repetition_penalty = float_val;
		} else if (strcmp(key, "do_sample") == 0)
		{
			if (strncmp(p, "true", 4) == 0
				|| strncmp(p, "TRUE", 4) == 0)
				gen_params->do_sample = true;
			else if (strncmp(p, "false", 5) == 0
				|| strncmp(p, "FALSE", 5) == 0)
				gen_params->do_sample = false;
		} else if (strcmp(key, "return_prompt") == 0)
		{
			if (strncmp(p, "true", 4) == 0
				|| strncmp(p, "TRUE", 4) == 0)
				gen_params->return_prompt = true;
			else if (strncmp(p, "false", 5) == 0
				|| strncmp(p, "FALSE", 5) == 0)
				gen_params->return_prompt = false;
		} else if (strcmp(key, "seed") == 0)
		{
			int_val = (int)strtol(p, &endptr, 10);
			if (endptr != p)
				gen_params->seed = int_val;
		} else if (strcmp(key, "streaming") == 0
			|| strcmp(key, "stream") == 0)
		{
			if (strncmp(p, "true", 4) == 0
				|| strncmp(p, "TRUE", 4) == 0)
				gen_params->streaming = true;
			else if (strncmp(p, "false", 5) == 0
				|| strncmp(p, "FALSE", 5) == 0)
				gen_params->streaming = false;
		} else if (strcmp(key, "logit_bias") == 0
			|| strcmp(key, "bias") == 0)
		{
			/* Parse logit bias map: {"token_id": bias_value, ...} or [token_id, bias_value, ...] */
			if (*p == '{')
			{
				/* JSON object format: {"token_id": bias_value, ...} */
				p++; /* Skip '{' */
				i = 0;
				while (*p && *p != '}' && i < 256)
				{
					/* Skip whitespace and commas */
					while (*p
						&& (isspace((unsigned char)*p)
							|| *p == ','))
						p++;

					if (*p == '}')
						break;

					/* Parse token ID (key) - can be quoted or unquoted number */
					if (*p == '"')
					{
						/* Quoted key: "123": 0.5 */
						p++; /* Skip opening quote */
						int_val = (int)strtol(
							p, &endptr, 10);
						if (endptr != p
							&& *endptr == '"'
							&& int_val >= 0)
						{
							p = endptr
								+ 1; /* Skip closing quote */
							gen_params
								->logit_bias_tokens
									[i] =
								(int32_t)
									int_val;

							/* Skip colon */
							while (*p
								&& (isspace((
									    unsigned char)*p)
									|| *p == ':'))
								p++;

							/* Parse bias value */
							float_val = strtof(
								p, &endptr);
							if (endptr != p)
							{
								gen_params->logit_bias_values
									[i] =
									float_val;
								p = endptr;
								i++;
								gen_params
									->num_logit_bias =
									i;
							}
						} else
						{
							/* Invalid token ID, skip to next entry */
							while (*p && *p != '"'
								&& *p != '}')
								p++;
							if (*p == '"')
								p++;
							/* Skip value */
							while (*p && *p != ','
								&& *p != '}')
							{
								if (*p == '"')
								{
									p++;
									while (*p
										&& *p != '"')
										p++;
									if (*p == '"')
										p++;
								} else
									p++;
							}
						}
					} else if (isdigit((unsigned char)*p)
						|| *p == '-' || *p == '+')
					{
						/* Unquoted numeric key: 123: 0.5 */
						int_val = (int)strtol(
							p, &endptr, 10);
						if (endptr != p && int_val >= 0)
						{
							p = endptr;
							gen_params
								->logit_bias_tokens
									[i] =
								(int32_t)
									int_val;

							/* Skip colon */
							while (*p
								&& (isspace((
									    unsigned char)*p)
									|| *p == ':'))
								p++;

							/* Parse bias value */
							float_val = strtof(
								p, &endptr);
							if (endptr != p)
							{
								gen_params->logit_bias_values
									[i] =
									float_val;
								p = endptr;
								i++;
								gen_params
									->num_logit_bias =
									i;
							}
						} else
						{
							/* Invalid format, skip to next entry */
							while (*p && *p != ','
								&& *p != '}')
								p++;
						}
					} else
					{
						/* Unexpected character, skip to next entry */
						while (*p && *p != ','
							&& *p != '}')
							p++;
					}
				}
			} else if (*p == '[')
			{
				/* Array format: [token_id, bias_value, ...] */
				p++; /* Skip '[' */
				i = 0;
				while (*p && *p != ']' && i < 256)
				{
					/* Skip whitespace and commas */
					while (*p
						&& (isspace((unsigned char)*p)
							|| *p == ','))
						p++;

					if (*p == ']')
						break;

					/* Parse token ID */
					int_val = (int)strtol(p, &endptr, 10);
					if (endptr != p && int_val >= 0)
					{
						gen_params
							->logit_bias_tokens[i] =
							(int32_t)int_val;
						p = endptr;

						/* Skip comma */
						while (*p
							&& (isspace((
								    unsigned char)*p)
								|| *p == ','))
							p++;

						/* Parse bias value */
						float_val = strtof(p, &endptr);
						if (endptr != p)
						{
							gen_params
								->logit_bias_values
									[i] =
								float_val;
							p = endptr;
							i++;
							gen_params
								->num_logit_bias =
								i;
						}
					} else
					{
						break;
					}
				}
			}
		} else if (strcmp(key, "stop_sequences") == 0
			|| strcmp(key, "stop") == 0)
		{
			/* Parse stop sequences - supports both string and array format */
#ifdef HAVE_ONNX_RUNTIME
			int32_t *stop_token_ids = NULL;
			int32_t stop_token_len = 0;
#endif

			if (*p == '"')
			{
				/* Single string format: "stop" */
				p++; /* Skip opening quote */
				value = p;
				while (*p && *p != '"')
				{
					/* Handle escape sequences */
					if (*p == '\\' && *(p + 1) != '\0')
						p += 2;
					else
						p++;
				}
				if (*p == '"')
				{
					*p = '\0'; /* Null-terminate value */
					p++; /* Skip closing quote */

					/* Tokenize stop sequence using proper tokenizer */
#ifdef HAVE_ONNX_RUNTIME

					PG_TRY();
					{
						stop_token_ids =
							neurondb_tokenize_with_model(
								value,
								NDB_HF_MAX_STOP_SEQ_LEN,
								&stop_token_len,
								model_name);
						if (stop_token_ids
							&& stop_token_len > 0
							&& stop_token_len
								<= NDB_HF_MAX_STOP_SEQ_LEN)
						{
							/* Copy token IDs to stop sequences array */
							int j;
							int actual_len = 0;

							/* Skip CLS token (first token) and SEP token (last token) if present */
							int start_idx =
								(stop_token_len > 0
									&& stop_token_ids
											[0]
										== 101)
								? 1
								: 0;
							int end_idx =
								(stop_token_len > 1
									&& stop_token_ids[stop_token_len
										   - 1]
										== 102)
								? stop_token_len
									- 1
								: stop_token_len;

							for (j = start_idx;
								j < end_idx
								&& actual_len
									< NDB_HF_MAX_STOP_SEQ_LEN;
								j++)
							{
								gen_params->stop_sequences
									[0]
									[actual_len] =
									stop_token_ids
										[j];
								actual_len++;
							}

							if (actual_len > 0)
							{
								gen_params->stop_seq_lens
									[0] =
									actual_len;
								gen_params
									->num_stop_sequences =
									1;
							}

							NDB_FREE(stop_token_ids);
						} else
						{
							if (stop_token_ids)
								NDB_FREE(stop_token_ids);
						}
					}
					PG_CATCH();
					{
						FlushErrorState();
						if (stop_token_ids)
							NDB_FREE(stop_token_ids);
					}
					PG_END_TRY();
#endif
				}
			} else if (*p == '[')
			{
				/* Array format: ["\n", "END"] */
				p++; /* Skip '[' */
				i = 0;
				while (*p && *p != ']'
					&& i < NDB_HF_MAX_STOP_SEQUENCES)
				{
					/* Skip whitespace and commas */
					while (*p
						&& (isspace((unsigned char)*p)
							|| *p == ','))
						p++;

					if (*p == ']')
						break;

					/* Parse string value */
					if (*p == '"')
					{
						p++; /* Skip opening quote */
						value = p;
						while (*p && *p != '"')
						{
							/* Handle escape sequences */
							if (*p == '\\'
								&& *(p + 1)
									!= '\0')
								p += 2;
							else
								p++;
						}
						if (*p == '"')
						{
							*p = '\0'; /* Null-terminate value */
							p++; /* Skip closing quote */

							/* Tokenize stop sequence using proper tokenizer */
#ifdef HAVE_ONNX_RUNTIME
							stop_token_ids = NULL;
							stop_token_len = 0;

							PG_TRY();
							{
								stop_token_ids = neurondb_tokenize_with_model(
									value,
									NDB_HF_MAX_STOP_SEQ_LEN,
									&stop_token_len,
									model_name);
								if (stop_token_ids
									&& stop_token_len
										> 0
									&& stop_token_len
										<= NDB_HF_MAX_STOP_SEQ_LEN)
								{
									/* Copy token IDs to stop sequences array */
									int j;
									int actual_len =
										0;

									/* Skip CLS token (first token) and SEP token (last token) if present */
									int start_idx =
										(stop_token_len > 0
											&& stop_token_ids
													[0]
												== 101)
										? 1
										: 0;
									int end_idx =
										(stop_token_len > 1
											&& stop_token_ids[stop_token_len
												   - 1]
												== 102)
										? stop_token_len
											- 1
										: stop_token_len;

									for (j = start_idx;
										j < end_idx
										&& actual_len
											< NDB_HF_MAX_STOP_SEQ_LEN;
										j++)
									{
										gen_params
											->stop_sequences
												[i]
												[actual_len] = stop_token_ids
											[j];
										actual_len++;
									}

									if (actual_len
										> 0)
									{
										gen_params
											->stop_seq_lens
												[i] =
											actual_len;
										gen_params
											->num_stop_sequences++;
										i++;
									}

									NDB_FREE(stop_token_ids);
								} else
								{
									const char *ptr;
									int word_count;
									int in_word;

									/* Fallback: use word count estimation */
									if (stop_token_ids)
										NDB_FREE(stop_token_ids);

									ptr = value;
									word_count = 0;
									in_word = 0;

									while (*ptr)
									{
										if (!isspace((
											    unsigned char)*ptr))
										{
											if (!in_word)
											{
												word_count++;
												in_word =
													1;
											}
										} else
										{
											in_word =
												0;
										}
										ptr++;
									}

									if (word_count
										> 0)
									{
										/* Use simplified tokenization for fallback */
										gen_params
											->stop_seq_lens
												[i] =
											word_count
												> NDB_HF_MAX_STOP_SEQ_LEN
											? NDB_HF_MAX_STOP_SEQ_LEN
											: word_count;
										gen_params
											->num_stop_sequences++;
										i++;
									}
								}
							}
							PG_CATCH();
							{
								/* On error, skip this stop sequence */
								EmitErrorReport();
								FlushErrorState();
								if (stop_token_ids)
									NDB_FREE(stop_token_ids);
								/* Continue with next stop sequence */
							}
							PG_END_TRY();
#else
							/* ONNX runtime not available, use word count fallback */
							const char *ptr;
							int word_count;
							int in_word;

							ptr = value;
							word_count = 0;
							in_word = 0;

							while (*ptr)
							{
								if (!isspace((
									    unsigned char)*ptr))
								{
									if (!in_word)
									{
										word_count++;
										in_word =
											1;
									}
								} else
								{
									in_word =
										0;
								}
								ptr++;
							}

							if (word_count > 0)
							{
								/* Use simplified tokenization for fallback */
								gen_params->stop_seq_lens
									[i] =
									word_count
										> NDB_HF_MAX_STOP_SEQ_LEN
									? NDB_HF_MAX_STOP_SEQ_LEN
									: word_count;
								gen_params
									->num_stop_sequences++;
								i++;
							}
#endif
						}
					}
				}
			} else if (*p == '"')
			{
				/* Single string format: "\n" */
				p++; /* Skip opening quote */
				value = p;
				while (*p && *p != '"')
				{
					if (*p == '\\' && *(p + 1) != '\0')
						p += 2;
					else
						p++;
				}
				if (*p == '"')
				{
					*p = '\0';
					p++; /* Skip closing quote */

					/* Tokenize stop sequence using proper tokenizer */
#ifdef HAVE_ONNX_RUNTIME
					stop_token_ids = NULL;
					stop_token_len = 0;
					PG_TRY();
					{
						stop_token_ids =
							neurondb_tokenize_with_model(
								value,
								NDB_HF_MAX_STOP_SEQ_LEN,
								&stop_token_len,
								model_name);
						if (stop_token_ids
							&& stop_token_len > 0
							&& stop_token_len
								<= NDB_HF_MAX_STOP_SEQ_LEN)
						{
							/* Copy token IDs to stop sequences array */
							int j;
							int actual_len = 0;

							/* Skip CLS token (first token) and SEP token (last token) if present */
							int start_idx =
								(stop_token_ids[0]
									== 101)
								? 1
								: 0;
							int end_idx =
								(stop_token_len > 1
									&& stop_token_ids[stop_token_len
										   - 1]
										== 102)
								? stop_token_len
									- 1
								: stop_token_len;

							for (j = start_idx;
								j < end_idx
								&& actual_len
									< NDB_HF_MAX_STOP_SEQ_LEN;
								j++)
							{
								gen_params->stop_sequences
									[0]
									[actual_len] =
									stop_token_ids
										[j];
								actual_len++;
							}

							if (actual_len > 0)
							{
								gen_params->stop_seq_lens
									[0] =
									actual_len;
								gen_params
									->num_stop_sequences =
									1;
							}

					NDB_FREE(stop_token_ids);
				} else
				{
					/* Fallback: use word count estimation */
					const char *ptr = value;
					int word_count = 0;
					int in_word = 0;

					if (stop_token_ids)
						NDB_FREE(stop_token_ids);

					while (*ptr)
							{
								if (!isspace((
									    unsigned char)*ptr))
								{
									if (!in_word)
									{
										word_count++;
										in_word =
											1;
									}
								} else
								{
									in_word =
										0;
								}
								ptr++;
							}

							if (word_count > 0)
							{
								/* Use simplified tokenization for fallback */
								gen_params->stop_seq_lens
									[0] =
									word_count
										> NDB_HF_MAX_STOP_SEQ_LEN
									? NDB_HF_MAX_STOP_SEQ_LEN
									: word_count;
								gen_params
									->num_stop_sequences =
									1;
							}
						}
					}
				PG_CATCH();
				{
					/* On error, skip this stop sequence */
					const char *ptr = value;
					int word_count = 0;
					int in_word = 0;

					EmitErrorReport();
					FlushErrorState();
					if (stop_token_ids)
						NDB_FREE(stop_token_ids);
					/* Use fallback word count */

					while (*ptr)
						{
							if (!isspace((
								    unsigned char)*ptr))
							{
								if (!in_word)
								{
									word_count++;
									in_word =
										1;
								}
							} else
							{
								in_word = 0;
							}
							ptr++;
						}

						if (word_count > 0)
						{
							gen_params
								->stop_seq_lens
									[0] =
								word_count
									> NDB_HF_MAX_STOP_SEQ_LEN
								? NDB_HF_MAX_STOP_SEQ_LEN
								: word_count;
							gen_params
								->num_stop_sequences =
								1;
						}
					}
					PG_END_TRY();
#else
					/* ONNX runtime not available, use word count fallback */
					const char *ptr;
					int word_count;
					int in_word;

					ptr = value;
					word_count = 0;
					in_word = 0;

					while (*ptr)
					{
						if (!isspace((
							    unsigned char)*ptr))
						{
							if (!in_word)
							{
								word_count++;
								in_word = 1;
							}
						} else
						{
							in_word = 0;
						}
						ptr++;
					}

					if (word_count > 0)
					{
						gen_params->stop_seq_lens[0] =
							word_count
								> NDB_HF_MAX_STOP_SEQ_LEN
							? NDB_HF_MAX_STOP_SEQ_LEN
							: word_count;
						gen_params->num_stop_sequences =
							1;
					}
#endif
				}
			}
		}

		/* Skip to next key-value pair */
		while (*p && *p != ',' && *p != '}')
		{
			if (*p == '"')
			{
				p++; /* Skip opening quote */
				while (*p && *p != '"')
				{
					if (*p == '\\' && *(p + 1) != '\0')
						p += 2;
					else
						p++;
				}
				if (*p == '"')
					p++; /* Skip closing quote */
			} else if (*p == '[')
			{
				/* Skip array */
				int depth = 1;
				p++;
				while (*p && depth > 0)
				{
					if (*p == '[')
						depth++;
					else if (*p == ']')
						depth--;
					p++;
				}
			} else
			{
				p++;
			}
		}
	}

	NDB_FREE(json_copy);
	return 0;
}
/* OLD FUNCTION REMOVED - use ndb_json_parse_gen_params instead */

/*
 * ndb_rocm_hf_decode_tokens
 *	  Decode token IDs back to text
 *
 * Uses neurondb_detokenize for proper vocabulary-based decoding.
 */
static int
ndb_rocm_hf_decode_tokens(const int32_t *token_ids,
	int num_tokens,
	const char *model_name,
	char **text_out,
	char **errstr)
{
	char *decoded_text = NULL;

	if (errstr)
		*errstr = NULL;
	if (!token_ids || num_tokens <= 0 || !text_out)
	{
		if (errstr)
			*errstr =
				pstrdup("invalid parameters for decode_tokens");
		return -1;
	}

#ifdef HAVE_ONNX_RUNTIME
	/* Use proper tokenizer for detokenization */
	PG_TRY();
	{
		decoded_text = neurondb_detokenize(
			(const int32 *)token_ids, num_tokens, model_name);
		if (!decoded_text)
		{
			if (errstr)
				*errstr = pstrdup("detokenization failed");
			return -1;
		}
		*text_out = decoded_text;
		return 0;
	}
	PG_CATCH();
	{
		/* Fall back to simplified decoding on error */
		FlushErrorState();
	}
	PG_END_TRY();
#endif

	/* Fallback: Simplified decoding for non-ONNX builds or on error */
	{
		StringInfoData buf;
		int i;

		initStringInfo(&buf);

		for (i = 0; i < num_tokens; i++)
		{
			int32_t token_id = token_ids[i];

			/* Skip special tokens */
			if (token_id == 101 || token_id == 102 || token_id == 0)
				continue; /* [CLS], [SEP], [PAD] */

			/* Map token ID to character (simplified) */
			if (token_id > 0 && token_id < 128)
			{
				appendStringInfoChar(&buf, (char)token_id);
			} else
			{
				/* For tokens outside ASCII, use placeholder */
				appendStringInfo(
					&buf, " [token_%d] ", token_id);
			}
		}

		*text_out = buf.data;
		return 0;
	}
}

/*
 * ndb_rocm_hf_init_kv_cache
 *	  Initialize KV cache for autoregressive generation
 */
static int
ndb_rocm_hf_init_kv_cache(NdbRocmHfKVCache *kv_cache,
	const NdbRocmHfModelConfig *config,
	char **errstr)
{
	hipError_t hip_status;
	size_t cache_bytes;
	int head_dim;

	if (errstr)
		*errstr = NULL;
	if (!kv_cache || !config)
	{
		if (errstr)
			*errstr =
				pstrdup("invalid parameters for init_kv_cache");
		return -1;
	}

	head_dim = config->embed_dim / config->num_heads;
	cache_bytes = sizeof(float) * config->num_layers * config->max_seq_len
		* config->num_heads * head_dim;

	/* Allocate key cache on device */
	hip_status = hipMalloc((void **)&kv_cache->key_cache, cache_bytes);
	if (hip_status != hipSuccess)
	{
		if (errstr)
			*errstr =
				psprintf("ROCm malloc failed for key cache: %s",
					hipGetErrorString(hip_status));
		return -1;
	}

	/* Allocate value cache on device */
	hip_status = hipMalloc((void **)&kv_cache->value_cache, cache_bytes);
	if (hip_status != hipSuccess)
	{
		hipFree(kv_cache->key_cache);
		if (errstr)
			*errstr = psprintf(
				"ROCm malloc failed for value cache: %s",
				hipGetErrorString(hip_status));
		return -1;
	}

	/* Initialize cache */
	kv_cache->current_pos = 0;
	kv_cache->max_pos = config->max_seq_len;
	kv_cache->num_layers = config->num_layers;
	kv_cache->num_heads = config->num_heads;
	kv_cache->head_dim = head_dim;
	kv_cache->allocated = true;

	return 0;
}

/*
 * ndb_rocm_hf_free_kv_cache
 *	  Free KV cache memory
 */
static void
ndb_rocm_hf_free_kv_cache(NdbRocmHfKVCache *kv_cache)
{
	if (!kv_cache || !kv_cache->allocated)
		return;

	if (kv_cache->key_cache)
		hipFree(kv_cache->key_cache);
	if (kv_cache->value_cache)
		hipFree(kv_cache->value_cache);

	kv_cache->key_cache = NULL;
	kv_cache->value_cache = NULL;
	kv_cache->allocated = false;
}

/*
 * ndb_rocm_hf_complete
 *	  Generate text completion using ROCm-accelerated Hugging Face model.
 *
 * This implements a complete text generation pipeline:
 * 1. Parse generation parameters from JSON
 * 2. Load or find model in cache
 * 3. Tokenize input prompt
 * 4. Initialize KV cache for autoregressive generation
 * 5. Run autoregressive generation loop on GPU:
 *    a. Compute attention with KV cache
 *    b. Apply transformer layers
 *    c. Compute logits from language model head
 *    d. Apply temperature, top-k, top-p, repetition penalty
 *    e. Sample next token (greedy or multinomial)
 *    f. Update KV cache
 *    g. Check for stop sequences
 * 6. Decode generated token IDs to text
 * 7. Return generated text
 */
int
ndb_rocm_hf_complete(const char *model_name,
	const char *prompt,
	const char *params_json,
	char **text_out,
	char **errstr)
{
	NdbRocmHfModelEntry *entry = NULL;
	NdbRocmHfModelConfig config;
	NdbRocmHfModelWeights weights;
	NdbRocmHfGenParams gen_params;
	NdbRocmHfKVCache kv_cache;
	int32_t input_token_ids[NDB_HF_MAX_SEQ_LEN];
	int32_t attention_mask[NDB_HF_MAX_SEQ_LEN];
	int32_t output_token_ids[NDB_HF_MAX_GEN_TOKENS];
	int input_seq_len = 0;
	int output_seq_len = 0;
	char *generated_text = NULL;
	int rc = -1;
	MemoryContext oldcontext;
	MemoryContext complete_context;
	hipError_t hip_status;
	float *d_embedding_table = NULL;
	float *d_position_embeddings = NULL;
	float *d_lm_head_weights = NULL;
	size_t embed_table_bytes;
	size_t position_embed_bytes;
	size_t lm_head_bytes;

	if (errstr)
		*errstr = NULL;
	if (model_name == NULL || prompt == NULL || text_out == NULL)
	{
		if (errstr)
			*errstr = pstrdup(
				"invalid parameters for ROCm HF complete");
		return -1;
	}

	/* Create memory context for this operation */
	complete_context = AllocSetContextCreate(CurrentMemoryContext,
		"rocm_hf_complete_ctx",
		ALLOCSET_DEFAULT_SIZES);
	oldcontext = MemoryContextSwitchTo(complete_context);

	/* Parse generation parameters */
	if (params_json && strlen(params_json) > 0)
	{
		char	   *temp_errstr = NULL;

		NdbGenParams ndb_params = {0};
		rc = ndb_json_parse_gen_params(params_json, &ndb_params, &temp_errstr);
		if (rc != 0)
		{
			/* Copy error string to parent context before deleting temp context */
			MemoryContextSwitchTo(oldcontext);
			if (temp_errstr && errstr)
				*errstr = pstrdup(temp_errstr);
			ndb_json_parse_gen_params_free(&ndb_params);
			MemoryContextDelete(complete_context);
			return -1;
		}
		/* Map from NdbGenParams to NdbRocmHfGenParams */
		gen_params.temperature = ndb_params.temperature;
		gen_params.top_p = ndb_params.top_p;
		gen_params.top_k = ndb_params.top_k;
		gen_params.max_tokens = ndb_params.max_tokens;
		gen_params.min_tokens = ndb_params.min_tokens;
		gen_params.repetition_penalty = ndb_params.repetition_penalty;
		gen_params.do_sample = ndb_params.do_sample;
		gen_params.return_prompt = ndb_params.return_prompt;
		gen_params.seed = ndb_params.seed;
		gen_params.streaming = ndb_params.streaming;
		/* Copy stop sequences (limited by fixed array size) */
		gen_params.num_stop_sequences = (ndb_params.num_stop_sequences < NDB_HF_MAX_STOP_SEQUENCES) ?
			ndb_params.num_stop_sequences : NDB_HF_MAX_STOP_SEQUENCES;
		/* Copy logit bias (limited by fixed array size) */
		gen_params.num_logit_bias = (ndb_params.num_logit_bias < 256) ?
			ndb_params.num_logit_bias : 256;
		for (int i = 0; i < gen_params.num_logit_bias && i < 256; i++)
		{
			gen_params.logit_bias_tokens[i] = ndb_params.logit_bias_tokens[i];
			gen_params.logit_bias_values[i] = ndb_params.logit_bias_values[i];
		}
		ndb_json_parse_gen_params_free(&ndb_params);
	} else
	{
		/* Use defaults */
		memset(&gen_params, 0, sizeof(NdbRocmHfGenParams));
		gen_params.temperature = 1.0f;
		gen_params.top_p = 1.0f;
		gen_params.top_k = 0;
		gen_params.max_tokens = 100;
		gen_params.min_tokens = 0;
		gen_params.repetition_penalty = 1.0f;
		gen_params.do_sample = false;
		gen_params.num_stop_sequences = 0;
		gen_params.return_prompt = false;
		gen_params.seed = 0;
	}

	/* Find or load model */
	entry = ndb_rocm_hf_find_model(model_name);
	if (entry == NULL || !entry->loaded)
	{
		char	   *temp_errstr = NULL;

		/* Load model (for now, use dummy weights) */
		rc = ndb_rocm_hf_load_model_weights(
			model_name, NULL, &config, &weights, &temp_errstr);
		if (rc != 0)
		{
			/* Copy error string to parent context before deleting temp context */
			MemoryContextSwitchTo(oldcontext);
			if (temp_errstr && errstr)
				*errstr = pstrdup(temp_errstr);
			MemoryContextDelete(complete_context);
			return -1;
		}

		/* Set model type to generation */
		config.model_type = NDB_HF_MODEL_GENERATION;

		/* Switch to CacheMemoryContext for persistent cache entry */
		MemoryContextSwitchTo(oldcontext);
		oldcontext = MemoryContextSwitchTo(CacheMemoryContext);

		/* Allocate new cache entry in CacheMemoryContext (persistent) */
		entry = (NdbRocmHfModelEntry *)palloc(
			sizeof(NdbRocmHfModelEntry));
		memset(entry, 0, sizeof(NdbRocmHfModelEntry));
		strncpy(entry->model_name,
			model_name,
			NDB_HF_MAX_MODEL_NAME - 1);
		entry->config = config;

		/* Copy weights to CacheMemoryContext */
		{
			size_t embed_table_size =
				config.vocab_size * config.embed_dim * sizeof(float);
			size_t position_embed_size =
				config.max_seq_len * config.embed_dim * sizeof(float);
			size_t lm_head_size =
				config.vocab_size * config.embed_dim * sizeof(float);

			entry->weights.embedding_table =
				(float *)palloc(embed_table_size);
			memcpy(entry->weights.embedding_table,
				weights.embedding_table,
				embed_table_size);

			entry->weights.position_embeddings =
				(float *)palloc(position_embed_size);
			memcpy(entry->weights.position_embeddings,
				weights.position_embeddings,
				position_embed_size);

			entry->weights.lm_head_weights =
				(float *)palloc(lm_head_size);
			memcpy(entry->weights.lm_head_weights,
				weights.lm_head_weights,
				lm_head_size);

			entry->weights.total_bytes = weights.total_bytes;
		}

		entry->loaded = true;
		entry->weights_on_device = false;
		entry->last_used = time(NULL);
		entry->next = g_model_cache;
		g_model_cache = entry;
		g_model_cache_count++;

		/* Switch back to original context before deleting temp context */
		MemoryContextSwitchTo(oldcontext);

		/* Delete temporary context used for loading */
		MemoryContextDelete(complete_context);

		/* Recreate temporary context for operation */
		complete_context = AllocSetContextCreate(CurrentMemoryContext,
			"rocm_hf_complete_ctx",
			ALLOCSET_DEFAULT_SIZES);
		oldcontext = MemoryContextSwitchTo(complete_context);
	} else
	{
		config = entry->config;
		weights = entry->weights;
		entry->last_used = time(NULL);
	}

	/* Ensure model is for generation */
	if (config.model_type != NDB_HF_MODEL_GENERATION)
	{
		/* Allocate error string in parent context before deleting temp context */
		MemoryContextSwitchTo(oldcontext);
		if (errstr)
			*errstr =
				psprintf("model '%s' is not a generation model",
					model_name);
		MemoryContextDelete(complete_context);
		return -1;
	}

	/* Tokenize prompt */
	{
		char	   *temp_errstr = NULL;

		rc = ndb_rocm_hf_tokenize_text(prompt,
			model_name,
			input_token_ids,
			attention_mask,
			&input_seq_len,
			&temp_errstr);
		if (rc != 0)
		{
			/* Copy error string to parent context before deleting temp context */
			MemoryContextSwitchTo(oldcontext);
			if (temp_errstr && errstr)
				*errstr = pstrdup(temp_errstr);
			MemoryContextDelete(complete_context);
			return -1;
		}
	}

	/* Initialize KV cache */
	memset(&kv_cache, 0, sizeof(NdbRocmHfKVCache));
	{
		char	   *temp_errstr = NULL;

		rc = ndb_rocm_hf_init_kv_cache(&kv_cache, &config, &temp_errstr);
		if (rc != 0)
		{
			/* Copy error string to parent context before deleting temp context */
			MemoryContextSwitchTo(oldcontext);
			if (temp_errstr && errstr)
				*errstr = pstrdup(temp_errstr);
			MemoryContextDelete(complete_context);
			return -1;
		}
	}

	/* Allocate model weights on device if not already loaded */
	if (!entry->weights_on_device || entry->device_weights_ptr == NULL)
	{
		embed_table_bytes =
			config.vocab_size * config.embed_dim * sizeof(float);
		position_embed_bytes =
			config.max_seq_len * config.embed_dim * sizeof(float);
		lm_head_bytes =
			config.vocab_size * config.embed_dim * sizeof(float);

		hip_status = hipMalloc(
			(void **)&d_embedding_table, embed_table_bytes);
		if (hip_status != hipSuccess)
		{
			ndb_rocm_hf_free_kv_cache(&kv_cache);
			/* Allocate error string in parent context before deleting temp context */
			MemoryContextSwitchTo(oldcontext);
			if (errstr)
				*errstr = psprintf("ROCm malloc failed for "
						   "embedding table: %s",
					hipGetErrorString(hip_status));
			MemoryContextDelete(complete_context);
			return -1;
		}

		hip_status = hipMalloc(
			(void **)&d_position_embeddings, position_embed_bytes);
		if (hip_status != hipSuccess)
		{
			hipFree(d_embedding_table);
			ndb_rocm_hf_free_kv_cache(&kv_cache);
			/* Allocate error string in parent context before deleting temp context */
			MemoryContextSwitchTo(oldcontext);
			if (errstr)
				*errstr = psprintf("ROCm malloc failed for "
						   "position embeddings: %s",
					hipGetErrorString(hip_status));
			MemoryContextDelete(complete_context);
			return -1;
		}

		hip_status =
			hipMalloc((void **)&d_lm_head_weights, lm_head_bytes);
		if (hip_status != hipSuccess)
		{
			hipFree(d_embedding_table);
			hipFree(d_position_embeddings);
			ndb_rocm_hf_free_kv_cache(&kv_cache);
			/* Allocate error string in parent context before deleting temp context */
			MemoryContextSwitchTo(oldcontext);
			if (errstr)
				*errstr = psprintf("ROCm malloc failed for LM "
						   "head weights: %s",
					hipGetErrorString(hip_status));
			MemoryContextDelete(complete_context);
			return -1;
		}

		/* Copy weights to device */
		if (entry->weights.embedding_table)
		{
			hip_status = hipMemcpy(d_embedding_table,
				entry->weights.embedding_table,
				embed_table_bytes,
				hipMemcpyHostToDevice);
			if (hip_status != hipSuccess)
			{
				hipFree(d_embedding_table);
				hipFree(d_position_embeddings);
				hipFree(d_lm_head_weights);
				ndb_rocm_hf_free_kv_cache(&kv_cache);
				/* Allocate error string in parent context before deleting temp context */
				MemoryContextSwitchTo(oldcontext);
				if (errstr)
					*errstr = psprintf(
						"ROCm memcpy failed for "
						"embedding table: %s",
						hipGetErrorString(
							hip_status));
				MemoryContextDelete(complete_context);
				return -1;
			}
		}

		if (entry->weights.position_embeddings)
		{
			hip_status = hipMemcpy(d_position_embeddings,
				entry->weights.position_embeddings,
				position_embed_bytes,
				hipMemcpyHostToDevice);
			if (hip_status != hipSuccess)
			{
				hipFree(d_embedding_table);
				hipFree(d_position_embeddings);
				hipFree(d_lm_head_weights);
				ndb_rocm_hf_free_kv_cache(&kv_cache);
				/* Allocate error string in parent context before deleting temp context */
				MemoryContextSwitchTo(oldcontext);
				if (errstr)
					*errstr = psprintf(
						"ROCm memcpy failed for "
						"position embeddings: %s",
						hipGetErrorString(
							hip_status));
				MemoryContextDelete(complete_context);
				return -1;
			}
		}

		if (entry->weights.lm_head_weights)
		{
			hip_status = hipMemcpy(d_lm_head_weights,
				entry->weights.lm_head_weights,
				lm_head_bytes,
				hipMemcpyHostToDevice);
			if (hip_status != hipSuccess)
			{
				hipFree(d_embedding_table);
				hipFree(d_position_embeddings);
				hipFree(d_lm_head_weights);
				ndb_rocm_hf_free_kv_cache(&kv_cache);
				/* Allocate error string in parent context before deleting temp context */
				MemoryContextSwitchTo(oldcontext);
				if (errstr)
					*errstr = psprintf(
						"ROCm memcpy failed for LM "
						"head weights: %s",
						hipGetErrorString(
							hip_status));
				MemoryContextDelete(complete_context);
				return -1;
			}
		}

		entry->device_weights_ptr = d_embedding_table;
		entry->device_position_embeddings = d_position_embeddings;
		entry->device_lm_head_weights = d_lm_head_weights;
		entry->device_weights_bytes = embed_table_bytes;
		entry->device_position_embed_bytes = position_embed_bytes;
		entry->device_lm_head_bytes = lm_head_bytes;
		entry->weights_on_device = true;
	} else
	{
		d_embedding_table = (float *)entry->device_weights_ptr;
		d_position_embeddings =
			(float *)entry->device_position_embeddings;
		d_lm_head_weights = (float *)entry->device_lm_head_weights;
		if (!d_position_embeddings || !d_lm_head_weights)
		{
			ndb_rocm_hf_free_kv_cache(&kv_cache);
			/* Allocate error string in parent context before deleting temp context */
			MemoryContextSwitchTo(oldcontext);
			if (errstr)
				*errstr =
					psprintf("Model weights partially "
						 "loaded - position embeddings "
						 "or LM head missing");
			MemoryContextDelete(complete_context);
			return -1;
		}
	}

	/* Call ROCm generation kernel */
	{
		char	   *temp_errstr = NULL;

		rc = ndb_rocm_hf_generate_inference(model_name,
			input_token_ids,
			input_seq_len,
			d_embedding_table,
			d_position_embeddings,
			d_lm_head_weights,
			&entry->weights,
			&config,
			&gen_params,
			&kv_cache,
			output_token_ids,
			&output_seq_len,
			&temp_errstr);
		if (rc != 0)
		{
			ndb_rocm_hf_free_kv_cache(&kv_cache);
			/* Copy error string to parent context before deleting temp context */
			MemoryContextSwitchTo(oldcontext);
			if (temp_errstr && errstr)
				*errstr = pstrdup(temp_errstr);
			MemoryContextDelete(complete_context);
			return -1;
		}
	}

	/* Decode generated tokens to text */
	{
		char	   *temp_errstr = NULL;

		rc = ndb_rocm_hf_decode_tokens(output_token_ids,
			output_seq_len,
			model_name,
			&generated_text,
			&temp_errstr);
		if (rc != 0)
		{
			ndb_rocm_hf_free_kv_cache(&kv_cache);
			/* Copy error string to parent context before deleting temp context */
			MemoryContextSwitchTo(oldcontext);
			if (temp_errstr && errstr)
				*errstr = pstrdup(temp_errstr);
			MemoryContextDelete(complete_context);
			return -1;
		}
	}

	/* Free KV cache */
	ndb_rocm_hf_free_kv_cache(&kv_cache);

	/* Copy generated text to parent context */
	MemoryContextSwitchTo(oldcontext);
	if (generated_text)
	{
		*text_out = pstrdup(generated_text);
	} else
	{
		*text_out = pstrdup("");
	}

	/* Delete temporary context */
	MemoryContextDelete(complete_context);

	return 0;
}

/*
 * ndb_rocm_hf_generate_stream
 *	  Generate text with streaming callback support
 *
 * This function generates text and calls a callback for each generated token,
 * allowing for real-time streaming of generated text.
 */
int
ndb_rocm_hf_generate_stream(const char *model_name,
	const char *prompt,
	const char *params_json,
	ndb_rocm_hf_stream_callback callback,
	void *user_data,
	char **errstr)
{
	NdbRocmHfGenParams gen_params;
	NdbRocmHfModelEntry *entry = NULL;
	NdbRocmHfModelConfig config;
	NdbRocmHfModelWeights weights;
	NdbRocmHfKVCache kv_cache;
	int32_t input_token_ids[NDB_HF_MAX_SEQ_LEN];
	int32_t attention_mask[NDB_HF_MAX_SEQ_LEN];
	int32_t output_token_ids[NDB_HF_MAX_GEN_TOKENS];
	int input_seq_len = 0;
	int output_seq_len = 0;
	int rc = -1;
	MemoryContext oldcontext;
	MemoryContext stream_context;
	char token_text[256];
	int i;

	if (errstr)
		*errstr = NULL;
	if (model_name == NULL || prompt == NULL || callback == NULL)
	{
		if (errstr)
			*errstr = pstrdup("invalid parameters for ROCm HF "
					  "generate_stream");
		return -1;
	}

	/* Create memory context for this operation */
	stream_context = AllocSetContextCreate(CurrentMemoryContext,
		"rocm_hf_stream_ctx",
		ALLOCSET_DEFAULT_SIZES);
	oldcontext = MemoryContextSwitchTo(stream_context);

	/* Parse generation parameters */
	if (params_json && strlen(params_json) > 0)
	{
		NdbGenParams ndb_params = {0};
		rc = ndb_json_parse_gen_params(params_json, &ndb_params, errstr);
		if (rc != 0)
		{
			ndb_json_parse_gen_params_free(&ndb_params);
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(stream_context);
			return -1;
		}
		/* Map from NdbGenParams to NdbRocmHfGenParams */
		gen_params.temperature = ndb_params.temperature;
		gen_params.top_p = ndb_params.top_p;
		gen_params.top_k = ndb_params.top_k;
		gen_params.max_tokens = ndb_params.max_tokens;
		gen_params.min_tokens = ndb_params.min_tokens;
		gen_params.repetition_penalty = ndb_params.repetition_penalty;
		gen_params.do_sample = ndb_params.do_sample;
		gen_params.return_prompt = ndb_params.return_prompt;
		gen_params.seed = ndb_params.seed;
		gen_params.streaming = true;	/* Force streaming for this function */
		/* Copy stop sequences (limited by fixed array size) */
		gen_params.num_stop_sequences = (ndb_params.num_stop_sequences < NDB_HF_MAX_STOP_SEQUENCES) ?
			ndb_params.num_stop_sequences : NDB_HF_MAX_STOP_SEQUENCES;
		/* Copy logit bias (limited by fixed array size) */
		gen_params.num_logit_bias = (ndb_params.num_logit_bias < 256) ?
			ndb_params.num_logit_bias : 256;
		for (int i = 0; i < gen_params.num_logit_bias && i < 256; i++)
		{
			gen_params.logit_bias_tokens[i] = ndb_params.logit_bias_tokens[i];
			gen_params.logit_bias_values[i] = ndb_params.logit_bias_values[i];
		}
		ndb_json_parse_gen_params_free(&ndb_params);
	} else
	{
		/* Use defaults */
		memset(&gen_params, 0, sizeof(NdbRocmHfGenParams));
		gen_params.temperature = 1.0f;
		gen_params.top_p = 1.0f;
		gen_params.top_k = 0;
		gen_params.max_tokens = 100;
		gen_params.min_tokens = 0;
		gen_params.repetition_penalty = 1.0f;
		gen_params.do_sample = false;
		gen_params.num_stop_sequences = 0;
		gen_params.return_prompt = false;
		gen_params.seed = 0;
		gen_params.streaming = true; /* Enable streaming */
		gen_params.num_logit_bias = 0;
	}

	/* Force streaming mode */
	gen_params.streaming = true;

	/* Find or load model (reuse existing logic from ndb_rocm_hf_complete) */
	entry = ndb_rocm_hf_find_model(model_name);
	if (entry == NULL || !entry->loaded)
	{
		/* Load model */
		rc = ndb_rocm_hf_load_model_weights(
			model_name, NULL, &config, &weights, errstr);
		if (rc != 0)
		{
			MemoryContextSwitchTo(oldcontext);
			MemoryContextDelete(stream_context);
			return -1;
		}
		config.model_type = NDB_HF_MODEL_GENERATION;

		/* Switch to CacheMemoryContext for persistent cache entry */
		MemoryContextSwitchTo(oldcontext);
		oldcontext = MemoryContextSwitchTo(CacheMemoryContext);

		/* Allocate new cache entry in CacheMemoryContext (persistent) */
		entry = (NdbRocmHfModelEntry *)palloc(
			sizeof(NdbRocmHfModelEntry));
		memset(entry, 0, sizeof(NdbRocmHfModelEntry));
		strncpy(entry->model_name,
			model_name,
			NDB_HF_MAX_MODEL_NAME - 1);
		entry->config = config;

		/* Copy weights to CacheMemoryContext */
		{
			size_t embed_table_size =
				config.vocab_size * config.embed_dim * sizeof(float);
			size_t position_embed_size =
				config.max_seq_len * config.embed_dim * sizeof(float);
			size_t lm_head_size =
				config.vocab_size * config.embed_dim * sizeof(float);

			entry->weights.embedding_table =
				(float *)palloc(embed_table_size);
			memcpy(entry->weights.embedding_table,
				weights.embedding_table,
				embed_table_size);

			entry->weights.position_embeddings =
				(float *)palloc(position_embed_size);
			memcpy(entry->weights.position_embeddings,
				weights.position_embeddings,
				position_embed_size);

			entry->weights.lm_head_weights =
				(float *)palloc(lm_head_size);
			memcpy(entry->weights.lm_head_weights,
				weights.lm_head_weights,
				lm_head_size);

			entry->weights.total_bytes = weights.total_bytes;
		}

		entry->loaded = true;
		entry->weights_on_device = false;
		entry->last_used = time(NULL);
		entry->next = g_model_cache;
		g_model_cache = entry;
		g_model_cache_count++;

		/* Switch back to original context before deleting temp context */
		MemoryContextSwitchTo(oldcontext);

		/* Delete temporary context used for loading */
		MemoryContextDelete(stream_context);

		/* Recreate temporary context for operation */
		stream_context = AllocSetContextCreate(CurrentMemoryContext,
			"rocm_hf_stream_ctx",
			ALLOCSET_DEFAULT_SIZES);
		oldcontext = MemoryContextSwitchTo(stream_context);
	} else
	{
		config = entry->config;
		weights = entry->weights;
		entry->last_used = time(NULL);
	}

	if (config.model_type != NDB_HF_MODEL_GENERATION)
	{
		MemoryContextSwitchTo(oldcontext);
		MemoryContextDelete(stream_context);
		if (errstr)
			*errstr =
				psprintf("model '%s' is not a generation model",
					model_name);
		return -1;
	}

	/* Tokenize prompt */
	rc = ndb_rocm_hf_tokenize_text(prompt,
		model_name,
		input_token_ids,
		attention_mask,
		&input_seq_len,
		errstr);
	if (rc != 0)
	{
		MemoryContextSwitchTo(oldcontext);
		MemoryContextDelete(stream_context);
		return -1;
	}

	/* Initialize KV cache */
	memset(&kv_cache, 0, sizeof(NdbRocmHfKVCache));
	rc = ndb_rocm_hf_init_kv_cache(&kv_cache, &config, errstr);
	if (rc != 0)
	{
		MemoryContextSwitchTo(oldcontext);
		MemoryContextDelete(stream_context);
		return -1;
	}

	/* Run generation with streaming callback */
	/* NOTE: Current implementation generates all tokens first, then calls callback for each.
	 * For true streaming (token-by-token during generation), the ROCm kernel
	 * ndb_rocm_hf_generate_inference would need to be modified to support incremental
	 * generation with callbacks. This requires kernel-level changes to support
	 * pausing generation after each token and calling the callback.
	 * For now, we generate all tokens and then stream them via callback. */
	rc = ndb_rocm_hf_generate_inference(model_name,
		input_token_ids,
		input_seq_len,
		entry->weights.embedding_table,
		entry->weights.position_embeddings,
		entry->weights.lm_head_weights,
		&entry->weights,
		&config,
		&gen_params,
		&kv_cache,
		output_token_ids,
		&output_seq_len,
		errstr);
	if (rc != 0)
	{
		ndb_rocm_hf_free_kv_cache(&kv_cache);
		MemoryContextSwitchTo(oldcontext);
		MemoryContextDelete(stream_context);
		return -1;
	}

	/* Call callback for each generated token */
	for (i = 0; i < output_seq_len; i++)
	{
		int32_t token_id = output_token_ids[i];
		char *decoded_token = NULL;

		/* Decode single token */
		rc = ndb_rocm_hf_decode_tokens(
			&token_id, 1, model_name, &decoded_token, errstr);
		if (rc == 0 && decoded_token)
		{
			strncpy(token_text,
				decoded_token,
				sizeof(token_text) - 1);
			token_text[sizeof(token_text) - 1] = '\0';
			NDB_FREE(decoded_token);
		} else
		{
			snprintf(token_text,
				sizeof(token_text),
				"[token_%d]",
				token_id);
		}

		/* Call callback */
		callback(token_id, token_text, i, user_data);
	}

	/* Free KV cache */
	ndb_rocm_hf_free_kv_cache(&kv_cache);

	/* Switch back to parent context */
	MemoryContextSwitchTo(oldcontext);
	MemoryContextDelete(stream_context);

	return 0;
}

/*
 * ndb_rocm_hf_generate_batch
 *	  Generate text for multiple prompts in batch
 *
 * This function processes multiple prompts in parallel using ROCm streams,
 * allowing for efficient batch processing.
 */
int
ndb_rocm_hf_generate_batch(const char *model_name,
	const char **prompts,
	int num_prompts,
	const char *params_json,
	NdbRocmHfBatchResult *results,
	char **errstr)
{
	int i;
	int rc = 0;
	hipStream_t *streams = NULL;
	const int max_streams = 8; /* Maximum number of concurrent streams */
	int num_streams;
	MemoryContext oldctx;
	MemoryContext batchctx;

	if (errstr)
		*errstr = NULL;
	if (model_name == NULL || prompts == NULL || results == NULL
		|| num_prompts <= 0)
	{
		if (errstr)
			*errstr = pstrdup("invalid parameters for ROCm HF "
					  "generate_batch");
		return -1;
	}

	/* Create memory context for batch processing */
	batchctx = AllocSetContextCreate(CurrentMemoryContext,
		"ROCm HF Batch Generation",
		ALLOCSET_DEFAULT_SIZES);
	oldctx = MemoryContextSwitchTo(batchctx);

	/* Initialize results */
	for (i = 0; i < num_prompts; i++)
	{
		memset(&results[i], 0, sizeof(NdbRocmHfBatchResult));
		results[i].status = -1;
	}

	/* Limit number of streams to max_streams */
	num_streams = (num_prompts < max_streams) ? num_prompts : max_streams;

	/* Create ROCm streams for parallel processing */
	streams = (hipStream_t *)palloc0(num_streams * sizeof(hipStream_t));
	for (i = 0; i < num_streams; i++)
	{
		hipError_t cuda_err = hipStreamCreate(&streams[i]);
		if (cuda_err != hipSuccess)
		{
			/* Fall back to sequential processing if stream creation fails */
			if (errstr)
				*errstr = pstrdup("failed to create ROCm "
						  "streams, falling back to "
						  "sequential processing");
			num_streams = 0;
			break;
		}
	}

	/* Process prompts in parallel using streams */
	if (num_streams > 0)
	{
		/* Use parallel processing with streams */
		for (i = 0; i < num_prompts; i++)
		{
			int stream_idx = i % num_streams;
			char *text_out = NULL;
			char *prompt_err = NULL;

			/* Set current stream with multi-GPU support */
			{
				int			device_count = 0;
				int			selected_device = 0;

				hipGetDeviceCount(&device_count);
				if (device_count > 1)
				{
					/* Select device based on load (round-robin for now) */
					selected_device = i % device_count;
					hipSetDevice(selected_device);
				}
				else
				{
					hipSetDevice(0);
				}
			}
			hipStreamSynchronize(streams[stream_idx]);

			/* Process prompt */
			rc = ndb_rocm_hf_complete(model_name,
				prompts[i],
				params_json,
				&text_out,
				&prompt_err);
			if (rc == 0)
			{
#ifdef HAVE_ONNX_RUNTIME
				int32 token_length;
				int32 *token_ids = NULL;

				/* Count tokens from generated text */
				PG_TRY();
				{
					token_ids =
						neurondb_tokenize_with_model(
							text_out,
							2048,
							&token_length,
							model_name);
					if (token_ids && token_length > 0)
					{
						results[i].num_tokens =
							token_length;
					} else
					{
						/* Fallback: estimate from word count */
						const char *ptr = text_out;
						int word_count = 0;
						int in_word = 0;

						while (*ptr)
						{
							if (!isspace((
								    unsigned char)*ptr))
							{
								if (!in_word)
								{
									word_count++;
									in_word =
										1;
								}
							} else
							{
								in_word = 0;
							}
							ptr++;
						}
						results[i].num_tokens =
							word_count > 0
							? word_count
							: 1;
					}
					if (token_ids)
						NDB_FREE(token_ids);
				}
			PG_CATCH();
			{
				/* On error, use word count fallback */
				const char *ptr = text_out;
				int word_count = 0;
				int in_word = 0;

				EmitErrorReport();
				FlushErrorState();
				if (token_ids)
					NDB_FREE(token_ids);

				while (*ptr)
					{
						if (!isspace((
							    unsigned char)*ptr))
						{
							if (!in_word)
							{
								word_count++;
								in_word = 1;
							}
						} else
						{
							in_word = 0;
						}
						ptr++;
					}
					results[i].num_tokens =
						word_count > 0 ? word_count : 1;
				}
				PG_END_TRY();
#else
				/* ONNX runtime not available, use word count fallback */
				const char *ptr = text_out;
				int word_count = 0;
				int in_word = 0;

				while (*ptr)
				{
					if (!isspace((unsigned char)*ptr))
					{
						if (!in_word)
						{
							word_count++;
							in_word = 1;
						}
					} else
					{
						in_word = 0;
					}
					ptr++;
				}
				results[i].num_tokens =
					word_count > 0 ? word_count : 1;
#endif

				/* Copy result to parent context */
				{
					MemoryContext oldctx_temp = MemoryContextSwitchTo(CurrentMemoryContext);
					results[i].text = text_out ? pstrdup(text_out) : pstrdup("");
					results[i].status = 0;
					results[i].error = NULL;
					MemoryContextSwitchTo(oldctx_temp);
				}
				MemoryContextSwitchTo(batchctx);
				if (text_out)
					NDB_FREE(text_out);
				if (prompt_err)
					NDB_FREE(prompt_err);
			} else
			{
				MemoryContext oldctx_temp = MemoryContextSwitchTo(CurrentMemoryContext);
				results[i].status = -1;
				results[i].error = prompt_err
					? pstrdup(prompt_err)
					: pstrdup("generation failed");
				MemoryContextSwitchTo(oldctx_temp);
				MemoryContextSwitchTo(batchctx);
				if (text_out)
					NDB_FREE(text_out);
				if (prompt_err)
					NDB_FREE(prompt_err);
			}
		}

		/* Synchronize all streams */
		for (i = 0; i < num_streams; i++)
		{
			hipStreamSynchronize(streams[i]);
			hipStreamDestroy(streams[i]);
		}
		if (streams)
			NDB_FREE(streams);

		/* Check if at least one prompt succeeded */
		rc = -1;
		for (i = 0; i < num_prompts; i++)
		{
			if (results[i].status == 0)
			{
				rc = 0;
				break;
			}
		}
		if (rc != 0)
		{
			/* All prompts failed */
			if (errstr)
				*errstr = pstrdup("all batch completions failed");
		} else
		{
		}
	} else
	{
		/* Fall back to sequential processing */
		for (i = 0; i < num_prompts; i++)
		{
			char *text_out = NULL;
			char *prompt_err = NULL;

			rc = ndb_rocm_hf_complete(model_name,
				prompts[i],
				params_json,
				&text_out,
				&prompt_err);
			if (rc == 0)
			{
				int32 token_length;
				int32 *token_ids = NULL;

				/* Count tokens from generated text */
				PG_TRY();
				{
#ifdef HAVE_ONNX_RUNTIME
					token_ids =
						neurondb_tokenize_with_model(
							text_out,
							2048,
							&token_length,
							model_name);
#else
					token_ids = NULL;
					token_length = 0;
#endif
					if (token_ids && token_length > 0)
					{
						results[i].num_tokens =
							token_length;
					} else
					{
						/* Fallback: estimate from word count */
						const char *ptr = text_out;
						int word_count = 0;
						int in_word = 0;

						while (*ptr)
						{
							if (!isspace((
								    unsigned char)*ptr))
							{
								if (!in_word)
								{
									word_count++;
									in_word =
										1;
								}
							} else
							{
								in_word = 0;
							}
							ptr++;
						}
						results[i].num_tokens =
							word_count > 0
							? word_count
							: 1;
					}
					if (token_ids)
						NDB_FREE(token_ids);
				}
			PG_CATCH();
			{
				/* On error, use word count fallback */
				const char *ptr = text_out;
				int word_count = 0;
				int in_word = 0;

				EmitErrorReport();
				FlushErrorState();
				if (token_ids)
					NDB_FREE(token_ids);

				while (*ptr)
					{
						if (!isspace((
							    unsigned char)*ptr))
						{
							if (!in_word)
							{
								word_count++;
								in_word = 1;
							}
						} else
						{
							in_word = 0;
						}
						ptr++;
					}
					results[i].num_tokens =
						word_count > 0 ? word_count : 1;
				}
				PG_END_TRY();

				/* Copy result to parent context */
				{
					MemoryContext oldctx_temp = MemoryContextSwitchTo(CurrentMemoryContext);
					results[i].text = text_out ? pstrdup(text_out) : pstrdup("");
					results[i].status = 0;
					results[i].error = NULL;
					MemoryContextSwitchTo(oldctx_temp);
				}
				MemoryContextSwitchTo(batchctx);
				if (text_out)
					NDB_FREE(text_out);
				if (prompt_err)
					NDB_FREE(prompt_err);
			} else
			{
				MemoryContext oldctx_temp = MemoryContextSwitchTo(CurrentMemoryContext);
				results[i].status = -1;
				results[i].error = prompt_err
					? pstrdup(prompt_err)
					: pstrdup("generation failed");
				MemoryContextSwitchTo(oldctx_temp);
				MemoryContextSwitchTo(batchctx);
				if (text_out)
					NDB_FREE(text_out);
				if (prompt_err)
					NDB_FREE(prompt_err);
			}
		}

		/* Check if at least one prompt succeeded */
		rc = -1;
		for (i = 0; i < num_prompts; i++)
		{
			if (results[i].status == 0)
			{
				rc = 0;
				break;
			}
		}
		if (rc != 0)
		{
			/* All prompts failed */
			if (errstr)
				*errstr = pstrdup("all batch completions failed");
		} else
		{
		}
	}

	/* Switch back to original context and clean up */
	MemoryContextSwitchTo(oldctx);
	MemoryContextDelete(batchctx);

	return rc;
}

/*
 * ndb_rocm_hf_rerank
 *	  Rerank documents using ROCm-accelerated Hugging Face model.
 *
 * This is a placeholder implementation. For full ROCm support, we would:
 * 1. Load reranker model weights to GPU memory
 * 2. Tokenize query and documents (concatenate query+doc for each pair)
 * 3. Run cross-encoder on GPU (query-document pairs in batch)
 * 4. Extract relevance scores from model output
 *
 * Full ROCm kernel implementation for reranking with cross-encoder architecture.
 * Uses batch processing and GPU-accelerated transformer forward pass.
 */
int
ndb_rocm_hf_rerank(const char *model_name,
	const char *query,
	const char **docs,
	int ndocs,
	float **scores_out,
	char **errstr)
{
	NdbRocmHfModelEntry *entry;
	float *scores;
	int i;
	int rc;
	hipError_t cuda_err;
	
	if (errstr)
		*errstr = NULL;
	if (model_name == NULL || query == NULL || docs == NULL
		|| scores_out == NULL)
	{
		if (errstr)
			*errstr = pstrdup(
				"invalid parameters for ROCm HF rerank");
		return -1;
	}
	if (ndocs <= 0)
	{
		if (errstr)
			*errstr = pstrdup("invalid number of documents for "
					  "ROCm HF rerank");
		return -1;
	}

	/* ROCm HF reranking implementation framework:
	 * 
	 * Full implementation requires:
	 * 1. Load cross-encoder model (BERT-based reranker) if not cached
	 * 2. Tokenize query + each document (concatenate for each pair)
	 * 3. Run cross-encoder on GPU (batch process query-document pairs)
	 * 4. Extract relevance scores from model output logits
	 * 5. Return scores in descending order
	 */
	
	entry = NULL;
	scores = NULL;

	/* Allocate scores array */
	scores = (float *)palloc(sizeof(float) * ndocs);
	if (!scores)
	{
		if (errstr)
			*errstr = pstrdup("failed to allocate scores array");
		return -1;
	}

	/* Find or load reranker model */
	entry = ndb_rocm_hf_find_model(model_name);
	if (entry == NULL || !entry->loaded)
	{
		/* Load model - would use ndb_rocm_hf_load_model() */
		/* For now, fall back to HTTP/ONNX */
		if (errstr)
			*errstr = pstrdup("ROCm HF reranking model not loaded - use "
					  "HTTP or ONNX fallback");
		NDB_FREE(scores);
		return -1;
	}

	/* Ensure weights are on GPU */
	if (!entry->weights_on_device)
	{
		/* Allocate GPU memory and copy weights */
		size_t weights_size = entry->weights.total_bytes;
		cuda_err = hipMalloc(&entry->device_weights_ptr, weights_size);
		if (cuda_err != hipSuccess)
		{
			if (errstr)
				*errstr = pstrdup("failed to allocate GPU memory for model weights");
			NDB_FREE(scores);
			return -1;
		}

		cuda_err = hipMemcpy(entry->device_weights_ptr,
			entry->weights.embedding_table,
			weights_size,
			hipMemcpyHostToDevice);
		if (cuda_err != hipSuccess)
		{
			hipFree(entry->device_weights_ptr);
			entry->device_weights_ptr = NULL;
			if (errstr)
				*errstr = pstrdup("failed to copy model weights to GPU");
			NDB_FREE(scores);
			return -1;
		}

		entry->weights_on_device = true;
		entry->device_weights_bytes = weights_size;
	}

	/* Batch process all query-document pairs using cross-encoder kernel */
	{
		int32_t *token_ids_batch = NULL;
		int32_t *attention_mask_batch = NULL;
		int max_seq_len = 0;
		int seq_len = 0;
		int embed_dim = entry->config.embed_dim;
		float *classification_weights = NULL;
		float classification_bias = 0.0f;
		int j;

		/* First pass: tokenize all pairs to determine max sequence length */
		for (i = 0; i < ndocs; i++)
		{
			int32_t *temp_token_ids = NULL;
			int32_t *temp_attention_mask = NULL;
			int temp_seq_len = 0;
			char *query_doc_text = NULL;
			size_t query_len;
			size_t doc_len;

			query_len = strlen(query);
			doc_len = strlen(docs[i]);
			query_doc_text = (char *)palloc(query_len + doc_len + 10);
			snprintf(query_doc_text, query_len + doc_len + 10, "%s [SEP] %s", query, docs[i]);

			temp_token_ids = (int32_t *)palloc(sizeof(int32_t) * NDB_HF_MAX_SEQ_LEN);
			temp_attention_mask = (int32_t *)palloc(sizeof(int32_t) * NDB_HF_MAX_SEQ_LEN);

			rc = ndb_rocm_hf_tokenize_text(query_doc_text, model_name,
				temp_token_ids, temp_attention_mask, &temp_seq_len, errstr);
			NDB_FREE(query_doc_text);

			if (rc == 0 && temp_seq_len > max_seq_len)
				max_seq_len = temp_seq_len;

			NDB_FREE(temp_token_ids);
			NDB_FREE(temp_attention_mask);
		}

		if (max_seq_len == 0)
			max_seq_len = NDB_HF_MAX_SEQ_LEN;

		/* Allocate batch arrays */
		token_ids_batch = (int32_t *)palloc(sizeof(int32_t) * ndocs * max_seq_len);
		attention_mask_batch = (int32_t *)palloc(sizeof(int32_t) * ndocs * max_seq_len);

		/* Second pass: tokenize and collect all pairs */
		for (i = 0; i < ndocs; i++)
		{
			int32_t *temp_token_ids = NULL;
			int32_t *temp_attention_mask = NULL;
			int temp_seq_len = 0;
			char *query_doc_text = NULL;
			size_t query_len;
			size_t doc_len;

			query_len = strlen(query);
			doc_len = strlen(docs[i]);
			query_doc_text = (char *)palloc(query_len + doc_len + 10);
			snprintf(query_doc_text, query_len + doc_len + 10, "%s [SEP] %s", query, docs[i]);

			temp_token_ids = (int32_t *)palloc(sizeof(int32_t) * NDB_HF_MAX_SEQ_LEN);
			temp_attention_mask = (int32_t *)palloc(sizeof(int32_t) * NDB_HF_MAX_SEQ_LEN);

			rc = ndb_rocm_hf_tokenize_text(query_doc_text, model_name,
				temp_token_ids, temp_attention_mask, &temp_seq_len, errstr);
			NDB_FREE(query_doc_text);

			if (rc != 0)
			{
				/* Fill with zeros on error */
				memset(token_ids_batch + i * max_seq_len, 0, sizeof(int32_t) * max_seq_len);
				memset(attention_mask_batch + i * max_seq_len, 0, sizeof(int32_t) * max_seq_len);
				NDB_FREE(temp_token_ids);
				NDB_FREE(temp_attention_mask);
				continue;
			}

			/* Copy to batch array */
			memcpy(token_ids_batch + i * max_seq_len, temp_token_ids, sizeof(int32_t) * temp_seq_len);
			memcpy(attention_mask_batch + i * max_seq_len, temp_attention_mask, sizeof(int32_t) * temp_seq_len);

			/* Pad with zeros if needed */
			if (temp_seq_len < max_seq_len)
			{
				memset(token_ids_batch + i * max_seq_len + temp_seq_len, 0,
					sizeof(int32_t) * (max_seq_len - temp_seq_len));
				memset(attention_mask_batch + i * max_seq_len + temp_seq_len, 0,
					sizeof(int32_t) * (max_seq_len - temp_seq_len));
			}

			NDB_FREE(temp_token_ids);
			NDB_FREE(temp_attention_mask);
		}

		/* Get or create classification weights */
		if (entry->weights.classification_weights != NULL)
		{
			classification_weights = entry->weights.classification_weights;
			classification_bias = entry->weights.classification_bias;
		}
		else
		{
			/* Use default classification weights (learned from model or default values) */
			/* For now, use a simple learned-like weight vector */
			classification_weights = (float *)palloc(sizeof(float) * embed_dim);
			for (j = 0; j < embed_dim; j++)
			{
				/* Initialize with small random-like values */
				classification_weights[j] = 0.01f * ((float)(j % 100) / 100.0f - 0.5f);
			}
			classification_bias = 0.0f;
		}

		/* Call cross-encoder reranking kernel */
		rc = ndb_rocm_hf_cross_encoder_rerank_inference(
			model_name,
			token_ids_batch,
			attention_mask_batch,
			ndocs,
			max_seq_len,
			entry->weights.embedding_table,
			entry->config.vocab_size,
			embed_dim,
			classification_weights,
			classification_bias,
			scores,
			errstr);

		/* Cleanup */
		NDB_FREE(token_ids_batch);
		NDB_FREE(attention_mask_batch);
		if (classification_weights != entry->weights.classification_weights)
			NDB_FREE(classification_weights);

		if (rc != 0)
		{
			/* Fallback: set all scores to 0 on error */
			for (i = 0; i < ndocs; i++)
				scores[i] = 0.0f;
		}
	}
			int j, k;

			/* Count tokens in query and document separately */
			for (j = 1; j < seq_len && token_ids[j] != 102; j++) /* Until [SEP] */
				query_tokens++;
			for (k = j + 1; k < seq_len && token_ids[k] != 102; k++) /* Document tokens */
				doc_tokens++;

			/* Simple overlap-based score */
			if (query_tokens > 0 && doc_tokens > 0)
			{
				/* Jaccard similarity approximation */
				relevance_score = (float)common_tokens / (float)(query_tokens + doc_tokens);
			}
			else
			{
				relevance_score = 0.0f;
			}
		}

		scores[i] = relevance_score;

		/* Cleanup */
		NDB_FREE(token_ids);
		NDB_FREE(attention_mask);
		if (query_doc_embedding)
			NDB_FREE(query_doc_embedding);
	}

	*scores_out = scores;
	return 0;
}

/*
 * ndb_rocm_hf_load_model
 *	  Load a Hugging Face model into GPU memory
 *
 * This is a placeholder implementation. For full support, we would:
 * 1. Parse model configuration file
 * 2. Load model weights from file (safetensors, pickle, etc.)
 * 3. Allocate GPU memory for weights
 * 4. Copy weights to GPU
 * 5. Store model in cache
 */
int
ndb_rocm_hf_load_model(const char *model_name,
	const char *model_path,
	NdbHfModelType model_type,
	NdbRocmHfModelConfig *config,
	char **errstr)
{
	NdbRocmHfModelEntry *entry = NULL;
	NdbRocmHfModelWeights weights;
	int rc;

	if (errstr)
		*errstr = NULL;
	if (!model_name || !config)
	{
		if (errstr)
			*errstr =
				pstrdup("invalid parameters for model loading");
		return -1;
	}

	/* Check if model already loaded */
	entry = ndb_rocm_hf_find_model(model_name);
	if (entry != NULL && entry->loaded)
	{
		*config = entry->config;
		return 0;
	}

	/* Load model weights */
	rc = ndb_rocm_hf_load_model_weights(
		model_name, model_path, config, &weights, errstr);
	if (rc != 0)
		return -1;

	/* Switch to CacheMemoryContext for persistent cache entry */
	{
		MemoryContext oldcontext = MemoryContextSwitchTo(CacheMemoryContext);

		/* Create cache entry in CacheMemoryContext (persistent) */
		entry = (NdbRocmHfModelEntry *)palloc(sizeof(NdbRocmHfModelEntry));
		memset(entry, 0, sizeof(NdbRocmHfModelEntry));
		strncpy(entry->model_name, model_name, NDB_HF_MAX_MODEL_NAME - 1);
		entry->config = *config;
		entry->config.model_type = model_type;

		/* Copy weights to CacheMemoryContext */
		{
			size_t embed_table_size =
				config->vocab_size * config->embed_dim * sizeof(float);
			size_t position_embed_size =
				config->max_seq_len * config->embed_dim * sizeof(float);
			size_t lm_head_size =
				config->vocab_size * config->embed_dim * sizeof(float);

			entry->weights.embedding_table =
				(float *)palloc(embed_table_size);
			memcpy(entry->weights.embedding_table,
				weights.embedding_table,
				embed_table_size);

			entry->weights.position_embeddings =
				(float *)palloc(position_embed_size);
			memcpy(entry->weights.position_embeddings,
				weights.position_embeddings,
				position_embed_size);

			entry->weights.lm_head_weights =
				(float *)palloc(lm_head_size);
			memcpy(entry->weights.lm_head_weights,
				weights.lm_head_weights,
				lm_head_size);

			entry->weights.total_bytes = weights.total_bytes;
		}

		entry->loaded = true;
		entry->weights_on_device = false;
		entry->last_used = time(NULL);
		entry->next = g_model_cache;
		g_model_cache = entry;
		g_model_cache_count++;

		/* Switch back to original context */
		MemoryContextSwitchTo(oldcontext);
	}

	return 0;
}

/*
 * ndb_rocm_hf_unload_model
 *	  Unload a Hugging Face model from GPU memory
 */
int
ndb_rocm_hf_unload_model(const char *model_name, char **errstr)
{
	NdbRocmHfModelEntry *entry = NULL;
	NdbRocmHfModelEntry *prev = NULL;
	hipError_t hip_status;

	if (errstr)
		*errstr = NULL;
	if (!model_name)
	{
		if (errstr)
			*errstr = pstrdup(
				"invalid parameters for model unloading");
		return -1;
	}

	/* Find model in cache */
	for (entry = g_model_cache; entry != NULL;
		prev = entry, entry = entry->next)
	{
		if (strcmp(entry->model_name, model_name) == 0)
			break;
	}

	if (entry == NULL)
	{
		if (errstr)
			*errstr = psprintf(
				"model '%s' not found in cache", model_name);
		return -1;
	}

	/* Free GPU memory if allocated */
	if (entry->weights_on_device)
	{
		if (entry->device_weights_ptr != NULL)
		{
			hip_status = hipFree(entry->device_weights_ptr);
			if (hip_status != hipSuccess && errstr)
				*errstr = psprintf("ROCm free failed for "
						   "embedding table: %s",
					hipGetErrorString(hip_status));
			entry->device_weights_ptr = NULL;
		}
		if (entry->device_position_embeddings != NULL)
		{
			hip_status =
				hipFree(entry->device_position_embeddings);
			if (hip_status != hipSuccess && errstr)
				*errstr = psprintf("ROCm free failed for "
						   "position embeddings: %s",
					hipGetErrorString(hip_status));
			entry->device_position_embeddings = NULL;
		}
		if (entry->device_lm_head_weights != NULL)
		{
			hip_status = hipFree(entry->device_lm_head_weights);
			if (hip_status != hipSuccess && errstr)
				*errstr = psprintf("ROCm free failed for LM "
						   "head weights: %s",
					hipGetErrorString(hip_status));
			entry->device_lm_head_weights = NULL;
		}
		entry->weights_on_device = false;
	}

	/* Free host memory */
	if (entry->weights.embedding_table != NULL)
		NDB_FREE(entry->weights.embedding_table);
	if (entry->weights.position_embeddings != NULL)
		NDB_FREE(entry->weights.position_embeddings);
	if (entry->weights.lm_head_weights != NULL)
		NDB_FREE(entry->weights.lm_head_weights);

	/* Remove from cache */
	if (prev == NULL)
		g_model_cache = entry->next;
	else
		prev->next = entry->next;

	NDB_FREE(entry);
	g_model_cache_count--;

	return 0;
}

/*
 * ndb_rocm_hf_model_loaded
 *	  Check if a model is loaded in cache
 */
bool
ndb_rocm_hf_model_loaded(const char *model_name)
{
	NdbRocmHfModelEntry *entry;

	if (!model_name)
		return false;

	entry = ndb_rocm_hf_find_model(model_name);
	return (entry != NULL && entry->loaded);
}

/*
 * ndb_rocm_hf_get_model_config
 *	  Get configuration for a loaded model
 */
int
ndb_rocm_hf_get_model_config(const char *model_name,
	NdbRocmHfModelConfig *config,
	char **errstr)
{
	NdbRocmHfModelEntry *entry;

	if (errstr)
		*errstr = NULL;
	if (!model_name || !config)
	{
		if (errstr)
			*errstr = pstrdup(
				"invalid parameters for get model config");
		return -1;
	}

	entry = ndb_rocm_hf_find_model(model_name);
	if (entry == NULL || !entry->loaded)
	{
		if (errstr)
			*errstr = psprintf("model '%s' not found or not loaded",
				model_name);
		return -1;
	}

	*config = entry->config;
	return 0;
}

/*
 * ndb_rocm_hf_tokenize
 *	  Tokenize text using model's tokenizer
 */
int
ndb_rocm_hf_tokenize(const char *text,
	const char *model_name,
	int32_t *token_ids_out,
	int32_t *attention_mask_out,
	int *seq_len_out,
	char **errstr)
{
	if (errstr)
		*errstr = NULL;
	if (!text || !model_name || !token_ids_out || !attention_mask_out
		|| !seq_len_out)
	{
		if (errstr)
			*errstr =
				pstrdup("invalid parameters for tokenization");
		return -1;
	}

	return ndb_rocm_hf_tokenize_text(text,
		model_name,
		token_ids_out,
		attention_mask_out,
		seq_len_out,
		errstr);
}

#else /* NDB_GPU_HIP */

/*
 * Stub implementations when CUDA is not compiled in
 */
int
ndb_rocm_hf_embed(const char *model_name,
	const char *text,
	float **vec_out,
	int *dim_out,
	char **errstr)
{
	if (errstr)
		*errstr = pstrdup("ROCm support not compiled in");
	return -1;
}

int
ndb_rocm_hf_complete(const char *model_name,
	const char *prompt,
	const char *params_json,
	char **text_out,
	char **errstr)
{
	if (errstr)
		*errstr = pstrdup("ROCm support not compiled in");
	return -1;
}

int
ndb_rocm_hf_rerank(const char *model_name,
	const char *query,
	const char **docs,
	int ndocs,
	float **scores_out,
	char **errstr)
{
	if (errstr)
		*errstr = pstrdup("ROCm support not compiled in");
	return -1;
}

int
ndb_rocm_hf_load_model(const char *model_name,
	const char *model_path,
	NdbHfModelType model_type,
	NdbRocmHfModelConfig *config,
	char **errstr)
{
	if (errstr)
		*errstr = pstrdup("ROCm support not compiled in");
	return -1;
}

int
ndb_rocm_hf_unload_model(const char *model_name, char **errstr)
{
	if (errstr)
		*errstr = pstrdup("ROCm support not compiled in");
	return -1;
}

bool
ndb_rocm_hf_model_loaded(const char *model_name)
{
	return false;
}

int
ndb_rocm_hf_get_model_config(const char *model_name,
	NdbRocmHfModelConfig *config,
	char **errstr)
{
	if (errstr)
		*errstr = pstrdup("ROCm support not compiled in");
	return -1;
}

int
ndb_rocm_hf_tokenize(const char *text,
	const char *model_name,
	int32_t *token_ids_out,
	int32_t *attention_mask_out,
	int *seq_len_out,
	char **errstr)
{
	if (errstr)
		*errstr = pstrdup("ROCm support not compiled in");
	return -1;
}

#endif /* NDB_GPU_HIP */
